[{"body":"This doc describes cloud provider config file, which is to be used via the --cloud-config flag of azure-cloud-controller-manager.\nHere is a config file sample:\n{ \"cloud\":\"AzurePublicCloud\", \"tenantId\": \"0000000-0000-0000-0000-000000000000\", \"aadClientId\": \"0000000-0000-0000-0000-000000000000\", \"aadClientSecret\": \"0000000-0000-0000-0000-000000000000\", \"subscriptionId\": \"0000000-0000-0000-0000-000000000000\", \"resourceGroup\": \"\u003cname\u003e\", \"location\": \"eastus\", \"subnetName\": \"\u003cname\u003e\", \"securityGroupName\": \"\u003cname\u003e\", \"vnetName\": \"\u003cname\u003e\", \"vnetResourceGroup\": \"\", \"routeTableName\": \"\u003cname\u003e\", \"primaryAvailabilitySetName\": \"\u003cname\u003e\", \"routeTableResourceGroup\": \"\u003cname\u003e\", \"cloudProviderBackoff\": false, \"useManagedIdentityExtension\": false, \"useInstanceMetadata\": true } Note: All values are of type string if not explicitly called out.\nAuth configs    Name Description Remark     cloud The cloud environment identifier Valid values could be found here. Default to AzurePublicCloud.   tenantID The AAD Tenant ID for the Subscription that the cluster is deployed in Required.   aadClientID The ClientID for an AAD application with RBAC access to talk to Azure RM APIs Used for service principal authn.   aadClientSecret The ClientSecret for an AAD application with RBAC access to talk to Azure RM APIs Used for service principal authn.   aadClientCertPath The path of a client certificate for an AAD application with RBAC access to talk to Azure RM APIs Used for client cert authn.   aadClientCertPassword The password of the client certificate for an AAD application with RBAC access to talk to Azure RM APIs Used for client cert authn.   useManagedIdentityExtension Use managed service identity for the virtual machine to access Azure ARM APIs Boolean type, default to false.   userAssignedIdentityID The Client ID of the user assigned MSI which is assigned to the underlying VMs Required for user-assigned managed identity.   subscriptionId The ID of the Azure Subscription that the cluster is deployed in Required.   identitySystem The identity system for AzureStack. Supported values are: ADFS Only used for AzureStack   networkResourceTenantID The AAD Tenant ID for the Subscription that the network resources are deployed in Optional. Supported since v1.18.0. Only used for hosting network resources in different AAD Tenant and Subscription than those for the cluster.   networkResourceSubscriptionID The ID of the Azure Subscription that the network resources are deployed in Optional. Supported since v1.18.0. Only used for hosting network resources in different AAD Tenant and Subscription than those for the cluster.    Note: Cloud provider currently supports three authentication methods, you can choose one combination of them:\n Managed Identity:  For system-assigned managed identity: set useManagedIdentityExtension to true For user-assigned managed identity: set useManagedIdentityExtension to true and also set userAssignedIdentityID   Service Principal: set aadClientID and aadClientSecret Client Certificate: set aadClientCertPath and aadClientCertPassword  If more than one value is set, the order is Managed Identity \u003e Service Principal \u003e Client Certificate.\nCluster config    Name Description Remark     resourceGroup The name of the resource group that the cluster is deployed in    location The location of the resource group that the cluster is deployed in    vnetName The name of the VNet that the cluster is deployed in    vnetResourceGroup The name of the resource group that the Vnet is deployed in    subnetName The name of the subnet that the cluster is deployed in    securityGroupName The name of the security group attached to the cluster's subnet    routeTableName The name of the route table attached to the subnet that the cluster is deployed in Optional in 1.6   primaryAvailabilitySetName* The name of the availability set that should be used as the load balancer backend Optional   vmType The type of azure nodes. Candidate values are: vmss and standard|Optional, default to standard|    primaryScaleSetName* The name of the scale set that should be used as the load balancer backend Optional   cloudProviderBackoff Enable exponential backoff to manage resource request retries Boolean value, default to false   cloudProviderBackoffRetries Backoff retry limit Integer value, valid if cloudProviderBackoff is true   cloudProviderBackoffExponent Backoff exponent Float value, valid if cloudProviderBackoff is true   cloudProviderBackoffDuration Backoff duration Integer value, valid if cloudProviderBackoff is true   cloudProviderBackoffJitter Backoff jitter Float value, valid if cloudProviderBackoff is true   cloudProviderBackoffMode Backoff mode, supported values are “v2” and “default”. Note that “v2” has been deprecated since v1.18.0. Default to “default”   cloudProviderRateLimit Enable rate limiting Boolean value, default to false   cloudProviderRateLimitQPS Rate limit QPS (Read) Float value, valid if cloudProviderRateLimit is true   cloudProviderRateLimitBucket Rate limit Bucket Size Integar value, valid if cloudProviderRateLimit is true   cloudProviderRateLimitQPSWrite Rate limit QPS (Write) Float value, valid if cloudProviderRateLimit is true   cloudProviderRateLimitBucketWrite Rate limit Bucket Size Integer value, valid if cloudProviderRateLimit is true   useInstanceMetadata Use instance metadata service where possible Boolean value, default to false   loadBalancerSku Sku of Load Balancer and Public IP. Candidate values are: basic and standard. Default to basic.   excludeMasterFromStandardLB ExcludeMasterFromStandardLB excludes master nodes from standard load balancer. Boolean value, default to true.   disableOutboundSNAT Disable outbound SNAT for SLB Default to false and available since v1.11.9, v1.12.7, v1.13.5 and v1.14.0   maximumLoadBalancerRuleCount Maximum allowed LoadBalancer Rule Count is the limit enforced by Azure Load balancer Integer value, default to 148   routeTableResourceGroup The resource group name for routeTable Default same as resourceGroup and available since v1.15.0   cloudConfigType The cloud configure type for Azure cloud provider. Supported values are file, secret and merge. Default to merge. and available since v1.15.0   loadBalancerName Working together with loadBalancerResourceGroup to determine the LB name in a different resource group Since v1.18.0, default is cluster name setting on kube-controller-manager   loadBalancerResourceGroup The load balancer resource group name, which is different from node resource group Since v1.18.0, default is same as resourceGroup   disableAvailabilitySetNodes Disable supporting for AvailabilitySet virtual machines in vmss cluster. It should be only used when vmType is “vmss” and all the nodes (including master) are VMSS virtual machines Since v1.18.0, default is false   availabilitySetNodesCacheTTLInSeconds Cache TTL in seconds for availabilitySet Nodes Since v1.18.0, default is 900   vmssCacheTTLInSeconds Cache TTL in seconds for VMSS Since v1.18.0, default is 600   vmssVirtualMachinesCacheTTLInSeconds Cache TTL in seconds for VMSS virtual machines Since v1.18.0, default is 600   vmCacheTTLInSeconds Cache TTL in seconds for virtual machines Since v1.18.0, default is 60   loadBalancerCacheTTLInSeconds Cache TTL in seconds for load balancers Since v1.18.0, default is 120   nsgCacheTTLInSeconds Cache TTL in seconds for network security group Since v1.18.0, default is 120   routeTableCacheTTLInSeconds Cache TTL in seconds for route table Since v1.18.0, default is 120    primaryAvailabilitySetName If this is set, the Azure cloudprovider will only add nodes from that availability set to the load balancer backend pool. If this is not set, and multiple agent pools (availability sets) are used, then the cloudprovider will try to add all nodes to a single backend pool which is forbidden. In other words, if you use multiple agent pools (availability sets), you MUST set this field.\nprimaryScaleSetName If this is set, the Azure cloudprovider will only add nodes from that scale set to the load balancer backend pool. If this is not set, and multiple agent pools (scale sets) are used, then the cloudprovider will try to add all nodes to a single backend pool which is forbidden when using Load Balancer Basic SKU. In other words, if you use multiple agent pools (scale sets), and loadBalancerSku is set to basic you MUST set this field.\nexcludeMasterFromStandardLB Master nodes are not added to the backends of Azure Load Balancer (ALB) if excludeMasterFromStandardLB is set.\nBy default, if nodes are labeled with node-role.kubernetes.io/master, they would also be excluded from ALB. If you want to add the master nodes to ALB, excludeMasterFromStandardLB should be set to false and label node-role.kubernetes.io/master should be removed if it has already been applied.\nSetting Azure cloud provider from Kubernetes secrets Since v1.15.0, Azure cloud provider supports reading the cloud config from Kubernetes secrets. The secret is a serialized version of azure.json file with key cloud-config. The secret should be put in kube-system namespace and its name should be azure-cloud-provider.\nTo enable this feature, set cloudConfigType to secret or merge (default is merge). All supported values for this option are:\n file: The cloud provider configuration is read from cloud-config file. secret: the cloud provider configuration must be overridden by the secret. merge: the cloud provider configuration can be optionally overridden by a secret when it is set explicitly in the secret, this is default value.  Since Azure cloud provider would read Kubernetes secrets, the following RBAC should also be configured:\n---apiVersion:rbac.authorization.k8s.io/v1beta1kind:ClusterRolemetadata:labels:kubernetes.io/cluster-service:\"true\"name:system:azure-cloud-provider-secret-getterrules:-apiGroups:[\"\"]resources:[\"secrets\"]resourceNames:[\"azure-cloud-provider\"]verbs:-get---apiVersion:rbac.authorization.k8s.io/v1beta1kind:ClusterRoleBindingmetadata:labels:kubernetes.io/cluster-service:\"true\"name:system:azure-cloud-provider-secret-getterroleRef:apiGroup:rbac.authorization.k8s.iokind:ClusterRolename:system:azure-cloud-provider-secret-gettersubjects:-kind:ServiceAccountname:azure-cloud-providernamespace:kube-systemper client rate limiting Since v1.18.0, the original global rate limiting has been switched to per-client. A set of new rate limit configure options are introduced for each client, which includes:\n routeRateLimit SubnetsRateLimit InterfaceRateLimit RouteTableRateLimit LoadBalancerRateLimit PublicIPAddressRateLimit SecurityGroupRateLimit VirtualMachineRateLimit StorageAccountRateLimit DiskRateLimit SnapshotRateLimit VirtualMachineScaleSetRateLimit VirtualMachineSizeRateLimit  The original rate limiting options (“cloudProviderRateLimitBucket”, “cloudProviderRateLimitBucketWrite”, “cloudProviderRateLimitQPS”, “cloudProviderRateLimitQPSWrite”) are still supported, and they would be the default values if per-client rate limiting is not configured.\nHere is an example of per-client config:\n{ // default rate limit (enabled). \"cloudProviderRatelimit\": true, \"cloudProviderRateLimitBucket\": 1, \"cloudProviderRateLimitBucketWrite\": 1, \"cloudProviderRateLimitQPS\": 1, \"cloudProviderRateLimitQPSWrite\": 1, \"virtualMachineScaleSetRateLimit\": { // VMSS specific (enabled). \"cloudProviderRatelimit\": true, \"cloudProviderRateLimitBucket\": 2, \"CloudProviderRateLimitBucketWrite\": 2, \"cloudProviderRateLimitQPS\": 0, \"CloudProviderRateLimitQPSWrite\": 0 }, \"loadBalancerRateLimit\": { // LB specific (disabled) \"cloudProviderRatelimit\": false }, ... // other cloud provider configs } Run Kubelet without Azure identity When running Kubelet with kube-controller-manager, it also supports running without Azure identity since v1.15.0.\nBoth kube-controller-manager and kubelet should configure --cloud-provider=azure --cloud-config=/etc/kubernetes/azure.json, but the contents for azure.json are different:\n(1) For kube-controller-manager, refer the above part for setting azure.json.\n(2) For kubelet, useInstanceMetadata is required to be true and Azure identities are not required. A sample for Kubelet's azure.json is\n{ \"useInstanceMetadata\": true, \"vmType\": \"vmss\" } Azure Stack Configuration Azure Stack has different API endpoints, depending on the Azure Stack deployment. These need to be provided to the Azure SDK and currently this is done by adding an extra json file with the arguments, as well as an environment variable pointing to this file.\nThere are several available presets, namely:\n AzureChinaCloud AzureGermanCloud AzurePublicCloud AzureUSGovernmentCloud  These are determined using cloud: \u003cPRESET\u003e described above in the description of azure.json.\nWhen cloud: AzureStackCloud, the extra environment variable used by the Azure SDK to find the Azure Stack configuration file is:\n AZURE_ENVIRONMENT_FILEPATH  The configuration parameters of this file:\n{ \"name\": \"AzureStackCloud\", \"managementPortalURL\": \"...\", \"publishSettingsURL\": \"...\", \"serviceManagementEndpoint\": \"...\", \"resourceManagerEndpoint\": \"...\", \"activeDirectoryEndpoint\": \"...\", \"galleryEndpoint\": \"...\", \"keyVaultEndpoint\": \"...\", \"graphEndpoint\": \"...\", \"serviceBusEndpoint\": \"...\", \"batchManagementEndpoint\": \"...\", \"storageEndpointSuffix\": \"...\", \"sqlDatabaseDNSSuffix\": \"...\", \"trafficManagerDNSSuffix\": \"...\", \"keyVaultDNSSuffix\": \"...\", \"serviceBusEndpointSuffix\": \"...\", \"serviceManagementVMDNSSuffix\": \"...\", \"resourceManagerVMDNSSuffix\": \"...\", \"containerRegistryDNSSuffix\": \"...\", \"cosmosDBDNSSuffix\": \"...\", \"tokenAudience\": \"...\", \"resourceIdentifiers\": { \"graph\": \"...\", \"keyVault\": \"...\", \"datalake\": \"...\", \"batch\": \"...\", \"operationalInsights\": \"...\" } } The full list of existing settings for the AzureChinaCloud, AzureGermanCloud, AzurePublicCloud and AzureUSGovernmentCloud is available in the source code at https://github.com/Azure/go-autorest/blob/master/autorest/azure/environments.go#L51.\nHost Network Resources in different AAD Tenant and Subscription Since v1.18.0, Azure cloud provider supports hosting network resources (Virtual Network, Network Security Group, Route Table, Load Balancer and Public IP) in different AAD Tenant and Subscription than those for the cluster. To enable this feature, set networkResourceTenantID and networkResourceSubscriptionID in auth config. Note that the value of them need to be different than value of tenantID and subscriptionID.\nWith this feature enabled, network resources of the cluster will be created in networkResourceSubscriptionID in networkResourceTenantID, and rest resources of the cluster still remain in subscriptionID in tenantID. Properties which specify the resource groups of network resources are compatible with this feature. For example, Virtual Network will be created in vnetResourceGroup in networkResourceSubscriptionID in networkResourceTenantID.\nFor authentication methods, only Service Principal supports this feature, and aadClientID and aadClientSecret are used to authenticate with those two AAD Tenants and Subscriptions. Managed Identity and Client Certificate doesn't support this feature. Azure Stack doesn't support this feature.\nCurrent default rate-limiting values The following are the default rate limiting values configured in AKS and AKS-Engine clusters prior to Kubernetes version v1.18.0.\n\"cloudProviderBackoff\": true, \"cloudProviderBackoffRetries\": 6, \"cloudProviderBackoffDuration\": 5, \"cloudProviderRatelimit\": true, \"cloudProviderRateLimitQPS\": 10, \"cloudProviderRateLimitBucket\": 100, \"cloudProviderRatelimitQPSWrite\": 10, \"cloudProviderRatelimitBucketWrite\": 100, For v1.18.0+ refer to per client rate limit config\n","excerpt":"This doc describes cloud provider config file, which is to be used via …","ref":"/cloud-provider-azure/install/configs/","title":"Configure Cloud Provider"},{"body":"Thanks for taking the time to join our community and start contributing!\nThe Contributor Guide provides detailed instructions on how to get your ideas and bug fixes seen and accepted.\nPlease remember to sign the CNCF CLA and read and observe the Code of Conduct.\n","excerpt":"Thanks for taking the time to join our community and start …","ref":"/cloud-provider-azure/contribute/contributing/","title":"Contributing"},{"body":"Switch to the project root directory and make image. This will build both CCM and CNM images. If you want to build only one of them, try make build-ccm-image or make build-node-image. To push the images to your own image registry, you can specify the registry and image tag while building: IMAGE_REGISTRY=\u003cimage registry name\u003e IMAGE_TAG=\u003ctag name\u003e make image. After building, you can push it by make push.\n","excerpt":"Switch to the project root directory and make image. This will build …","ref":"/cloud-provider-azure/development/custom-images/","title":"Deploy with Customized Images"},{"body":"To deploy an In-tree Cloud Provider Azure, all you need to do is deploy a cluster using AKS-Engine with the API model defined here. The AKS-Engine will automatically deploy the Kubernetes components needed and you don't have to deploy them manually. However, customization is possible by modifying the manifests in /etc/kubernetes on master node. Here are the examples:\nkube-apiserver\nkube-controller-manager\nkube-scheduler\nTo customize kubelet, you need to modify the starting command like here.\n","excerpt":"To deploy an In-tree Cloud Provider Azure, all you need to do is …","ref":"/cloud-provider-azure/example/in-tree/","title":"Deploy with In-tree Cloud Provider Azure"},{"body":"","excerpt":"","ref":"/cloud-provider-azure/blog/releases/","title":"Release Notes"},{"body":"azure-cloud-controller-manager is a Kubernetes component which provides interoperability with Azure API, and will be used by Kubernetes clusters running on Azure. It runs together with other components to provide the Kubernetes cluster’s control plane.\nUsing cloud-controller-manager is a new alpha feature for Kubernetes since v1.14. cloud-controller-manager runs cloud provider related controller loops, which used to be run by controller-manager.\nazure-cloud-controller-manager is a specialization of cloud-controller-manager. It depends on cloud-controller-manager app and azure cloud provider.\nUsage To use cloud controller manager, the following components need to be configured:\n  kubelet\n   Flag Value Remark     –cloud-provider external cloud-provider should be set external   –azure-container-registry-config /etc/kubernetes/azure.json Used for Azure credential provider      kube-controller-manager\n   Flag Value Remark     –cloud-provider external cloud-provider should be set external   –external-cloud-volume-plugin azure Optional*    * Since cloud controller manager does not support volume controllers, it will not provide volume capabilities compared to using previous built-in cloud provider case. You can add this flag to turn on volume controller for in-tree cloud providers. This option is likely to be removed with in-tree cloud providers in future.\n  kube-apiserver\nDo not set flag --cloud-provider\n  azure-cloud-controller-manager\nSet following flags:\n   Flag Value Remark     –cloud-provider azure cloud-provider should be set azure   –cloud-config /etc/kubernetes/azure.json Path for cloud provider config    For other flags such as --allocate-node-cidrs, --configure-cloud-routes, --cluster-cidr, they are moved from kube-controller-manager. If you are migrating from kube-controller-manager, they should be set to same value.\nFor details of those flags, please refer to this doc.\n  Alternatively, you can use aks-engine to deploy a Kubernetes cluster running with cloud-controller-manager. It supports deploying Kubernetes azure-cloud-controller-manager for Kubernetes v1.16+.\nAzureDisk and AzureFile AzureDisk and AzureFile volume plugins are not supported with external coud provider (See kubernetes/kubernetes#71018 for explanations).\nHence, azuredisk-csi-driver and azurefile-csi-driver should be used for persistent volumes.\nDeploy AzureDisk CSI plugin Run following commands:\nkubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/azuredisk-csi-driver/master/deploy/csi-azuredisk-driver.yaml kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/azuredisk-csi-driver/master/deploy/rbac-csi-azuredisk-controller.yaml kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/azuredisk-csi-driver/master/deploy/rbac-csi-azuredisk-node.yaml kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/azuredisk-csi-driver/master/deploy/csi-azuredisk-controller.yaml kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/azuredisk-csi-driver/master/deploy/csi-azuredisk-node.yaml kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/azuredisk-csi-driver/master/deploy/csi-azuredisk-node-windows.yaml # skip below yaml configurations if snapshot feature(only available from v1.17.0) will not be used kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/azuredisk-csi-driver/master/deploy/crd-csi-snapshot.yaml kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/azuredisk-csi-driver/master/deploy/rbac-csi-snapshot-controller.yaml kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/azuredisk-csi-driver/master/deploy/csi-snapshot-controller.yaml See azuredisk-csi-driver for more details.\nDeploy AzureFile CSI plugin Run following commands:\nkubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/azurefile-csi-driver/master/deploy/rbac-csi-azurefile-controller.yaml kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/azurefile-csi-driver/master/deploy/rbac-csi-azurefile-node.yaml kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/azurefile-csi-driver/master/deploy/csi-azurefile-controller.yaml kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/azurefile-csi-driver/master/deploy/csi-azurefile-driver.yaml kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/azurefile-csi-driver/master/deploy/csi-azurefile-node.yaml kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/azurefile-csi-driver/master/deploy/csi-azurefile-node-windows.yaml See azurefile-csi-driver for more details.\nChange default storage class Follow the steps bellow if you want change the current default storage class to AzureDisk CSI driver.\nFirst, delete the default storage class:\nkubectl delete storageclass default Then create a new storage class named default:\ncat \u003c\u003cEOF | kubectl apply -f- apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: annotations: storageclass.beta.kubernetes.io/is-default-class: \"true\" name: default provisioner: disk.csi.azure.com parameters: skuname: Standard_LRS # available values: Standard_LRS, Premium_LRS, StandardSSD_LRS and UltraSSD_LRS kind: managed # value \"dedicated\", \"shared\" are deprecated since it's using unmanaged disk cachingMode: ReadOnly reclaimPolicy: Delete volumeBindingMode: Immediate EOF Development Build project:\nmake Build image:\nIMAGE_REGISTRY=\u003cregistry\u003e make image Run unit tests:\nmake test-unit Updating dependency: (please check Dependency management for additional information)\nmake update Limitations Because CSI is not ready on Windows, AzureDisk/AzureFile CSI drivers don't support Windows either. If you have Windows nodes in the cluster, please use kube-controller-manager instead of cloud-controller-manager.\n","excerpt":"azure-cloud-controller-manager is a Kubernetes component which …","ref":"/cloud-provider-azure/install/azure-ccm/","title":"Deploy Cloud Controller Manager"},{"body":"The way Azure defines a LoadBalancer is different from GCE or AWS. Azure's LB can have multiple frontend IP refs. GCE and AWS only allow one, if you want more, you would need multiple LBs. Since Public IP's are not part of the LB in Azure, an NSG is not part of the LB in Azure either. However, you cannot delete them in parallel, a Public IP can only be deleted after the LB's frontend IP ref is removed.\nThe different Azure Resources such as LB, Public IP, and NSG are the same tier of Azure resources and circular dependencies need to be avoided. In other words, they should only depend on service state.\nBy default the basic SKU is selected for a load balancer. Services can be annotated to allow auto selection of available load balancers. Service annotations can also be used to provide specific availability sets that host the load balancers. Note that in case of auto selection or specific availability set selection, services are currently not auto-reassigned to an available loadbalancer when the availability set is lost in case of downtime or cluster scale down.\nLoadBalancer annotations Below is a list of annotations supported for Kubernetes services with type LoadBalancer:\n   Annotation Value Description Kubernetes Version     service.beta.kubernetes.io/azure-load-balancer-internal true or false Specify whether the load balancer should be internal. It’s defaulting to public if not set. v1.10.0 and later   service.beta.kubernetes.io/azure-load-balancer-internal-subnet Name of the subnet Specify which subnet the internal load balancer should be bound to. It’s defaulting to the subnet configured in cloud config file if not set. v1.10.0 and later   service.beta.kubernetes.io/azure-load-balancer-mode auto, {name1},{name2} Specify the Azure load balancer selection algorithm based on availability sets. There are currently three possible load balancer selection modes : default, auto or “{name1}, {name2}\". This is only working for basic LB (see below for how it works) v1.10.0 and later   service.beta.kubernetes.io/azure-dns-label-name Name of the PIP DNS label Specify the DNS label name for the service's public IP address (PIP). If it is set to empty string, DNS in PIP would be deleted. Because of a bug, before v1.15.10/v1.16.7/v1.17.3, the DNS label on PIP would also be deleted if the annotation is not specified. v1.15.0 and later   service.beta.kubernetes.io/azure-shared-securityrule true or false Specify that the service should be exposed using an Azure security rule that may be shared with another service, trading specificity of rules for an increase in the number of services that can be exposed. This relies on the Azure “augmented security rules” feature. v1.10.0 and later   service.beta.kubernetes.io/azure-load-balancer-resource-group Name of the PIP resource group Specify the resource group of the service's PIP that are not in the same resource group as the cluster. v1.10.0 and later   service.beta.kubernetes.io/azure-allowed-service-tags List of allowed service tags Specify a list of allowed service tags separated by comma. v1.11.0 and later   service.beta.kubernetes.io/azure-load-balancer-tcp-idle-timeout TCP idle timeouts in minutes Specify the time, in minutes, for TCP connection idle timeouts to occur on the load balancer. Default and minimum value is 4. Maximum value is 30. Must be an integer. v1.11.4, v1.12.0 and later   service.beta.kubernetes.io/azure-pip-name Name of PIP Specify the PIP that will be applied to load balancer v1.16 and later   service.beta.kubernetes.io/azure-load-balancer-disable-tcp-reset true Disable enableTcpReset for SLB v1.16-v1.18. The annotation has been deprecated and would be removed in a future release.    Load balancer selection modes There are currently three possible load balancer selection modes :\n Default mode - service has no annotation (“service.beta.kubernetes.io/azure-load-balancer-mode”). In this case the Loadbalancer of the primary Availability set is selected “auto” mode - service is annotated with auto value, this when loadbalancer from any availability set is selected which has the minimum rules associated with it. “{name1}, {name2}” mode - this is when the load balancer from the specified availability sets is selected that has the minimum rules associated with it.  The selection mode for a load balancer only works for Azure's basic SKU (see below) because of the difference in backend pool endpoints:\n Standard SKU supports any virtual machine in a single virtual network, including a mix of virtual machines, availability sets, and virtual machine scale sets. So all the nodes would be added to the same standard LB backend pool with a max size of 1000. Basic SKU only supports virtual machines in a single availability set, or a virtual machine scale set. Only nodes with the same availability set or virtual machine scale set would be added to the basic LB backend pool.  LoadBalancer SKUs Azure cloud provider supports both basic and standard SKU load balancers, which can be set via loadBalancerSku option in cloud config file. A list of differences between these two SKUs can be found here.\n Note that the public IPs used in load balancer frontend configurations should be the same SKU. That is a standard SKU public IP for standard load balancer and a basic SKU public IP for a basic load balancer.\n Azure doesn’t support a network interface joining load balancers with different SKUs, hence migration dynamically between them is not supported.\n If you do require migration, please delete all services with type LoadBalancer (or change to other type)\n Outbound connectivity Outbound connectivity is also different between the two load balancer SKUs:\n  For the basic SKU, the outbound connectivity is opened by default. If multiple frontends are set, then the outbound IP is selected randomly (and configurable) from them.\n  For the standard SKU, the outbound connectivity is disabled by default. There are two ways to open the outbound connectivity: use a standard public IP with the standard load balancer or define outbound rules.\n  Standard LoadBalancer Because the load balancer in a Kubernetes cluster is managed by the Azure cloud provider, and it may change dynamically (e.g. the public load balancer would be deleted if no services defined with type LoadBalancer), outbound rules are the recommended path if you want to ensure the outbound connectivity for all nodes.\n Especially note:\n  In the context of outbound connectivity, a single standalone VM, all the VM's in an Availability Set, all the instances in a VMSS behave as a group. This means, if a single VM in an Availability Set is associated with a Standard SKU, all VM instances within this Availability Set now behave by the same rules as if they are associated with Standard SKU, even if an individual instance is not directly associated with it.\n  Public IP's used as instance-level public IP are mutually exclusive with outbound rules.\n   Here is the recommended way to define the outbound rules when using separate provisioning tools:\n Create a separate IP (or multiple IPs for scale) in a standard SKU for outbound rules. Make use of the allocatedOutboundPorts parameter to allocate sufficient ports for your desired scenario scale. Create a separate pool definition for outbound, and ensure all virtual machines or VMSS virtual machines are in this pool. Azure cloud provider will manage the load balancer rules with another pool, so that provisioning tools and the Azure cloud provider won't affect each other. Define inbound with load balancing rules and inbound NAT rules as needed, and set disableOutboundSNAT to true on the load balancing rule(s). Don't rely on the side effect from these rules for outbound connectivity. It makes it messier than it needs to be and limits your options. Use inbound NAT rules to create port forwarding mappings for SSH access to the VM's rather than burning public IPs per instance.  Exclude nodes from the load balancer (v1.20.0) The kubernetes controller manager supports excluding nodes from the load balancer backend pools by enabling the feature gate ServiceNodeExclusion, which is in beta state since v1.19. This PR let users to exclude nodes from the LB by labeling node.kubernetes.io/exclude-from-external-load-balancers=true on the nodes. There are several things that need to be mentioned.\n  To use the feature, the feature gate ServiceNodeExclusion should be on.\n  The labeled nodes would be excluded from the LB in the next LB reconcile loop, which needs one or more LB typed services to trigger. Basically, users could trigger the update by creating a service. If there are one or more LB typed services existing, no extra operations are needed.\n  To re-include the nodes, just remove the label and the update would be operated in the next LB reconcile loop.\n  ","excerpt":"The way Azure defines a LoadBalancer is different from GCE or AWS. …","ref":"/cloud-provider-azure/topics/loadbalancer/","title":"Azure LoadBalancer"},{"body":"cloud-provider-azure uses go modules for Go dependency management.\nUsage Run hack/update-dependencies.sh whenever vendored dependencies change. This takes a minute to complete.\nUpdating dependencies New dependencies causes golang to recompute the minor version used for each major version of each dependency. And golang automatically removes dependencies that nothing imports any more.\nTo upgrade to the latest version for all direct and indirect dependencies of the current module:\n run go get -u \u003cpackage\u003e to use the latest minor or patch releases run go get -u=patch \u003cpackage\u003e to use the latest patch releases run go get \u003cpackage\u003e@VERSION to use the specified version  You can also manually editing go.mod and update the versions in require and replace parts.\nBecause of staging in Kubernetes, manually go.mod updating is required for Kubernetes and its staging packages. In cloud-provider-azure, their versions are set in replace part, e.g.\nreplace ( ... k8s.io/kubernetes =\u003e k8s.io/kubernetes v0.0.0-20190815230911-4e7fd98763aa k8s.io/legacy-cloud-providers =\u003e k8s.io/kubernetes/staging/src/k8s.io/legacy-cloud-providers v0.0.0-20190815230911-4e7fd98763aa ) To update their versions, you need switch to $GOPATH/src/k8s.io/kubernetes, checkout to the version you want upgrade to, and finally run the following commands to get the go modules expected version:\ncommit=$(TZ=UTC git --no-pager show --quiet --abbrev=12 --date='format-local:%Y%m%d%H%M%S' --format=\"%cd-%h\") echo \"v0.0.0-$commit\" After this, replace all kubernetes and staging versions (e.g. v0.0.0-20190815230911-4e7fd98763aa in above example) in go.mod.\nAlways run hack/update-dependencies.sh after changing go.mod by any of these methods (or adding new imports).\nSee golang's go.mod, Using Go Modules and Kubernetes Go modules docs for more details.\n","excerpt":"cloud-provider-azure uses go modules for Go dependency management. …","ref":"/cloud-provider-azure/development/dependencies/","title":"Dependency Management"},{"body":"NOTE This page only applies after Azure cloud provider implementation code has been moved to this repository.\nThere are some ongoing issues and pull requests addressing the Azure cloud provider in Kubernetes repository.\nWhen we turned to use the standalone cloud provider in this repository, those issues and pull requests should also be moved.\nHere are some notes for issues and pull requests migration.\nIssue migration If issue applies only to Azure cloud provider, please close it and create a new one in this repository.\nIf issue also involves other component, leave it there, but do create a new issue in this repository to track counterpater in Azure cloud provider.\nIn both cases, leave a link to the new created issue in the old issue.\nPull request migration Basically we are migrating code from k8s.io/legacy-cloud-providers/azure/ to github.com/kubernetes-sigs/cloud-provider-azure/cloud-controller-manager/azureprovider.\nThe following steps describe how to port an existing PR from kubernetes repository to this repository.\n Generate pull request patch  In your kubernetes repository, run following to generate a patch for your PR.\n PR_ID: Pull Request ID in kubernetes repository UPSTREAM_BRANCH: Branch name pointing to upstream, basically the branch with url https://github.com/kubernetes/kubernetes.git or https://k8s.io/kubernetes  PR_ID= UPSTREAM_BRANCH=origin PR_BRANCH_LOCAL=PR$PR_ID git fetch $UPSTREAM_BRANCH pull/$PR_ID/head:$PR_BRANCH_LOCAL MERGE_BASE=$(git merge-base $UPSTREAM_BRANCH/master $PR_BRANCH_LOCAL) PATCH_FILE=/tmp/${PR_ID}.patch git diff $MERGE_BASE $PR_BRANCH_LOCAL \u003e $PATCH_FILE git branch -D $PR_BRANCH_LOCAL Transform the patch and apply  Switch to kubernetes-azure-cloud-controller-manager repo. Apply the patch:\nhack/transform-patch.pl $PATCH_FILE | git apply If any of file in the patch does not fall under Azure cloud provider directory, the transform script will prompt a warning.\n","excerpt":"NOTE This page only applies after Azure cloud provider implementation …","ref":"/cloud-provider-azure/contribute/issues-and-pull-requests-migration/","title":"Issues and pull requests migration"},{"body":"","excerpt":"","ref":"/cloud-provider-azure/faq/known-issues/","title":"Known Issues"},{"body":"The AKS-engine supports deploying clusters with customized Cloud Controller Manager (CCM) and Cloud Node Manager (CNM) images. The API model is defined here. Follow this guide to build your CCM and CNM images and fill them into the AKS-engine API model.\nTo manually deploy an out-of-tree cluster, we need to deploy the following manifests. Note that there are some restrictions when setting config flags. To get more infomation, checkout this doc.\ncloud-controller-manager\ncloud-node-manager\nkube-apiserver\nkube-controller-manager\nkube-shedular\nkubelet command\n","excerpt":"The AKS-engine supports deploying clusters with customized Cloud …","ref":"/cloud-provider-azure/example/out-of-tree/","title":"Deploy with Out-of-tree Cloud Provider Azure"},{"body":"Azure cloud provider requires a set of permissions to manage the Azure resources. Here is a list of all permissions and reasons of why they're required.\n// Required to create, delete or update LoadBalancer for LoadBalancer service Microsoft.Network/loadBalancers/delete Microsoft.Network/loadBalancers/read Microsoft.Network/loadBalancers/write // Required to allow query, create or delete public IPs for LoadBalancer service Microsoft.Network/publicIPAddresses/delete Microsoft.Network/publicIPAddresses/read Microsoft.Network/publicIPAddresses/write // Required if public IPs from another resource group are used for LoadBalancer service // This is because of the linked access check when adding the public IP to LB frontendIPConfiguration Microsoft.Network/publicIPAddresses/join/action // Required to create or delete security rules for LoadBalancer service Microsoft.Network/networkSecurityGroups/read Microsoft.Network/networkSecurityGroups/write // Required to create, delete or update AzureDisks Microsoft.Compute/disks/delete Microsoft.Compute/disks/read Microsoft.Compute/disks/write Microsoft.Compute/locations/DiskOperations/read // Required to create, update or delete storage accounts for AzureFile or AzureDisk Microsoft.Storage/storageAccounts/delete Microsoft.Storage/storageAccounts/listKeys/action Microsoft.Storage/storageAccounts/read Microsoft.Storage/storageAccounts/write Microsoft.Storage/operations/read // Required to create, delete or update routeTables and routes for nodes Microsoft.Network/routeTables/read Microsoft.Network/routeTables/routes/delete Microsoft.Network/routeTables/routes/read Microsoft.Network/routeTables/routes/write Microsoft.Network/routeTables/write // Required to query information for VM (e.g. zones, faultdomain, size and data disks) Microsoft.Compute/virtualMachines/read // Required to attach AzureDisks to VM Microsoft.Compute/virtualMachines/write // Required to query information for vmssVM (e.g. zones, faultdomain, size and data disks) Microsoft.Compute/virtualMachineScaleSets/read Microsoft.Compute/virtualMachineScaleSets/virtualMachines/read Microsoft.Compute/virtualMachineScaleSets/virtualmachines/instanceView/read // Required to add VM to LoadBalancer backendAddressPools Microsoft.Network/networkInterfaces/write // Required to add vmss to LoadBalancer backendAddressPools Microsoft.Compute/virtualMachineScaleSets/write // Required to attach AzureDisks and add vmssVM to LB Microsoft.Compute/virtualMachineScaleSets/virtualmachines/write // Required to upgrade VMSS models to latest for all instances // only needed for Kubernetes 1.11.0-1.11.9, 1.12.0-1.12.8, 1.13.0-1.13.5, 1.14.0-1.14.1 Microsoft.Compute/virtualMachineScaleSets/manualupgrade/action // Required to query internal IPs and loadBalancerBackendAddressPools for VM Microsoft.Network/networkInterfaces/read // Required to query internal IPs and loadBalancerBackendAddressPools for vmssVM microsoft.Compute/virtualMachineScaleSets/virtualMachines/networkInterfaces/read // Required to get public IPs for vmssVM Microsoft.Compute/virtualMachineScaleSets/virtualMachines/networkInterfaces/ipconfigurations/publicipaddresses/read // Required to check whether subnet existing for ILB in another resource group Microsoft.Network/virtualNetworks/read Microsoft.Network/virtualNetworks/subnets/read // Required to create, update or delete snapshots for AzureDisk Microsoft.Compute/snapshots/delete Microsoft.Compute/snapshots/read Microsoft.Compute/snapshots/write // Required to get vm sizes for getting AzureDisk volume limit Microsoft.Compute/locations/vmSizes/read Microsoft.Compute/locations/operations/read ","excerpt":"Azure cloud provider requires a set of permissions to manage the Azure …","ref":"/cloud-provider-azure/topics/azure-permissions/","title":"Azure Permissions"},{"body":"Azure disk Container Storage Interface (CSI) Storage Plugin is moved to https://github.com/kubernetes-sigs/azuredisk-csi-driver. Please check the github link for the documentation.\n","excerpt":"Azure disk Container Storage Interface (CSI) Storage Plugin is moved …","ref":"/cloud-provider-azure/install/azuredisk/","title":"Deploy AzureDisk CSI Driver"},{"body":"","excerpt":"","ref":"/cloud-provider-azure/development/e2e/","title":"E2E tests"},{"body":"Release source There are two major code change sources for this project, either may push forward a new release for Kubernetes azure-cloud-controller-manager:\n  Changes in Kubernetes cloud-controller-manager, which happens in Kubernetes repository Since this project dependes on Kubernetes cloud-controller-manager, we'll periodically sync changes from Kubernetes upstream repository. When upstream shipped a new release tag, we may consider publishing a new release\n  Changes in Azure cloud provider, which happens directly in this repository Azure cloud provider also accepts new features and bug changes. In cases when a security fix is required or when the changes accumulated to certain amount, we may also consider publishing a new release, even if there is no change from Kubernetes upstream.\n  Versioning This project is a Kubernetes component whereas the functionalities and APIs all go with Kubernetes upstream project, thus we will use same versioning mechanism of Kubernetes, with some subtle differences for Azure cloud provider and non-Kubernetes changes.\nThe basic rule is:\n Every release version follows Semantic Versioning, in the form of MAJOR.MINOR.PATCH For MAJOR.MINOR, it keeps same value as the Kubernetes upstream For PATCH, it is calculated independently:  If upstream Kubernetes has a new a patch release, which introduces change in cloud-controller-manager or any component we depend on, then sync the change and increase the PATCH number. If any code change happens in Azure cloud provider or other dependency projects, which becomes eligible for a new release, then increase the PATCH number.    References:\n Kubernetes Release Versioning Semantic Versioning  Branch and version scheme This project uses golang's vendoring mechanism for managing dependencies (see Dependency management for detail). When talking about ‘sync from Kubernetes upstream’, it actually means vendoring Kubernetes repository code under the vendor directory.\nDuring each sync from upstream, it is usually fine to sync to latest commit. But if there is a new tagged commit in upstream that we haven't vendored, we should sync to that tagged commit first, and apply a version tag correspondingly if applicable. The version tag mechanism is a bit different on master branch and releasing branch, please see below for detail.\nThe upstream syncing change should be made in a single Pull Request. If in some case, the upstream change causes a test break, then the pull requests should not be merged until follow up fix commits are added.\nFor example, if upstream change adds a new cloud provider interface, syncing the upstream change may raise a test break, and we should add the implementation (even no-op) in same pull request.\nmaster branch This is the main development branch for merging pull requests. When upgrading dependencies, it will sync from Kubernetes upstream's master branch.\nFixes to releasing branches should be merged in master branch first, and then ported to corresponding release branch.\nVersion tags:\n X.Y.0-alpha.0  This is initial tag for a new release, it will be applied when a release branch is created. See below for detail   X.Y.0-alpha.W, W \u003e 0  Those version tags are periodically created if enough change accumulated. It does not have direct mapping with X.Y.0-alpha.W in Kubernetes upstream    releasing branch For release X.Y, the branch will have name release-X.Y. When upgrading dependencies, it will sync with Kubernetes upstream's release-X.Y branch. Release branch would be created when upstream release branch is created and first X.Y.0-beta.0 tag is applied.\nVersion tags:\n X.Y.0-beta.0  X.Y.0-beta.0 would be tagged at first independent commit on release branch, the corresponding separation point commit on master would be tagged X.Y+1.0-alpha.0 No new feature changes are allowed from this time on   X.Y.0-beta.W, W \u003e 0  Those version tags are periodically created if enough change accumulated. It does not have direct mapping with X.Y.0-beta.W in Kubernetes upstream   X.Y.0  This is the final release version. When upstream X.Y.0 tag rolls out, we will begin prepare X.Y.0 release After merging upstream X.Y.0 tag commit, we will run full test cycle to ensure the Azure cloud provider works well before release:  If any test fails, prepare fixes first. If the fix also applies to master branch, then also apply it to master. Rerun full test cycle till all tests got passed stablely Finally, apply X.Y.0 to latest commit of releasing branch   X.Y.1-beta.0 will be tagged at the same commit   X.Y.Z, Z \u003e 0  Those version tags are periodically created if enough change accumulated. It does not have direct mapping with X.Y.Z in Kubernetes upstream Testing and release process follows same rule as X.Y.0    CI and dev version scheme We use git-describe as versioning source, please check version for detail.\nIn this case, for commits that does not have a certain tag, the result version would be something like ‘v0.1.0-alpha.0-25-gd7999d10’.\n","excerpt":"Release source There are two major code change sources for this …","ref":"/cloud-provider-azure/contribute/release-versioning/","title":"Release Versioning"},{"body":"Feature Status: Alpha since v1.12.\nKubernetes v1.12 adds support for Azure availability zones (AZ). Nodes in availability zone will be added with label failure-domain.beta.kubernetes.io/zone=\u003cregion\u003e-\u003cAZ\u003e and topology-aware provisioning is added for Azure managed disks storage class.\nTOC:\n Availability Zones  Pre-requirements Node labels Load Balancer Managed Disks  StorageClass examples PV examples   Appendix Reference    Pre-requirements Because only standard load balancer is supported with AZ, it is a prerequisite to enable AZ for the cluster. It should be configured in Azure cloud provider configure file (e.g. /etc/kubernetes/azure.json):\n{ \"loadBalancerSku\": \"standard\", ... } If topology-aware provisioning feature is used, feature gate VolumeScheduling should be enabled on master components (e.g. kube-apiserver, kube-controller-manager and kube-scheduler).\nNode labels Both zoned and unzoned nodes are supported, but the value of node label failure-domain.beta.kubernetes.io/zone are different:\n For zoned nodes, the value is \u003cregion\u003e-\u003cAZ\u003e, e.g. centralus-1. For unzoned nodes, the value is faultDomain, e.g. 0.  e.g.\n$kubectlgetnodes--show-labelsNAMESTATUSAGEVERSIONLABELSkubernetes-node12Ready6mv1.11failure-domain.beta.kubernetes.io/region=centralus,failure-domain.beta.kubernetes.io/zone=centralus-1,...Load Balancer loadBalancerSku has been set to standard in cloud provider configure file, so standard load balancer and standard public IPs will be provisioned automatically for services with type LoadBalancer. Both load balancer and public IPs are zone redundant.\nManaged Disks Zone-aware and topology-aware provisioning are supported for Azure managed disks. To support these features, a few options are added in AzureDisk storage class:\n zoned: indicates whether new disks are provisioned with AZ. Default is true. allowedTopologies: indicates which topologies are allowed for topology-aware provisioning. Only can be set if zoned is not false.  StorageClass examples An example of zone-aware provisioning storage class is:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:annotations:labels:kubernetes.io/cluster-service:\"true\"name:managed-premiumparameters:kind:Managedstorageaccounttype:Premium_LRSzoned:\"true\"provisioner:kubernetes.io/azure-diskvolumeBindingMode:WaitForFirstConsumerAnother example of topology-aware provisioning storage class is:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:annotations:labels:kubernetes.io/cluster-service:\"true\"name:managed-premiumparameters:kind:Managedstorageaccounttype:Premium_LRSprovisioner:kubernetes.io/azure-diskvolumeBindingMode:WaitForFirstConsumerallowedTopologies:-matchLabelExpressions:-key:failure-domain.beta.kubernetes.io/zonevalues:-centralus-1-centralus-2PV examples When feature gate VolumeScheduling disabled, no NodeAffinity set for zoned PV:\n$ kubectl describe pv Name: pvc-d30dad05-9ad8-11e8-94f2-000d3a07de8c Labels: failure-domain.beta.kubernetes.io/region=southeastasia failure-domain.beta.kubernetes.io/zone=southeastasia-2 Annotations: pv.kubernetes.io/bound-by-controller=yes pv.kubernetes.io/provisioned-by=kubernetes.io/azure-disk volumehelper.VolumeDynamicallyCreatedByKey=azure-disk-dynamic-provisioner Finalizers: [kubernetes.io/pv-protection] StorageClass: default Status: Bound Claim: default/pvc-azuredisk Reclaim Policy: Delete Access Modes: RWO Capacity: 5Gi Node Affinity: Required Terms: Term 0: failure-domain.beta.kubernetes.io/region in [southeastasia] failure-domain.beta.kubernetes.io/zone in [southeastasia-2] Message: Source: Type: AzureDisk (an Azure Data Disk mount on the host and bind mount to the pod) DiskName: k8s-5b3d7b8f-dynamic-pvc-d30dad05-9ad8-11e8-94f2-000d3a07de8c DiskURI: /subscriptions/\u003csubscription-id\u003e/resourceGroups/\u003crg-name\u003e/providers/Microsoft.Compute/disks/k8s-5b3d7b8f-dynamic-pvc-d30dad05-9ad8-11e8-94f2-000d3a07de8c Kind: Managed FSType: CachingMode: None ReadOnly: false Events: \u003cnone\u003e When feature gate VolumeScheduling enabled, NodeAffinity will be populated for zoned PV:\n$ kubectl describe pv Name: pvc-0284337b-9ada-11e8-a7f6-000d3a07de8c Labels: failure-domain.beta.kubernetes.io/region=southeastasia failure-domain.beta.kubernetes.io/zone=southeastasia-2 Annotations: pv.kubernetes.io/bound-by-controller=yes pv.kubernetes.io/provisioned-by=kubernetes.io/azure-disk volumehelper.VolumeDynamicallyCreatedByKey=azure-disk-dynamic-provisioner Finalizers: [kubernetes.io/pv-protection] StorageClass: default Status: Bound Claim: default/pvc-azuredisk Reclaim Policy: Delete Access Modes: RWO Capacity: 5Gi Node Affinity: Required Terms: Term 0: failure-domain.beta.kubernetes.io/region in [southeastasia] failure-domain.beta.kubernetes.io/zone in [southeastasia-2] Message: Source: Type: AzureDisk (an Azure Data Disk mount on the host and bind mount to the pod) DiskName: k8s-5b3d7b8f-dynamic-pvc-0284337b-9ada-11e8-a7f6-000d3a07de8c DiskURI: /subscriptions/\u003csubscription-id\u003e/resourceGroups/\u003crg-name\u003e/providers/Microsoft.Compute/disks/k8s-5b3d7b8f-dynamic-pvc-0284337b-9ada-11e8-a7f6-000d3a07de8c Kind: Managed FSType: CachingMode: None ReadOnly: false Events: \u003cnone\u003e While unzoned disks are not able to attach in zoned nodes, NodeAffinity will also be set for them so that they will only be scheduled to unzoned nodes:\n$ kubectl describe pv pvc-bdf93a67-9c45-11e8-ba6f-000d3a07de8c Name: pvc-bdf93a67-9c45-11e8-ba6f-000d3a07de8c Labels: \u003cnone\u003e Annotations: pv.kubernetes.io/bound-by-controller=yes pv.kubernetes.io/provisioned-by=kubernetes.io/azure-disk volumehelper.VolumeDynamicallyCreatedByKey=azure-disk-dynamic-provisioner Finalizers: [kubernetes.io/pv-protection] StorageClass: azuredisk-unzoned Status: Bound Claim: default/unzoned-pvc Reclaim Policy: Delete Access Modes: RWO Capacity: 5Gi Node Affinity: Required Terms: Term 0: failure-domain.beta.kubernetes.io/region in [southeastasia] failure-domain.beta.kubernetes.io/zone in [0] Term 1: failure-domain.beta.kubernetes.io/region in [southeastasia] failure-domain.beta.kubernetes.io/zone in [1] Term 2: failure-domain.beta.kubernetes.io/region in [southeastasia] failure-domain.beta.kubernetes.io/zone in [2] Message: Source: Type: AzureDisk (an Azure Data Disk mount on the host and bind mount to the pod) DiskName: k8s-5b3d7b8f-dynamic-pvc-bdf93a67-9c45-11e8-ba6f-000d3a07de8c DiskURI: /subscriptions/\u003csubscription\u003e/resourceGroups/\u003crg-name\u003e/providers/Microsoft.Compute/disks/k8s-5b3d7b8f-dynamic-pvc-bdf93a67-9c45-11e8-ba6f-000d3a07de8c Kind: Managed FSType: CachingMode: None ReadOnly: false Events: \u003cnone\u003e Appendix Note that unlike most cases, fault domain and availability zones mean different on Azure:\n A Fault Domain (FD) is essentially a rack of servers. It consumes subsystems like network, power, cooling etc. Availability Zones are unique physical locations within an Azure region. Each zone is made up of one or more data centers equipped with independent power, cooling, and networking.  An Availability Zone in an Azure region is a combination of a fault domain, and an update domain (Same like FD, but for updates. When upgrading a deployment, it is carried out one update domain at a time). For example, if you create three or more VMs across three zones in an Azure region, your VMs are effectively distributed across three fault domains and three update domains.\nReference See design docs for AZ in KEP for Azure availability zones.\n","excerpt":"Feature Status: Alpha since v1.12.\nKubernetes v1.12 adds support for …","ref":"/cloud-provider-azure/topics/availability-zones/","title":"Use Availability Zones"},{"body":"Azure file Container Storage Interface (CSI) Storage Plugin is moved to https://github.com/kubernetes-sigs/azurefile-csi-driver. Please check the github link for the documentation.\n","excerpt":"Azure file Container Storage Interface (CSI) Storage Plugin is moved …","ref":"/cloud-provider-azure/install/azurefile/","title":"Deploy AzureFile CSI Driver"},{"body":"To be completed.\n","excerpt":"To be completed.\n","ref":"/cloud-provider-azure/development/design-docs/","title":"Design Docs and KEPs"},{"body":"Security Announcements Join the kubernetes-security-announce group for security and vulnerability announcements.\nYou can also subscribe to an RSS feed of the above using this link.\nReporting a Vulnerability Instructions for reporting a vulnerability can be found on the Kubernetes Security and Disclosure Information page.\nSupported Versions Information about supported Kubernetes versions can be found on the Kubernetes version and version skew support policy page on the Kubernetes website.\n","excerpt":"Security Announcements Join the kubernetes-security-announce group for …","ref":"/cloud-provider-azure/contribute/security/","title":"Security Policy"},{"body":"Feature status: Alpha since v1.12.\nKubernetes v1.12 adds support for cross resource group (RG) nodes and unmanaged (such as on-prem) nodes in Azure cloud provider. A few assumptions are made for such nodes:\n Cross-RG nodes are in same region and set with required labels (as clarified in the following part) Nodes will not be part of the load balancer managed by cloud provider Both node and container networking should be configured properly by provisioning tools AzureDisk is supported for Azure cross-RG nodes, but not for on-prem nodes  TOC:\n Cross resource group nodes  Pre-requirements Cross-RG nodes Unmanaged nodes Reference    Pre-requirements Because cross-RG nodes and unmanaged nodes won't be added to Azure load balancer backends, feature gate ServiceNodeExclusion should be enabled for master components (e.g. kube-controller-manager).\nCross-RG nodes Cross-RG nodes should register themselves with required labels together with cloud provider:\n node.kubernetes.io/exclude-balancer, which is used to exclude the node from load balancer.  alpha.service-controller.kubernetes.io/exclude-balancer=true should be used if the cluster version is below v1.16.0.   kubernetes.azure.com/resource-group=\u003crg-name\u003e, which provides external RG and is used to get node information. cloud provider config  --cloud-provider=azure when using kube-controller-manager --cloud-provider=external when using cloud-controller-manager    For example,\nkubelet ... \\  --cloud-provider=azure \\  --cloud-config=/etc/kubernetes/azure.json \\  --node-labels=node.kubernetes.io/exclude-balancer=true,kubernetes.azure.com/resource-group=\u003crg-name\u003e Unmanaged nodes On-prem nodes are different from Azure nodes, all Azure coupled features (such as load balancers and Azure managed disks) are not supported for them. To prevent the node being deleted, Azure cloud provider will always assumes the node existing.\nOn-prem nodes should register themselves with labels node.kubernetes.io/exclude-balancer=true and kubernetes.azure.com/managed=false:\n node.kubernetes.io/exclude-balancer=true, which is used to exclude the node from load balancer. kubernetes.azure.com/managed=false, which indicates the node is on-prem or on other clouds.  For example,\nkubelet ...\\  --cloud-provider= \\  --node-labels=node.kubernetes.io/exclude-balancer=true,kubernetes.azure.com/managed=false Reference See design docs for cross resource group nodes in KEP 20180809-cross-resource-group-nodes.\n","excerpt":"Feature status: Alpha since v1.12.\nKubernetes v1.12 adds support for …","ref":"/cloud-provider-azure/topics/cross-resource-group-nodes/","title":"Deploy Cross Resource Group Nodes"},{"body":"To be completed.\n","excerpt":"To be completed.\n","ref":"/cloud-provider-azure/development/future/","title":"Future Plans"},{"body":" Note that this feature is supported since v1.20.0.\n Provider Azure supports sharing one IP address among multiple load balancer typed external or internal services. To share an IP address among multiple public services, a public IP resource is needed. This public IP could be created in advance or let the cloud provider provision it when creating the first external service. Specifically, Azure would create a public IP resource automatically when an external service is discovered.\napiVersion:v1kind:Servicemetadata:name:nginxnamespace:defaultspec:ports:-port:80protocol:TCPtargetPort:80selector:app:nginxtype:LoadBalancerNote that the loadBalancerIP is not set, or Azure would find a pre-allocated public IP with the address. After obtaining the IP address of the service, you could create other services using this address.\napiVersion:v1kind:Servicemetadata:name:httpsnamespace:defaultspec:loadBalancerIP:1.2.3.4# the IP address could be the same as it is of `nginx` serviceports:-port:443protocol:TCPtargetPort:443selector:app:httpstype:LoadBalancerNote that if you specify the loadBalancerIP but there is no corresponding public IP pre-allocated, an error would be reported.\n","excerpt":" Note that this feature is supported since v1.20.0.\n Provider Azure …","ref":"/cloud-provider-azure/topics/shared-ip/","title":"Multiple Services Sharing One IP Address"},{"body":"Major changes since v0.5.0   Update vendor against k/k release-1.19(#385) Increase the e2e test coverage for cluster autoscaler(#364) Use hugo to generate doc website(#358) Update E2E test related docs and script(#355) Partly decouple k/k(#350) Update go module against k8s.io/cloud-provider(#348) Use distroless/static as base image(#333) Enable running ccm e2e test in a job(#345)  The image is available at\n mcr.microsoft.com/oss/kubernetes/azure-cloud-node-manager:v0.6.0 mcr.microsoft.com/oss/kubernetes/azure-cloud-controller-manager:v0.6.0  Since v0.5.0, our docs are moved to a dedicated website and the docs/ directory is deprecated.\n","excerpt":"Major changes since v0.5.0   Update vendor against k/k …","ref":"/cloud-provider-azure/blog/2020/09/01/v0.6.0/","title":"v0.6.0"},{"body":"Changes since v0.5.0   Update Kubernetes vendor to adopt bug fixes from in-tree cloud provider(#330) Use a service account for CCM (#329) Update images for out-of-tree examples (#328) Fix wrong init url for kubemark tests (#327)  The image is available at\n mcr.microsoft.com/oss/kubernetes/azure-cloud-node-manager:v0.5.1 mcr.microsoft.com/oss/kubernetes/azure-cloud-controller-manager:v0.5.1  ","excerpt":"Changes since v0.5.0   Update Kubernetes vendor to adopt bug fixes …","ref":"/cloud-provider-azure/blog/2020/04/27/v0.5.1/","title":"v0.5.1"},{"body":"The alpha version of azure-cloud-controller-manager and azure-cloud-node-manager, which have upgraded Kubernetes version to v1.18. It also adds Windows support for azure-cloud-node-manager.\nPlease see docs for documentation.\nThe image is available at\n mcr.microsoft.com/oss/kubernetes/azure-cloud-node-manager:v0.5.0 mcr.microsoft.com/oss/kubernetes/azure-cloud-controller-manager:v0.5.0  ","excerpt":"The alpha version of azure-cloud-controller-manager and …","ref":"/cloud-provider-azure/blog/2020/03/27/v0.5.0/","title":"v0.5.0"},{"body":"The alpha version of azure-cloud-controller-manager and azure-cloud-node-manager, which fixes the node address update issues.\nThe image is available at\n mcr.microsoft.com/oss/kubernetes/azure-cloud-node-manager:v0.4.1 mcr.microsoft.com/oss/kubernetes/azure-cloud-controller-manager:v0.4.1  ","excerpt":"The alpha version of azure-cloud-controller-manager and …","ref":"/cloud-provider-azure/blog/2019/12/30/v0.4.1/","title":"v0.4.1"},{"body":"The alpha version of azure-cloud-controller-manager and azure-cloud-node-manager, which have upgraded Kubernetes version to v1.17.\nPlease see docs for documentation.\nThe image is available at\n mcr.microsoft.com/oss/kubernetes/azure-cloud-node-manager:v0.4.0 mcr.microsoft.com/oss/kubernetes/azure-cloud-controller-manager:v0.4.0  ","excerpt":"The alpha version of azure-cloud-controller-manager and …","ref":"/cloud-provider-azure/blog/2019/12/17/v0.4.0/","title":"v0.4.0"},{"body":"The alpha version of azure-cloud-controller-manager and azure-cloud-node-manager, which have upgraded Kubernetes version to v1.16.\nPlease see docs for documentation.\nThe image is available at\n mcr.microsoft.com/oss/kubernetes/azure-cloud-controller-manager:v0.3.0 mcr.microsoft.com/oss/kubernetes/azure-cloud-node-manager:v0.3.0  ","excerpt":"The alpha version of azure-cloud-controller-manager and …","ref":"/cloud-provider-azure/blog/2019/09/24/v0.3.0/","title":"v0.3.0"},{"body":"The alpha version of azure-cloud-controller-manager, which has upgraded Kubernetes version to v1.15.0.\nPlease see docs for documentation.\nThe image is available at mcr.microsoft.com/k8s/core/azure-cloud-controller-manager:v0.2.0.\n","excerpt":"The alpha version of azure-cloud-controller-manager, which has …","ref":"/cloud-provider-azure/blog/2019/06/27/v0.2.0/","title":"v0.2.0"},{"body":"The alpha version of azure-cloud-controller-manager. Please see docs for documentation.\nThe image is available at mcr.microsoft.com/k8s/core/azure-cloud-controller-manager:v0.1.0.\n","excerpt":"The alpha version of azure-cloud-controller-manager. Please see docs …","ref":"/cloud-provider-azure/blog/2019/03/26/v0.1.0/","title":"v0.1.0"},{"body":"Overview Here provides some E2E tests only specific to Azure provider.\nPrerequisite Deploy a Kubernetes cluster with Azure CCM Refer step 1-3 in e2e-tests for deploying the Kubernetes cluster.\nSetup Azure credentials export K8S_AZURE_TENANTID=\u003ctenant-id\u003e # the tenant ID export K8S_AZURE_SUBSID=\u003csubscription-id\u003e # the subscription ID export K8S_AZURE_SPID=\u003cservice-principal-id\u003e # the service principal ID export K8S_AZURE_SPSEC=\u003cservice-principal-secret\u003e # the service principal secret export K8S_AZURE_ENVIRONMENT=\u003cAzurePublicCloud\u003e # the cloud environment (optional, default is AzurePublicCloud) export K8S_AZURE_LOCATION=\u003clocation\u003e # the location export K8S_AZURE_LOADBALANCE_SKU=\u003cloadbalancer-sku\u003e # the sku of load balancer (optional, default is basic) Setup KUBECONFIG   Locate your kubeconfig and set it as env variable export KUBECONFIG=\u003ckubeconfig\u003e or cp \u003ckubeconfig\u003e ~/.kube/config\n  Test it via kubectl version\n  Run Test Have installed ginkgo   Run ginkgo ./tests/e2e/ \nFor more usage of ginkgo, please follow ginkgo\n  Without ginkgo  Run go test ./tests/e2e/ -timeout 0  After a long time test, a JUnit report will be generated in a directory named by the cluster name\n","excerpt":"Overview Here provides some E2E tests only specific to Azure provider. …","ref":"/cloud-provider-azure/development/e2e/e2e-tests-azure/","title":"Azure E2E tests"},{"body":" azure disk plugin known issues  Recommended stable version for azure disk 1. disk attach error 2. disk unavailable after attach/detach a data disk on a node 3. Azure disk support on Sovereign Cloud 4. Time cost for Azure Disk PVC mount 5. Azure disk PVC Multi-Attach error, makes disk mount very slow or mount failure forever 6. WaitForAttach failed for azure disk: parsing “/dev/disk/azure/scsi1/lun1”: invalid syntax 7. uid and gid setting in azure disk 8. Addition of a blob based disk to VM with managed disks is not supported 9. dynamic azure disk PVC try to access wrong storage account (of other resource group) 10. data loss if using existing azure disk with partitions in disk mount 11. Delete azure disk PVC which is already in use by a pod 12. create azure disk PVC failed due to account creation failure 13. cannot find Lun for disk 14. azure disk attach/detach failure, mount issue, i/o error    Recommended stable version for azure disk    k8s version stable version     v1.7 1.7.14 or later   v1.8 1.8.13 or later   v1.9 1.9.7 or later (1.9.6 on AKS)   v1.10 1.10.12 or later   v1.11 1.11.6 or later   v1.12 1.12.4 or later   v1.13 1.13.0    1. disk attach error Issue details:\nIn some corner case(detaching multiple disks on a node simultaneously), when scheduling a pod with azure disk mount from one node to another, there could be lots of disk attach error(no recovery) due to the disk not being released in time from the previous node. This issue is due to lack of lock before DetachDisk operation, actually there should be a central lock for both AttachDisk and DetachDisk operations, only one AttachDisk or DetachDisk operation is allowed at one time.\nThe disk attach error could be like following:\nCannot attach data disk 'cdb-dynamic-pvc-92972088-11b9-11e8-888f-000d3a018174' to VM 'kn-edge-0' because the disk is currently being detached or the last detach operation failed. Please wait until the disk is completely detached and then try again or delete/detach the disk explicitly again. Related issues\n Azure Disk Detach are not working with multiple disk detach on the same Node Azure disk fails to attach and mount, causing rescheduled pod to stall following node disruption Since Intel CPU Azure update, new Azure Disks are not mounting, very critical…  Busy azure-disk regularly fail to mount causing K8S Pod deployments to halt  Mitigation:\n option#1: Update every agent node that has attached or detached the disk in problem  In Azure cloud shell, run\n$vm = Get-AzureRMVM -ResourceGroupName $rg -Name $vmname Update-AzureRmVM -ResourceGroupName $rg -VM $vm -verbose -debug In Azure cli, run\naz vm update -g \u003cgroup\u003e -n \u003cname\u003e  option#2:   kubectl cordon node #make sure no scheduling on this node kubectl drain node #schedule pod in current node to other node restart the Azure VM for node via the API or portal, wait until VM is “Running” kubectl uncordon node  Fix\n PR fix race condition issue when detaching azure disk has fixed this issue by add a lock before DetachDisk     k8s version fixed version     v1.6 no fix since v1.6 does not accept any cherry-pick   v1.7 1.7.14   v1.8 1.8.9   v1.9 1.9.5   v1.10 1.10.0    2. disk unavailable after attach/detach a data disk on a node  💡 NOTE: Azure platform has fixed the host cache issue, the suggested host cache setting of data disk is ReadOnly now, more details about azure disk cache setting Issue details:\n From k8s v1.7, default host cache setting changed from None to ReadWrite, this change would lead to device name change after attach multiple disks on a node, finally lead to disk unavailable from pod. When access data disk inside a pod, will get following error:\n[root@admin-0 /]# ls /datadisk ls: reading directory .: Input/output error In my testing on Ubuntu 16.04 D2_V2 VM, when attaching the 6th data disk will cause device name change on agent node, e.g. following lun0 disk should be sdc other than sdk.\nazureuser@k8s-agentpool2-40588258-0:~$ tree /dev/disk/azure ... â””â”€â”€ scsi1 â”œâ”€â”€ lun0 -\u003e ../../../sdk â”œâ”€â”€ lun1 -\u003e ../../../sdj â”œâ”€â”€ lun2 -\u003e ../../../sde â”œâ”€â”€ lun3 -\u003e ../../../sdf â”œâ”€â”€ lun4 -\u003e ../../../sdg â”œâ”€â”€ lun5 -\u003e ../../../sdh â””â”€â”€ lun6 -\u003e ../../../sdi Related issues\n device name change due to azure disk host cache setting unable to use azure disk in StatefulSet since /dev/sd* changed after detach/attach disk Disk error when pods are mounting a certain amount of volumes on a node unable to use azure disk in StatefulSet since /dev/sd* changed after detach/attach disk Input/output error when accessing PV PersistentVolumeClaims changing to Read-only file system suddenly  Workaround:\n add cachingmode: None in azure disk storage class(default is ReadWrite), e.g.  kind:StorageClassapiVersion:storage.k8s.io/v1metadata:name:hddprovisioner:kubernetes.io/azure-diskparameters:skuname:Standard_LRSkind:Managedcachingmode:NoneFix\n PR fix device name change issue for azure disk could fix this issue too, it will change default cachingmode value from ReadWrite to None.     k8s version fixed version     v1.6 no such issue as cachingmode is already None by default   v1.7 1.7.14   v1.8 1.8.11   v1.9 1.9.4   v1.10 1.10.0    3. Azure disk support on Sovereign Cloud Fix\n PR Azure disk on Sovereign Cloud fixed this issue     k8s version fixed version     v1.7 1.7.9   v1.8 1.8.3   v1.9 1.9.0   v1.10 1.10.0    4. Time cost for Azure Disk PVC mount Original time cost for Azure Disk PVC mount on a standard node size(e.g. Standard_D2_V2) is around 1 minute, podAttachAndMountTimeout is 2 minutes, total waitForAttachTimeout is 10 minutes, so a disk remount(detach and attach in sequential) would possibly cost more than 2min, thus may fail.\n Note: for some smaller VM size which has only 1 CPU core, time cost would be much bigger(e.g. \u003e 10min) since container is hard to get CPU slot.\n Related issues\n ‘timeout expired waiting for volumes to attach/mount for pod when cluster’ when node-vm-size is Standard_B1s  Fix\n PR using cache fix fixed this issue, which could reduce the mount time cost to around 30s.     k8s version fixed version     v1.8 no fix   v1.9 1.9.2   v1.10 1.10.0    5. Azure disk PVC Multi-Attach error, makes disk mount very slow or mount failure forever Issue details:\nWhen schedule a pod with azure disk volume from one node to another, total time cost of detach \u0026 attach is around 1 min from v1.9.2, while in v1.9.x, there is an UnmountDevice failure issue in containerized kubelet which makes disk mount very slow or mount failure forever, this issue only exists in v1.9.x due to PR Refactor nsenter, v1.10.0 won't have this issue since devicePath is updated in v1.10 code\nerror logs:\n kubectl describe po POD-NAME  Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 3m default-scheduler Successfully assigned deployment-azuredisk1-6cd8bc7945-kbkvz to k8s-agentpool-88970029-0 Warning FailedAttachVolume 3m attachdetach-controller Multi-Attach error for volume \"pvc-6f2d0788-3b0b-11e8-a378-000d3afe2762\" Volume is already exclusively attached to one node and can't be attached to another Normal SuccessfulMountVolume 3m kubelet, k8s-agentpool-88970029-0 MountVolume.SetUp succeeded for volume \"default-token-qt7h6\" Warning FailedMount 1m kubelet, k8s-agentpool-88970029-0 Unable to mount volumes for pod \"deployment-azuredisk1-6cd8bc7945-kbkvz_default(5346c040-3e4c-11e8-a378-000d3afe2762)\": timeout expired waiting for volumes to attach/mount for pod \"default\"/\"deployment-azuredisk1-6cd8bc7945-kbkvz\". list of unattached/unmounted volumes=[azuredisk]  kubelet logs from the new node  E0412 20:08:10.920284 7602 nestedpendingoperations.go:263] Operation for \"\\\"kubernetes.io/azure-disk//subscriptions/xxx/resourceGroups/MC_xxx_eastus/providers/Microsoft.Compute/disks/kubernetes-dynamic-pvc-11035a31-3e8d-11e8-82ec-0a58ac1f04cf\\\"\" failed. No retries permitted until 2018-04-12 20:08:12.920234762 +0000 UTC m=+1467.278612421 (durationBeforeRetry 2s). Error: \"Volume has not been added to the list of VolumesInUse in the node's volume status for volume \\\"pvc-11035a31-3e8d-11e8-82ec-0a58ac1f04cf\\\" (UniqueName: \\\"kubernetes.io/azure-disk//subscriptions/xxx/resourceGroups/MC_xxx_eastus/providers/Microsoft.Compute/disks/kubernetes-dynamic-pvc-11035a31-3e8d-11e8-82ec-0a58ac1f04cf\\\") pod \\\"symbiont-node-consul-0\\\" (UID: \\\"11043b12-3e8d-11e8-82ec-0a58ac1f04cf\\\") \" Related issues\n UnmountDevice would fail in containerized kubelet upgrade k8s process is broke  Mitigation:\nIf azure disk PVC mount successfully in the end, there is no action, while if it could not be mounted for more than 20min, following actions could be taken:\n check whether volumesInUse list has unmounted azure disks, run:  kubectl get no NODE-NAME -o yaml \u003e node.log all volumes in volumesInUse should be also in volumesAttached, otherwise there would be issue\n restart kubelet on the original node would solve this issue: sudo kubectl kubelet restart  Fix\n PR fix nsenter GetFileType issue in containerized kubelet fixed this issue     k8s version fixed version     v1.8 no such issue   v1.9 v1.9.7   v1.10 no such issue    After fix in v1.9.7, it took about 1 minute for scheduling one azure disk mount from one node to another, you could find details here.\nSince azure disk attach/detach operation on a VM cannot be parallel, scheduling 3 azure disk mounts from one node to another would cost about 3 minutes.\n6. WaitForAttach failed for azure disk: parsing “/dev/disk/azure/scsi1/lun1”: invalid syntax Issue details: MountVolume.WaitForAttach may fail in the azure disk remount\nerror logs:\nin v1.10.0 \u0026 v1.10.1, MountVolume.WaitForAttach will fail in the azure disk remount, error logs would be like following:\n incorrect DevicePath format on Linux  MountVolume.WaitForAttach failed for volume \"pvc-f1562ecb-3e5f-11e8-ab6b-000d3af9f967\" : azureDisk - Wait for attach expect device path as a lun number, instead got: /dev/disk/azure/scsi1/lun1 (strconv.Atoi: parsing \"/dev/disk/azure/scsi1/lun1\": invalid syntax) Warning FailedMount 1m (x10 over 21m) kubelet, k8s-agentpool-66825246-0 Unable to mount volumes for pod  wrong DevicePath(LUN) number on Windows   Warning FailedMount 1m kubelet, 15282k8s9010 MountVolume.WaitForAttach failed for volume \"disk01\" : azureDisk - WaitForAttach failed within timeout node (15282k8s9010) diskId:(andy-mghyb 1102-dynamic-pvc-6c526c51-4a18-11e8-ab5c-000d3af7b38e) lun:(4) Related issues\n WaitForAttach failed for azure disk: parsing “/dev/disk/azure/scsi1/lun1”: invalid syntax Pod unable to attach PV after being deleted (Wait for attach expect device path as a lun number, instead got: /dev/disk/azure/scsi1/lun0 (strconv.Atoi: parsing “/dev/disk/azure/scsi1/lun0”: invalid syntax)  Fix\n PR fix WaitForAttach failure issue for azure disk fixed this issue     k8s version fixed version     v1.8 no such issue   v1.9 no such issue   v1.10 1.10.2    7. uid and gid setting in azure disk Issue details: Unlike azure file mountOptions, you will get following failure if set mountOptions like uid=999,gid=999 in azure disk mount:\nazureDisk - mountDevice:FormatAndMount failed with exit status 32 That's because azureDisk use ext4 file system by default, mountOptions like [uid=x,gid=x] could not be set in mount time.\nRelated issues\n Timeout expired waiting for volumes to attach  Solution:\n option#1: Set uid in runAsUser and gid in fsGroup for pod: security context for a Pod  e.g. Following setting will set pod run as root, make it accessible to any file:\napiVersion:v1kind:Podmetadata:name:security-context-demospec:securityContext:runAsUser:0fsGroup:0 Note: Since gid \u0026 uid is mounted as 0(root) by default, if set as non-root(e.g. 1000), k8s will use chown to change all dir/files under that disk, this is a time consuming job, which would make mount device very slow, in this issue: Timeout expired waiting for volumes to attach, it costs about 10 min for chown operation complete.\n  option#2: use chown in initContainers  initContainers: - name: volume-mount image: busybox command: [\"sh\", \"-c\", \"chown -R 100:100 /data\"] volumeMounts: - name: \u003cyour data volume\u003e mountPath: /data 8. Addition of a blob based disk to VM with managed disks is not supported Issue details:\nFollowing error may occur if attach a blob based(unmanaged) disk to VM with managed disks:\nWarning FailedMount 42s (x2 over 1m) attachdetach AttachVolume.Attach failed for volume \"pvc-f17e5e77-474e-11e8-a2ea-000d3a10df6d\" : Attach volume \"holo-k8s-dev-dynamic-pvc-f17e5e77-474e-11e8-a2ea-000d3a10df6d\" to instance \"k8s-master-92699158-0\" failed with compute.VirtualMachinesClient#CreateOrUpdate: Failure responding to request: StatusCode=409 -- Original Error: autorest/azure: Service returned an error. Status=409 Code=\"OperationNotAllowed\" Message=\"Addition of a blob based disk to VM with managed disks is not supported.\" This issue is by design as in Azure, there are two kinds of disks, blob based(unmanaged) disk and managed disk, an Azure VM could not attach both of these two kinds of disks.\nSolution:\nUse default azure disk storage class in aks-engine, as default will always be identical to the agent pool, that is, if VM is managed, it will be managed azure disk class, if unmanaged, then it's unmanaged disk class.\n9. dynamic azure disk PVC try to access wrong storage account (of other resource group) Issue details:\nIn a k8s cluster with blob based VMs(won't happen in AKS since AKS only use managed disk), create dynamic azure disk PVC may fail, error logs is like following:\nFailed to provision volume with StorageClass \"default\": azureDisk - account ds6c822a4d484211eXXXXXX does not exist while trying to create/ensure default container Related issues\n Multiple clusters - dynamic PVCs try to access wrong storage account (of other resource group)  Fix\n PR fix storage account not found issue: use ListByResourceGroup instead of List() fixed this issue     k8s version fixed version     v1.8 1.8.13   v1.9 1.9.9   v1.10 no such issue    Work around:\nthis bug only exists in blob based VM in v1.8.x, v1.9.x, so if specify ManagedDisks when creating k8s cluster in aks-engine(AKS is using managed disk by default), it won't have this issue:\n\"agentPoolProfiles\": [ { ... \"storageProfile\" : \"ManagedDisks\", ... } 10. data loss if using existing azure disk with partitions in disk mount Issue details:\nWhen use an existing azure disk(also called static provisioning) in pod, if that disk has partitions, the disk will be formatted in the pod mounting process, actually k8s volume don't support mount disk with partitions, disk mount would fail finally. While for mounting existing azure disk that has partitions, data will be lost since it will format that disk first. This issue happens only on Linux.\nRelated issues\n data loss if using existing azure disk with partitions in disk mount  Fix\n PR fix data loss issue if using existing azure disk with partitions in disk mount will let azure provider return error when mounting existing azure disk that has partitions     k8s version fixed version     v1.8 1.8.15   v1.9 1.9.11   v1.10 1.10.5   v1.11 1.11.0    Work around:\nDon't use existing azure disk that has partitions, e.g. following disk in LUN 0 that has one partition:\nazureuser@aks-nodepool1-28371372-0:/$ ls -l /dev/disk/azure/scsi1/ total 0 lrwxrwxrwx 1 root root 12 Apr 27 08:04 lun0 -\u003e ../../../sdc lrwxrwxrwx 1 root root 13 Apr 27 08:04 lun0-part1 -\u003e ../../../sdc1 11. Delete azure disk PVC which is already in use by a pod Issue details:\nFollowing error may occur if delete azure disk PVC which is already in use by a pod:\nkubectl describe pv pvc-d8eebc1d-74d3-11e8-902b-e22b71bb1c06 ... Message: disk.DisksClient#Delete: Failure responding to request: StatusCode=409 -- Original Error: autorest/azure: Service returned an error. Status=409 Code=\"OperationNotAllowed\" Message=\"Disk kubernetes-dynamic-pvc-d8eebc1d-74d3-11e8-902b-e22b71bb1c06 is attached to VM /subscriptions/{subs-id}/resourceGroups/MC_markito-aks-pvc_markito-aks-pvc_westus/providers/Microsoft.Compute/virtualMachines/aks-agentpool-25259074-0.\" Fix:\nThis is a common k8s issue, other cloud provider would also has this issue. There is a PVC protection feature to prevent this, it's alpha in v1.9, and beta(enabled by default) in v1.10\nWork around: delete pod first and then delete azure disk pvc after a few minutes\n12. create azure disk PVC failed due to account creation failure  please note this issue only happens on unmanaged k8s cluster\n Issue details: User may get Account property kind is invalid for the request error when trying to create a new unmanaged azure disk PVC, error would be like following:\nazureuser@k8s-master-17140924-0:/tmp$ kubectl describe pvc Name: pvc-azuredisk Namespace: default StorageClass: hdd Status: Bound ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning ProvisioningFailed 31m persistentvolume-controller Failed to provision volume with StorageClass \"hdd\": Create Storage Account: ds10e15ed89c5811e8a0a70, error: storage.AccountsClient#Create: Failure sending request: StatusCode=400 -- Original Error: Code=\"AccountPropertyIsInvalid\" Message=\"Account property kind is invalid for the request.\" Fix\n PR fix azure disk create failure due to sdk upgrade fixed this issue     k8s version fixed version     v1.9 no such issue   v1.10 no such issue   v1.11 1.11.3   v1.12 no such issue    Work around:\n create a storage account and specify that account in azure disk storage class, e.g.  kind:StorageClassapiVersion:storage.k8s.io/v1beta1metadata:name:ssdprovisioner:kubernetes.io/azure-diskparameters:skuname:Premium_LRSstorageAccount:customerstorageaccountkind:Dedicated13. cannot find Lun for disk Issue details:\nFollowing error may occur if attach a disk to a node:\nMountVolume.WaitForAttach failed for volume \"pvc-12b458f4-c23f-11e8-8d27-46799c22b7c6\" : Cannot find Lun for disk kubernetes-dynamic-pvc-12b458f4-c23f-11e8-8d27-46799c22b7c6 Related issues\n GetAzureDiskLun sometimes costs 1 min which is too long time  Fix\n PR fix azure disk attachment error on Linux will extract the LUN num from device path only on Linux     k8s version fixed version     v1.9 no such issue   v1.10 1.10.10   v1.11 1.11.5   v1.12 1.12.3   v1.13 no such issue    Work around:\nwait for a few more minutes should work\n14. azure disk attach/detach failure, mount issue, i/o error Issue details:\nWe found a disk attach/detach issue due to dirty vm cache PR introduced from v1.9.2, it would lead to following disk issues:\n disk attach/detach failure for a long time disk I/O error unexpected disk detachment from VM VM running into failed state due to attaching non-existing disk   Note: above error may only happen when there are multiple disk attach/detach operations in parallel and it's not easy to repro since it happens on a little possibility.\n Related issues\n Azure Disks volume attach still times out on Kubernetes 1.10 Azure Disks occasionally mounted in a way leading to I/O errors  Fix\nWe changed the azure disk attach/detach retry logic in k8s v1.13, switch to use k8s attach-detach controller to do attach/detach disk retry and clean vm cache after every disk operation, this issue is proved to be fixed in our disk attach/detach stress test and also verified in customer env:\n PR remove retry operation on attach/detach azure disk in azure cloud provider PR fix azure disk attach/detach failed forever issue PR fix detach azure disk issue due to dirty cache     k8s version fixed version     v1.9 issue introduced in v1.9.2, no cherry-pick fix allowed   v1.10 1.10.12   v1.11 1.11.6   v1.12 1.12.4   v1.13 no such issue    Work around:\n if there is attach disk failure for long time, restart controller manager may work if there is disk not detached for long time, detach that disk manually  ","excerpt":" azure disk plugin known issues  Recommended stable version for azure …","ref":"/cloud-provider-azure/faq/known-issues/azuredisk/","title":"AzureDisk CSI Driver Known Issues"},{"body":" azure file plugin known issues  Recommended stable version for azure file 1. azure file mountOptions setting  file/dir mode setting: other useful mountOptions setting:   2. permission issue of azure file dynamic provision in aks-engine 3. Azure file support on Sovereign Cloud 4. azure file dynamic provision failed due to cluster name length issue 5. azure file dynamic provision failed due to no storage account in current resource group 6. azure file plugin on Windows does not work after node restart 7. file permission could not be changed using azure file, e.g. postgresql 8. Could not delete pod with AzureFile volume if storage account key changed 9. Long latency when handling lots of small files    Recommended stable version for azure file    k8s version stable version     v1.7 1.7.14 or later   v1.8 1.8.11 or later   v1.9 1.9.7 or later   v1.10 1.10.2 or later    1. azure file mountOptions setting file/dir mode setting: Issue details:\n fileMode, dirMode value would be different in different versions, in latest master branch, it's 0755 by default, to set a different value, follow this mount options support of azure file (available from v1.8.5). For version v1.8.0-v1.8.4, since mount options support of azure file is not available, as a workaround, securityContext could be specified for the pod, detailed pod example  securityContext:runAsUser:XXXfsGroup:XXX   version fileMode, dirMode value     v1.6.x, v1.7.x 0777   v1.8.0 ~ v1.8.5, v1.9.0 0700   v1.8.6 or later, v1.9.1 ~ v1.10.9, v1.11.0 ~ v1.11.3, v1.12.0 ~ v.12.1 0755   v1.10.10 or later 0777   v1.11.4 or later 0777   v1.12.2 or later 0777   v1.13.x 0777    other useful mountOptions setting:  mfsymlinks: make azure file(cifs) mount supports symbolic link nobrl: Do not send byte range lock requests to the server. This is necessary for certain applications that break with cifs style mandatory byte range locks (and most cifs servers do not yet support requesting advisory byte range locks). Error message could be like following:  Error: SQLITE_BUSY: database is locked Related issues\n azureFile volume mode too strict for container with non root user Unable to connect to SQL-lite db mounted on AzureFile/AzureDisks [SQLITE_BUSY: database is locked] Allow nobrl parameter like docker to use sqlite over network drive Error to deploy mongo with azure file storage  2. permission issue of azure file dynamic provision in aks-engine Issue details:\nFrom acs-engine v0.12.0, RBAC is enabled, azure file dynamic provision does not work from this version\nerror logs:\nEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Warning ProvisioningFailed 8s persistentvolume-controller Failed to provision volume with StorageClass \"azurefile\": Couldn't create secret secrets is forbidden: User \"system:serviceaccount:kube-syste m:persistent-volume-binder\" cannot create secrets in the namespace \"default\" Warning ProvisioningFailed 8s persistentvolume-controller Failed to provision volume with StorageClass \"azurefile\": failed to find a matching storage account Related issues\n azure file PVC need secrets create permission for persistent-volume-binder  Workaround:\n Add a ClusterRole and ClusterRoleBinding for azure file dynamic privision  kubectl create -f https://raw.githubusercontent.com/andyzhangx/Demo/master/acs-engine/rbac/azure-cloud-provider-deployment.yaml  delete the original PVC and recreate PVC  Fix\n PR in acs-engine: fix azure file dynamic provision permission issue  3. Azure file support on Sovereign Cloud Azure file on Sovereign Cloud is supported from v1.7.11, v1.8.0\n4. azure file dynamic provision failed due to cluster name length issue Issue details: k8s cluster name length must be less than 16 characters, otherwise following error will be received when creating dynamic privisioning azure file pvc, this bug exists in [v1.7.0, v1.7.10]:\n Note: check cluster-name by running grep cluster-name /etc/kubernetes/manifests/kube-controller-manager.yaml on master node\n persistentvolume-controller Warning ProvisioningFailed Failed to provision volume with StorageClass \"azurefile\": failed to find a matching storage account Fix\n PR Fix share name generation in azure file provisioner     k8s version fixed version     v1.7 1.7.11   v1.8 1.8.0   v1.9 1.9.0    5. azure file dynamic provision failed due to no storage account in current resource group Issue details:\nWhen create an azure file PVC, there will be error if there is no storage account in current resource group, error info would be like following:\nEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Warning ProvisioningFailed 10s (x5 over 1m) persistentvolume-controller Failed to provision volume with StorageClass \"azurefile-premium\": failed to find a matching storage account Related issues\n failed to create azure file pvc if there is no storage account in current resource group  Workaround: specify a storage account in azure file dynamic provision, you should make sure the specified storage account is in the same resource group as your k8s cluster. In AKS, the specified storage account should be in shadow resource group(naming as MC_+{RESOUCE-GROUP-NAME}+{CLUSTER-NAME}+{REGION}) which contains all resources of your aks cluster.\nFix\n PR fix the create azure file pvc failure if there is no storage account in current resource group     k8s version fixed version     v1.7 1.7.14   v1.8 1.8.9   v1.9 1.9.4   v1.10 1.10.0    6. azure file plugin on Windows does not work after node restart Issue details: azure file plugin on Windows does not work after node restart, this is due to New-SmbGlobalMapping cmdlet has lost account name/key after reboot\nRelated issues\n azure file plugin on Windows does not work after node restart  Workaround:\n delete the original pod with azure file mount create the pod again  Fix\n PR fix azure file plugin failure issue on Windows after node restart     k8s version fixed version     v1.7 not support in upstream   v1.8 1.8.10   v1.9 1.9.7   v1.10 1.10.0    7. file permission could not be changed using azure file, e.g. postgresql error logs when running postgresql on azure file plugin:\ninitdb: could not change permissions of directory \"/var/lib/postgresql/data\": Operation not permitted fixing permissions on existing directory /var/lib/postgresql/data Issue details: azure file plugin is using cifs/SMB protocol, file/dir permission could not be changed after mounting\nWorkaround: Use subPath together with azure disk plugin (for ext3/4 disk type, there is a lost+found directory after disk format)\nRelated issues Persistent Volume Claim permissions\n8. Could not delete pod with AzureFile volume if storage account key changed Issue details:\n kubelet fails to umount azurefile volume when there is azure file connection, below is an easy repro:  create a pod with azure file mount regenerate the account key of the storage account delete the pod, and the pod will never be deleted due to UnmountVolume.TearDown error    error logs\nnestedpendingoperations.go:263] Operation for \"\\\"kubernetes.io/azure-file/cc5c86cd-422a-11e8-91d7-000d3a03ee84-myvolume\\\" (\\\"cc5c86cd-422a-11e8-91d7-000d3a03ee84\\\")\" failed. No retries permitted until 2018-04-17 10:35:40.240272223 +0000 UTC m=+1185722.391925424 (durationBeforeRetry 500ms). Error: \"UnmountVolume.TearDown failed for volume \\\"myvolume\\\" (UniqueName: \\\"kubernetes.io/azure-file/cc5c86cd-422a-11e8-91d7-000d3a03ee84-myvolume\\\") pod \\\"cc5c86cd-422a-11e8-91d7-000d3a03ee84\\\" (UID: \\\"cc5c86cd-422a-11e8-91d7-000d3a03ee84\\\") : Error checking if path exists: stat /var/lib/kubelet/pods/cc5c86cd-422a-11e8-91d7-000d3a03ee84/volumes/kubernetes.io~azure-file/myvolume: resource temporarily unavailable ... kubelet_volumes.go:128] Orphaned pod \"380b02f3-422b-11e8-91d7-000d3a03ee84\"found, but volume paths are still present on disk Workaround:\nmanually umount the azure file mount path on the agent node and then the pod will be deleted right after that\nsudo umount /var/lib/kubelet/pods/cc5c86cd-422a-11e8-91d7-000d3a03ee84/volumes/kubernetes.io~azure-file/myvolume Fix\n PR Fix bug:Kubelet failure to umount mount points     k8s version fixed version     v1.7 no fix(no cherry-pick fix is allowed)   v1.8 1.8.8   v1.9 1.9.7   v1.10 1.10.0    Related issues\n UnmountVolume.TearDown fails for AzureFile volume, locks up node Kubelet failure to umount glusterfs mount points  9. Long latency compared to disk when handling lots of small files Related issues\n azurefile is very slow Can't roll out Wordpress chart with PV on AzureFile  ","excerpt":" azure file plugin known issues  Recommended stable version for azure …","ref":"/cloud-provider-azure/faq/known-issues/azurefile/","title":"AzureFile CSI Driver Known Issues"},{"body":" Cloud Provider Azure An Azure implementation of the Kubernetes Cloud Provider.\nGet Started   Contribute   \n          Contributions welcome! We do a Pull Request contributions workflow on GitHub. New users are always welcome!\nRead more …\n   Join our slack channel! Please join #provider-azure in Kubernetes slack workspace.\nRead more …\n   Check out release notes! For announcement of latest features, etc.\nRead more …\n    ","excerpt":" Cloud Provider Azure An Azure implementation of the Kubernetes Cloud …","ref":"/cloud-provider-azure/","title":"Cloud Provider Azure"},{"body":"","excerpt":"","ref":"/cloud-provider-azure/contribute/","title":"Contribution"},{"body":"","excerpt":"","ref":"/cloud-provider-azure/development/","title":"Development Guide"},{"body":"","excerpt":"","ref":"/cloud-provider-azure/example/","title":"Example"},{"body":"What is Cloud Provider Azure? A Kubernetes Cloud Provider consists of two parts: a provider-specified cloud-controller-manager (or kube-controller-manager for in-tree version) and a provider-specified implementation of Kubernetes cloud provider interface. Currently, the Azure cloud-controller-manager is outside of Kubernetes repo and the cloud provider interface implementation is still in k/k (will be moved outside of the repo in the future).\nThe cloud-controller-manager is a Kubernetes control plane component which embeds cloud-specific control logic. It lets you link your cluster into your cloud provider's API, and separates out the components that interact with that cloud platform from components that just interact with your cluster.\nBy decoupling the interoperability logic between Kubernetes and the underlying cloud infrastructure, the cloud-controller-manager component enables cloud providers to release features at a different pace compared to the main Kubernetes project.\nWhat is the difference between in-tree and out-of-tree cloud provider? In-tree cloud providers are the providers we develop \u0026 release in the main Kubernetes repository. This results in embedding the knowledge and context of each cloud provider into most of the Kubernetes components. This enables more native integrations such as the kubelet requesting information about itself via a metadata service from the cloud provider.\nOut-of-tree cloud providers are providers that can be developed, built, and released independent of Kubernetes core. This requires deploying a new component called the cloud-controller-manager which is responsible for running all the cloud specific controllers that were previously run in the kube-controller-manager.\nWhich one is recommended? We recommend using the in-tree cloud provider at this time because it's out-of-tree counterpart is not 100% ready. However, out-of-tree cloud provider will become the No.1 pick in the near future.\n","excerpt":"What is Cloud Provider Azure? A Kubernetes Cloud Provider consists of …","ref":"/cloud-provider-azure/faq/","title":"FAQ"},{"body":"","excerpt":"","ref":"/cloud-provider-azure/install/","title":"Deploy Cloud Provider Azure"},{"body":"Prerequisite   An azure service principal\nPlease follow this guide for creating an azure service principal The service principal should either have:\n Contributor permission of a subscription Contributor permission of a resource group. In this case, please create the resource group first    Docker daemon enabled\n  How to run Kubernetes e2e tests locally  Prepare dependency project    aks-engine\nBinary downloads for the latest version of aks-engine for are available on Github. Download AKS Engine for your operating system, extract the binary and copy it to your $PATH.\nOn macOS, you can install aks-engine with Homebrew. Run the command brew install Azure/aks-engine/aks-engine to do so. You can install Homebrew following the instructions.\nOn Windows, you can install aks-engine via Chocolatey by executing the command choco install aks-engine. You can install Chocolatey following the instructions.\nOn Linux, it could also be installed by following commands:\n$ curl -o get-akse.sh https://raw.githubusercontent.com/Azure/aks-engine/master/scripts/get-akse.sh $ chmod 700 get-akse.sh $ ./get-akse.sh   Kubernetes\nThis serves as E2E tests case source, it should be located at $GOPATH/src/k8s.io/kubernetes.\ncd $GOPATH/src go get -d k8s.io/kubernetes   kubectl\nKubectl allows you to run command against Kubernetes cluster, which is also used for deploying CSI plugins. You can follow here to install kubectl. e.g. on Linux\ncurl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl chmod +x kubectl sudo mv kubectl /usr/local/bin/    Build docker image azure-cloud-controller-manager and push it to your docker image repository.\ngit clone https://github.com/kubernetes-sigs/cloud-provider-azure $GOPATH/src/sigs.k8s.io/cloud-provider-azure cd $GOPATH/src/sigs.k8s.io/cloud-provider-azure export IMAGE_REGISTRY=\u003cusername\u003e export IMAGE_TAG=\u003ctag\u003e make build-images make push-images # or manually `docker push`   Deploy a Kubernetes cluster with the above azure-cloud-controller-manager image.\nTo deploy a cluster, export all the required environmental variables first and then invoke make deploy:\nexport RESOURCE_GROUP_NAME=\u003cresource group name\u003e export K8S_AZURE_LOCATION=\u003clocation\u003e export K8S_AZURE_SUBSID=\u003csubscription ID\u003e export K8S_AZURE_SPID=\u003cclient id\u003e export K8S_AZURE_SPSEC=\u003cclient secret\u003e export K8S_AZURE_TENANTID=\u003ctenant id\u003e export USE_CSI_DEFAULT_STORAGECLASS=\u003ctrue/false\u003e export K8S_RELEASE_VERSION=\u003ck8s release version\u003e export CCM_IMAGE=\u003cimage of the cloud controller manager\u003e export CNM_IMAGE=\u003cimage of the cloud node manager\u003e make deploy To connect the cluster:\nexport KUBECONFIG=$GOPATH/src/sigs.k8s.io/cloud-provider-azure/_output/$(ls -t _output | head -n 1)/kubeconfig/kubeconfig.$LOCATION.json kubectl cluster-info   To check out more of the deployed cluster , replace kubectl cluster-info with other kubectl commands. To further debug and diagnose cluster problems, use kubectl cluster-info dump\nGet kubetest binary  go get -u k8s.io/test-infra/kubetest Run E2E tests  Please first ensure the kubernetes project locates at $GOPATH/src/k8s.io/kubernetes, the e2e tests will be built from that location.\ncd $GOPATH/src/k8s.io/kubernetes make WHAT='test/e2e/e2e.test' make WHAT=cmd/kubectl make ginkgo export KUBERNETES_PROVIDER=azure export KUBERNETES_CONFORMANCE_TEST=y export KUBERNETES_CONFORMANCE_PROVIDER=azure export CLOUD_CONFIG=$GOPATH/src/sigs.k8s.io/cloud-provider-azure/tests/k8s-azure/manifest/azure.json # some test cases require ssh configurations export KUBE_SSH_KEY_PATH=path/to/ssh/privatekey export KUBE_SSH_USER={ssh_user} # Replace the test_args with your own. kubetest --test --provider=local --check-version-skew=false --test_args='--ginkgo.focus=Port\\sforwarding' ","excerpt":"Prerequisite   An azure service principal\nPlease follow this guide for …","ref":"/cloud-provider-azure/development/e2e/e2e-tests/","title":"Kubernetes E2E tests"},{"body":"","excerpt":"","ref":"/cloud-provider-azure/blog/","title":"Release Notes"},{"body":"","excerpt":"","ref":"/cloud-provider-azure/search/","title":"Search Results"},{"body":"","excerpt":"","ref":"/cloud-provider-azure/topics/","title":"Topics"}]