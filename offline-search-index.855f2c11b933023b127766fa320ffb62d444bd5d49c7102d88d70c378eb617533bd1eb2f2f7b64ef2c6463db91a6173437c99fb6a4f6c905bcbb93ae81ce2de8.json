[{"body":" The key words “MUST”, “MUST NOT”, “REQUIRED”, “SHALL”, “SHALL NOT”, “SHOULD”, “SHOULD NOT”, “RECOMMENDED”, “MAY”, and “OPTIONAL” in this document are to be interpreted as described in RFC 2119.\n Here is a list of Azure resource assumptions that are required for cloud provider Azure:\n All Azure resources MUST be under the same tenant. All virtual machine names MUST be the same as their hostname. Node LoadBalancer’s names SHOULD be following rules (\u003cclusterName\u003e is coming from --cluster-name configuration, default is kubernetes)  When enableMultipleStandardLoadBalancers is configured to false, LoadBalancer’s name SHOULD be \u003cclusterName\u003e for external type and \u003cclusterName\u003e-internal for internal type. When enableMultipleStandardLoadBalancers is configured to true, multiple standard load balancers SHOULD be provisioned:  All the virtual machines MUST be part of either VirtualMachineScaleSet (VMSS) or AvailabilitySet (VMAS). Each VMAS and VMSS SHOULD be put behind a different standard LoadBalancer. The primary LoadBalancer’s name SHOULD be \u003cclusterName\u003e for external type and \u003cclusterName\u003e-internal for internal type. Virtual machines that are part of primary VMAS (set by primaryAvailabilitySetName) or primary VMSS (set by primaryScaleSetName) SHOULD be added to primary LoadBalancer backend address pool. Other standard LoadBalancer’s name SHOULD be same as VMAS or VMSS name.     The cluster name set for kube-controller-manager --cluster-name=\u003ccluster-name\u003e MUST not end with -internal.  After the cluster is provisioned, cloud provider Azure MAY update the following Azure resources based on workloads:\n New routes would be added for each node if --configure-cloud-routes is enabled. New LoadBalancer (including external and internal) would be created if they’re not existing yet. Virtual machines and virtual machine scale sets would be added to LoadBalancer backend address pools if they’re not added yet. New public IPs and NSG rules would be added when LoadBalancer typed services are created.  ","excerpt":" The key words “MUST”, “MUST NOT”, “REQUIRED”, “SHALL”, “SHALL NOT”, …","ref":"/cloud-provider-azure/topics/assumptions/","title":"Cluster Provisioning Tools Contract"},{"body":"This doc describes cloud provider config file, which is to be used via the --cloud-config flag of azure-cloud-controller-manager.\nHere is a config file sample:\n{ \"cloud\":\"AzurePublicCloud\", \"tenantId\": \"0000000-0000-0000-0000-000000000000\", \"aadClientId\": \"0000000-0000-0000-0000-000000000000\", \"aadClientSecret\": \"0000000-0000-0000-0000-000000000000\", \"subscriptionId\": \"0000000-0000-0000-0000-000000000000\", \"resourceGroup\": \"\u003cname\u003e\", \"location\": \"eastus\", \"subnetName\": \"\u003cname\u003e\", \"securityGroupName\": \"\u003cname\u003e\", \"securityGroupResourceGroup\": \"\u003cname\u003e\", \"vnetName\": \"\u003cname\u003e\", \"vnetResourceGroup\": \"\u003cname\u003e\", \"routeTableName\": \"\u003cname\u003e\", \"primaryAvailabilitySetName\": \"\u003cname\u003e\", \"routeTableResourceGroup\": \"\u003cname\u003e\", \"cloudProviderBackoff\": false, \"useManagedIdentityExtension\": false, \"useInstanceMetadata\": true } Note: All values are of type string if not explicitly called out.\nAuth configs    Name Description Remark     cloud The cloud environment identifier Valid values could be found here. Default to AzurePublicCloud.   tenantID The AAD Tenant ID for the Subscription that the cluster is deployed in Required.   aadClientID The ClientID for an AAD application with RBAC access to talk to Azure RM APIs Used for service principal authn.   aadClientSecret The ClientSecret for an AAD application with RBAC access to talk to Azure RM APIs Used for service principal authn.   aadClientCertPath The path of a client certificate for an AAD application with RBAC access to talk to Azure RM APIs Used for client cert authn.   aadClientCertPassword The password of the client certificate for an AAD application with RBAC access to talk to Azure RM APIs Used for client cert authn.   useManagedIdentityExtension Use managed service identity for the virtual machine to access Azure ARM APIs Boolean type, default to false.   userAssignedIdentityID The Client ID of the user assigned MSI which is assigned to the underlying VMs Required for user-assigned managed identity.   subscriptionId The ID of the Azure Subscription that the cluster is deployed in Required.   identitySystem The identity system for AzureStack. Supported values are: ADFS Only used for AzureStack   networkResourceTenantID The AAD Tenant ID for the Subscription that the network resources are deployed in Optional. Supported since v1.18.0. Only used for hosting network resources in different AAD Tenant and Subscription than those for the cluster.   networkResourceSubscriptionID The ID of the Azure Subscription that the network resources are deployed in Optional. Supported since v1.18.0. Only used for hosting network resources in different AAD Tenant and Subscription than those for the cluster.    Note: Cloud provider currently supports three authentication methods, you can choose one combination of them:\n Managed Identity:  For system-assigned managed identity: set useManagedIdentityExtension to true For user-assigned managed identity: set useManagedIdentityExtension to true and also set userAssignedIdentityID   Service Principal: set aadClientID and aadClientSecret Client Certificate: set aadClientCertPath and aadClientCertPassword  If more than one value is set, the order is Managed Identity \u003e Service Principal \u003e Client Certificate.\nCluster config    Name Description Remark     resourceGroup The name of the resource group that the cluster is deployed in    location The location of the resource group that the cluster is deployed in    vnetName The name of the VNet that the cluster is deployed in    vnetResourceGroup The name of the resource group that the Vnet is deployed in    subnetName The name of the subnet that the cluster is deployed in    securityGroupName The name of the security group attached to the cluster’s subnet    securityGroupResourceGroup The name of the resource group that the security group is deployed in    routeTableName The name of the route table attached to the subnet that the cluster is deployed in Optional in 1.6   primaryAvailabilitySetName* The name of the availability set that should be used as the load balancer backend Optional   vmType The type of azure nodes. Candidate values are: vmss and standard Optional, default to standard   primaryScaleSetName* The name of the scale set that should be used as the load balancer backend Optional   cloudProviderBackoff Enable exponential backoff to manage resource request retries Boolean value, default to false   cloudProviderBackoffRetries Backoff retry limit Integer value, valid if cloudProviderBackoff is true   cloudProviderBackoffExponent Backoff exponent Float value, valid if cloudProviderBackoff is true   cloudProviderBackoffDuration Backoff duration Integer value, valid if cloudProviderBackoff is true   cloudProviderBackoffJitter Backoff jitter Float value, valid if cloudProviderBackoff is true   cloudProviderBackoffMode Backoff mode, supported values are “v2” and “default”. Note that “v2” has been deprecated since v1.18.0. Default to “default”   cloudProviderRateLimit Enable rate limiting Boolean value, default to false   cloudProviderRateLimitQPS Rate limit QPS (Read) Float value, valid if cloudProviderRateLimit is true   cloudProviderRateLimitBucket Rate limit Bucket Size Integar value, valid if cloudProviderRateLimit is true   cloudProviderRateLimitQPSWrite Rate limit QPS (Write) Float value, valid if cloudProviderRateLimit is true   cloudProviderRateLimitBucketWrite Rate limit Bucket Size Integer value, valid if cloudProviderRateLimit is true   useInstanceMetadata Use instance metadata service where possible Boolean value, default to false   loadBalancerSku Sku of Load Balancer and Public IP. Candidate values are: basic and standard. Default to basic.   excludeMasterFromStandardLB ExcludeMasterFromStandardLB excludes master nodes from standard load balancer. Boolean value, default to true.   disableOutboundSNAT Disable outbound SNAT for SLB Default to false and available since v1.11.9, v1.12.7, v1.13.5 and v1.14.0   maximumLoadBalancerRuleCount Maximum allowed LoadBalancer Rule Count is the limit enforced by Azure Load balancer Integer value, default to 148   routeTableResourceGroup The resource group name for routeTable Default same as resourceGroup and available since v1.15.0   loadBalancerName Working together with loadBalancerResourceGroup to determine the LB name in a different resource group Since v1.18.0, default is cluster name setting on kube-controller-manager   loadBalancerResourceGroup The load balancer resource group name, which is different from node resource group Since v1.18.0, default is same as resourceGroup   disableAvailabilitySetNodes Disable supporting for AvailabilitySet virtual machines in vmss cluster. It should be only used when vmType is “vmss” and all the nodes (including master) are VMSS virtual machines Since v1.18.0, default is false   availabilitySetNodesCacheTTLInSeconds Cache TTL in seconds for availabilitySet Nodes Since v1.18.0, default is 900   vmssCacheTTLInSeconds Cache TTL in seconds for VMSS Since v1.18.0, default is 600   vmssVirtualMachinesCacheTTLInSeconds Cache TTL in seconds for VMSS virtual machines Since v1.18.0, default is 600   vmCacheTTLInSeconds Cache TTL in seconds for virtual machines Since v1.18.0, default is 60   loadBalancerCacheTTLInSeconds Cache TTL in seconds for load balancers Since v1.18.0, default is 120   nsgCacheTTLInSeconds Cache TTL in seconds for network security group Since v1.18.0, default is 120   routeTableCacheTTLInSeconds Cache TTL in seconds for route table Since v1.18.0, default is 120   disableAzureStackCloud DisableAzureStackCloud disables AzureStackCloud support. It should be used when setting Cloud with “AZURESTACKCLOUD” to customize ARM endpoints while the cluster is not running on AzureStack. Default is false. Optional. Supported since v1.20.0 in out-of-tree cloud provider Azure.   tags tags that would be tagged onto the cloud provider managed resources, including lb, public IP, network security group and route table. Optional. Supported since v1.20.0.   systemTags tag keys that should not be deleted when being updated. Optional. Supported since v1.21.0.    primaryAvailabilitySetName If this is set, the Azure cloudprovider will only add nodes from that availability set to the load balancer backend pool. If this is not set, and multiple agent pools (availability sets) are used, then the cloudprovider will try to add all nodes to a single backend pool which is forbidden. In other words, if you use multiple agent pools (availability sets), you MUST set this field.\nprimaryScaleSetName If this is set, the Azure cloudprovider will only add nodes from that scale set to the load balancer backend pool. If this is not set, and multiple agent pools (scale sets) are used, then the cloudprovider will try to add all nodes to a single backend pool which is forbidden when using Load Balancer Basic SKU. In other words, if you use multiple agent pools (scale sets), and loadBalancerSku is set to basic you MUST set this field.\nexcludeMasterFromStandardLB Master nodes are not added to the backends of Azure Load Balancer (ALB) if excludeMasterFromStandardLB is set.\nBy default, if nodes are labeled with node-role.kubernetes.io/master, they would also be excluded from ALB. If you want to add the master nodes to ALB, excludeMasterFromStandardLB should be set to false and label node-role.kubernetes.io/master should be removed if it has already been applied.\nSetting Azure cloud provider from Kubernetes secrets Since v1.21.0, Azure cloud provider supports reading the cloud config from Kubernetes secrets. The secret is a serialized version of azure.json file. When the secret is changed, the cloud controller manager will re-constructing itself without restarting the pod.\nTo enable this feature, set --enable-dynamic-reloading=true and configure the secret name, namespace and data key by --cloud-config-secret-name, --cloud-config-secret-namespace and --cloud-config-key. When initializing from secret, the --cloud-config would be ignored.\n Note that the --enable-dynamic-reloading cannot be false if --cloud-config is empty. To build the cloud provider from classic config file, please explicitly specify the --cloud-config and do not set --enable-dynamic-reloading=true. In this manner, the cloud controller manager will not be updated when the config file is changed. You need to restart the pod to manually trigger the re-initialization.\n Since Azure cloud provider would read Kubernetes secrets, the following RBAC should also be configured:\n---apiVersion:rbac.authorization.k8s.io/v1beta1kind:ClusterRolemetadata:labels:kubernetes.io/cluster-service:\"true\"name:system:azure-cloud-provider-secret-getterrules:- apiGroups:[\"\"]resources:[\"secrets\"]resourceNames:[\"azure-cloud-provider\"]verbs:- get---apiVersion:rbac.authorization.k8s.io/v1beta1kind:ClusterRoleBindingmetadata:labels:kubernetes.io/cluster-service:\"true\"name:system:azure-cloud-provider-secret-getterroleRef:apiGroup:rbac.authorization.k8s.iokind:ClusterRolename:system:azure-cloud-provider-secret-gettersubjects:- kind:ServiceAccountname:azure-cloud-providernamespace:kube-systemper client rate limiting Since v1.18.0, the original global rate limiting has been switched to per-client. A set of new rate limit configure options are introduced for each client, which includes:\n routeRateLimit SubnetsRateLimit InterfaceRateLimit RouteTableRateLimit LoadBalancerRateLimit PublicIPAddressRateLimit SecurityGroupRateLimit VirtualMachineRateLimit StorageAccountRateLimit DiskRateLimit SnapshotRateLimit VirtualMachineScaleSetRateLimit VirtualMachineSizeRateLimit  The original rate limiting options (“cloudProviderRateLimitBucket”, “cloudProviderRateLimitBucketWrite”, “cloudProviderRateLimitQPS”, “cloudProviderRateLimitQPSWrite”) are still supported, and they would be the default values if per-client rate limiting is not configured.\nHere is an example of per-client config:\n{ // default rate limit (enabled). \"cloudProviderRatelimit\": true, \"cloudProviderRateLimitBucket\": 1, \"cloudProviderRateLimitBucketWrite\": 1, \"cloudProviderRateLimitQPS\": 1, \"cloudProviderRateLimitQPSWrite\": 1, \"virtualMachineScaleSetRateLimit\": { // VMSS specific (enabled). \"cloudProviderRatelimit\": true, \"cloudProviderRateLimitBucket\": 2, \"CloudProviderRateLimitBucketWrite\": 2, \"cloudProviderRateLimitQPS\": 0, \"CloudProviderRateLimitQPSWrite\": 0 }, \"loadBalancerRateLimit\": { // LB specific (disabled) \"cloudProviderRatelimit\": false }, ... // other cloud provider configs } Run Kubelet without Azure identity When running Kubelet with kube-controller-manager, it also supports running without Azure identity since v1.15.0.\nBoth kube-controller-manager and kubelet should configure --cloud-provider=azure --cloud-config=/etc/kubernetes/azure.json, but the contents for azure.json are different:\n(1) For kube-controller-manager, refer the above part for setting azure.json.\n(2) For kubelet, useInstanceMetadata is required to be true and Azure identities are not required. A sample for Kubelet’s azure.json is\n{ \"useInstanceMetadata\": true, \"vmType\": \"vmss\" } Azure Stack Configuration Azure Stack has different API endpoints, depending on the Azure Stack deployment. These need to be provided to the Azure SDK and currently this is done by adding an extra json file with the arguments, as well as an environment variable pointing to this file.\nThere are several available presets, namely:\n AzureChinaCloud AzureGermanCloud AzurePublicCloud AzureUSGovernmentCloud  These are determined using cloud: \u003cPRESET\u003e described above in the description of azure.json.\nWhen cloud: AzureStackCloud, the extra environment variable used by the Azure SDK to find the Azure Stack configuration file is:\n AZURE_ENVIRONMENT_FILEPATH  The configuration parameters of this file:\n{ \"name\": \"AzureStackCloud\", \"managementPortalURL\": \"...\", \"publishSettingsURL\": \"...\", \"serviceManagementEndpoint\": \"...\", \"resourceManagerEndpoint\": \"...\", \"activeDirectoryEndpoint\": \"...\", \"galleryEndpoint\": \"...\", \"keyVaultEndpoint\": \"...\", \"graphEndpoint\": \"...\", \"serviceBusEndpoint\": \"...\", \"batchManagementEndpoint\": \"...\", \"storageEndpointSuffix\": \"...\", \"sqlDatabaseDNSSuffix\": \"...\", \"trafficManagerDNSSuffix\": \"...\", \"keyVaultDNSSuffix\": \"...\", \"serviceBusEndpointSuffix\": \"...\", \"serviceManagementVMDNSSuffix\": \"...\", \"resourceManagerVMDNSSuffix\": \"...\", \"containerRegistryDNSSuffix\": \"...\", \"cosmosDBDNSSuffix\": \"...\", \"tokenAudience\": \"...\", \"resourceIdentifiers\": { \"graph\": \"...\", \"keyVault\": \"...\", \"datalake\": \"...\", \"batch\": \"...\", \"operationalInsights\": \"...\" } } The full list of existing settings for the AzureChinaCloud, AzureGermanCloud, AzurePublicCloud and AzureUSGovernmentCloud is available in the source code at https://github.com/Azure/go-autorest/blob/master/autorest/azure/environments.go#L51.\nHost Network Resources in different AAD Tenant and Subscription Since v1.18.0, Azure cloud provider supports hosting network resources (Virtual Network, Network Security Group, Route Table, Load Balancer and Public IP) in different AAD Tenant and Subscription than those for the cluster. To enable this feature, set networkResourceTenantID and networkResourceSubscriptionID in auth config. Note that the value of them need to be different than value of tenantID and subscriptionID.\nWith this feature enabled, network resources of the cluster will be created in networkResourceSubscriptionID in networkResourceTenantID, and rest resources of the cluster still remain in subscriptionID in tenantID. Properties which specify the resource groups of network resources are compatible with this feature. For example, Virtual Network will be created in vnetResourceGroup in networkResourceSubscriptionID in networkResourceTenantID.\nFor authentication methods, only Service Principal supports this feature, and aadClientID and aadClientSecret are used to authenticate with those two AAD Tenants and Subscriptions. Managed Identity and Client Certificate doesn’t support this feature. Azure Stack doesn’t support this feature.\nCurrent default rate-limiting values The following are the default rate limiting values configured in AKS and AKS-Engine clusters prior to Kubernetes version v1.18.0.\n\"cloudProviderBackoff\": true, \"cloudProviderBackoffRetries\": 6, \"cloudProviderBackoffDuration\": 5, \"cloudProviderRatelimit\": true, \"cloudProviderRateLimitQPS\": 10, \"cloudProviderRateLimitBucket\": 100, \"cloudProviderRatelimitQPSWrite\": 10, \"cloudProviderRatelimitBucketWrite\": 100, For v1.18.0+ refer to per client rate limit config\n","excerpt":"This doc describes cloud provider config file, which is to be used via …","ref":"/cloud-provider-azure/install/configs/","title":"Configure Cloud Provider"},{"body":"Thanks for taking the time to join our community and start contributing!\nThe Contributor Guide provides detailed instructions on how to get your ideas and bug fixes seen and accepted.\nPlease remember to sign the CNCF CLA and read and observe the Code of Conduct.\n","excerpt":"Thanks for taking the time to join our community and start …","ref":"/cloud-provider-azure/contribute/contributing/","title":"Contributing"},{"body":"Switch to the project root directory and run the following command to build both CCM and CNM images:\nmake image If you want to build only one of them, try make build-ccm-image or make build-node-image.\nTo push the images to your own image registry, you can specify the registry and image tag while building:\nIMAGE_REGISTRY=\u003cimage registry name\u003e IMAGE_TAG=\u003ctag name\u003e make image After building, you can push them to your image registry by make push.\n","excerpt":"Switch to the project root directory and run the following command to …","ref":"/cloud-provider-azure/development/custom-images/","title":"Deploy with Customized Images"},{"body":"To deploy an In-tree Cloud Provider Azure, all you need to do is deploy a cluster using AKS-Engine with the API model defined here. The AKS-Engine will automatically deploy the Kubernetes components needed and you don’t have to deploy them manually. However, customization is possible by modifying the manifests in /etc/kubernetes on master node. Here are the examples:\nkube-apiserver\nkube-controller-manager\nkube-scheduler\nTo customize kubelet, you need to modify the starting command like here.\n","excerpt":"To deploy an In-tree Cloud Provider Azure, all you need to do is …","ref":"/cloud-provider-azure/example/in-tree/","title":"Deploy with In-tree Cloud Provider Azure"},{"body":"","excerpt":"","ref":"/cloud-provider-azure/blog/releases/","title":"Release Notes"},{"body":"azure-cloud-controller-manager is a Kubernetes component which provides interoperability with Azure API, and will be used by Kubernetes clusters running on Azure. It runs together with other components to provide the Kubernetes cluster’s control plane.\nUsing cloud-controller-manager is a new alpha feature for Kubernetes since v1.14. cloud-controller-manager runs cloud provider related controller loops, which used to be run by controller-manager.\nazure-cloud-controller-manager is a specialization of cloud-controller-manager. It depends on cloud-controller-manager app and azure cloud provider.\nDeployment To deploy Azure cloud controller manager, the following components need to be configured.\nkubelet    Flag Value Remark     --cloud-provider external cloud-provider should be set external   --azure-container-registry-config /etc/kubernetes/azure.json Used for Azure credential provider    kube-controller-manager    Flag Value Remark     --cloud-provider external cloud-provider should be set external   --external-cloud-volume-plugin azure Optional*    *: Since cloud controller manager does not support volume controllers, it will not provide volume capabilities compared to using previous built-in cloud provider case. You can add this flag to turn on volume controller for in-tree cloud providers. This option is likely to be removed with in-tree cloud providers in future.\nkube-apiserver Do not set flag --cloud-provider.\nazure-cloud-controller-manager    Flag Value Remark     --cloud-provider azure cloud-provider should be set azure   --cloud-config /etc/kubernetes/azure.json Path for cloud provider config   --controllers *,-cloud-node cloud node controller should be disabled   --configure-cloud-routes “false” for Azure CNI and “true” for other network plugins Used for non-AzureCNI clusters    For other flags such as --allocate-node-cidrs, --cluster-cidr and --cluster-name, they are moved from kube-controller-manager. If you are migrating from kube-controller-manager, they should be set to same value.\nFor details of those flags, please refer to this doc.\nazure-cloud-node-manager azure-cloud-node-manager should be run as daemonsets on both Windows and Linux nodes, and the following configurations should be set:\n   Flag Value Remark     --node-name The node name for the Pod Kubernetes Downward API could be used to get Pod’s name   --wait-routes only set to true when --configure-cloud-routes=true in cloud-controller-manager Used for non-AzureCNI clusters    Please refer examples here for sample deployment manifests for above components.\nAlternatively, you can use aks-engine to deploy a Kubernetes cluster running with cloud-controller-manager. It supports deploying Kubernetes azure-cloud-controller-manager for Kubernetes v1.16+.\nAzureDisk and AzureFile AzureDisk and AzureFile volume plugins are not supported with in-tree cloud provider (See kubernetes/kubernetes#71018 for explanations).\nHence, azuredisk-csi-driver and azurefile-csi-driver should be used for persistent volumes. Please refer the installation guides here and here for their deployments.\nChange default storage class Follow the steps bellow if you want change the current default storage class to AzureDisk CSI driver.\nFirst, delete the default storage class:\nkubectl delete storageclass default Then create a new storage class named default:\ncat \u003c\u003cEOF | kubectl apply -f- apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: annotations: storageclass.beta.kubernetes.io/is-default-class: \"true\" name: default provisioner: disk.csi.azure.com parameters: skuname: Standard_LRS # available values: Standard_LRS, Premium_LRS, StandardSSD_LRS and UltraSSD_LRS kind: managed # value \"dedicated\", \"shared\" are deprecated since it's using unmanaged disk cachingMode: ReadOnly reclaimPolicy: Delete volumeBindingMode: Immediate EOF ","excerpt":"azure-cloud-controller-manager is a Kubernetes component which …","ref":"/cloud-provider-azure/install/azure-ccm/","title":"Deploy Cloud Controller Manager"},{"body":"The way Azure defines a LoadBalancer is different from GCE or AWS. Azure’s LB can have multiple frontend IP refs. GCE and AWS only allow one, if you want more, you would need multiple LBs. Since Public IP’s are not part of the LB in Azure, an NSG is not part of the LB in Azure either. However, you cannot delete them in parallel, a Public IP can only be deleted after the LB’s frontend IP ref is removed.\nThe different Azure Resources such as LB, Public IP, and NSG are the same tier of Azure resources and circular dependencies need to be avoided. In other words, they should only depend on service state.\nBy default the basic SKU is selected for a load balancer. Services can be annotated to allow auto selection of available load balancers. Service annotations can also be used to provide specific availability sets that host the load balancers. Note that in case of auto selection or specific availability set selection, services are currently not auto-reassigned to an available loadbalancer when the availability set is lost in case of downtime or cluster scale down.\nLoadBalancer annotations Below is a list of annotations supported for Kubernetes services with type LoadBalancer:\n   Annotation Value Description Kubernetes Version     service.beta.kubernetes.io/azure-load-balancer-internal true or false Specify whether the load balancer should be internal. It’s defaulting to public if not set. v1.10.0 and later   service.beta.kubernetes.io/azure-load-balancer-internal-subnet Name of the subnet Specify which subnet the internal load balancer should be bound to. It’s defaulting to the subnet configured in cloud config file if not set. v1.10.0 and later   service.beta.kubernetes.io/azure-load-balancer-mode auto, {vmset-name} Specify the Azure load balancer selection algorithm based on vm sets (VMSS or VMAS). There are currently three possible load balancer selection modes : default, auto or “{vmset-name}”. This is only working for basic LB or multiple standard LB (see below for how it works) v1.10.0 and later   service.beta.kubernetes.io/azure-dns-label-name Name of the PIP DNS label Specify the DNS label name for the service’s public IP address (PIP). If it is set to empty string, DNS in PIP would be deleted. Because of a bug, before v1.15.10/v1.16.7/v1.17.3, the DNS label on PIP would also be deleted if the annotation is not specified. v1.15.0 and later   service.beta.kubernetes.io/azure-shared-securityrule true or false Specify that the service should be exposed using an Azure security rule that may be shared with another service, trading specificity of rules for an increase in the number of services that can be exposed. This relies on the Azure “augmented security rules” feature. v1.10.0 and later   service.beta.kubernetes.io/azure-load-balancer-resource-group Name of the PIP resource group Specify the resource group of the service’s PIP that are not in the same resource group as the cluster. v1.10.0 and later   service.beta.kubernetes.io/azure-allowed-service-tags List of allowed service tags Specify a list of allowed service tags separated by comma. v1.11.0 and later   service.beta.kubernetes.io/azure-load-balancer-tcp-idle-timeout TCP idle timeouts in minutes Specify the time, in minutes, for TCP connection idle timeouts to occur on the load balancer. Default and minimum value is 4. Maximum value is 30. Must be an integer. v1.11.4, v1.12.0 and later   service.beta.kubernetes.io/azure-pip-name Name of PIP Specify the PIP that will be applied to load balancer v1.16 and later   service.beta.kubernetes.io/azure-load-balancer-disable-tcp-reset true Disable enableTcpReset for SLB v1.16-v1.18. The annotation has been deprecated and would be removed in a future release.   service.beta.kubernetes.io/azure-pip-tags Tags of the PIP Specify the tags of the PIP that will be associated to the load balancer typed service. Doc v1.20 and later   service.beta.kubernetes.io/azure-load-balancer-health-probe-protocol Health probe protocol of the load balancer typed service Refer the detailed docs here v1.20 and later   service.beta.kubernetes.io/azure-load-balancer-health-probe-request-path Request path of the health probe Refer the detailed docs here v1.20 and later   service.beta.kubernetes.io/azure-load-balancer-enable-high-availability-ports Enable high availability ports on internal SLB HA ports is required when applications require IP fragments v1.20 and later   service.beta.kubernetes.io/azure-deny-all-except-load-balancer-source-ranges true or false Deny all traffic to the service. This is helpful when the service.Spec.LoadBalancerSourceRanges is set to an internal load balancer typed service. When set the loadBalancerSourceRanges field on the service in order to whitelist ip src addresses, although the generated NSG has added the rules for loadBalancerSourceRanges, the default rule (65000) will allow any vnet traffic, basically meaning the whitelist is of no use. This annotation solves this issue. v1.21 and later   service.beta.kubernetes.io/azure-additional-public-ips External public IPs besides the service’s own public IP It is mainly used for global VIP on Azure cross-region LoadBalancer v1.20 and later with out-of-tree cloud provider    Please note that\n When loadBalancerSourceRanges have been set on service spec, service.beta.kubernetes.io/azure-allowed-service-tags won’t work because of DROP iptables rules from kube-proxy. The CIDRs from service tags should be merged into loadBalancerSourceRanges to make it work.  Load balancer selection modes There are currently three possible load balancer selection modes :\n Default mode - service has no annotation (“service.beta.kubernetes.io/azure-load-balancer-mode”). In this case the Loadbalancer of the primary Availability set is selected “auto” mode - service is annotated with __auto__ value. In this case, services would be associated with the Loadbalancer with the minimum number of rules. “{vmset-name}” mode - service is annotated with the name of a VMSS/VMAS. In this case, only load balancers of the specified VMSS/VMAS would be selected, and services would be associated with the one with the minimum number of rules.   Note that the “auto” mode is valid only if the service is newly created. It is not allowed to change the annotation value to __auto__ of an existed service.\n The selection mode for a load balancer only works for basic load balancers or multiple standard load balancers. Following is the detailed information of allowed number of VMSS/VMAS in a load balancer.\n Standard SKU supports any virtual machine in a single virtual network, including a mix of virtual machines, availability sets, and virtual machine scale sets. So all the nodes would be added to the same standard LB backend pool with a max size of 1000. Basic SKU only supports virtual machines in a single availability set, or a virtual machine scale set. Only nodes with the same availability set or virtual machine scale set would be added to the basic LB backend pool.  LoadBalancer SKUs Azure cloud provider supports both basic and standard SKU load balancers, which can be set via loadBalancerSku option in cloud config file. A list of differences between these two SKUs can be found here.\n Note that the public IPs used in load balancer frontend configurations should be the same SKU. That is a standard SKU public IP for standard load balancer and a basic SKU public IP for a basic load balancer.\n Azure doesn’t support a network interface joining load balancers with different SKUs, hence migration dynamically between them is not supported.\n If you do require migration, please delete all services with type LoadBalancer (or change to other type)\n Outbound connectivity Outbound connectivity is also different between the two load balancer SKUs:\n  For the basic SKU, the outbound connectivity is opened by default. If multiple frontends are set, then the outbound IP is selected randomly (and configurable) from them.\n  For the standard SKU, the outbound connectivity is disabled by default. There are two ways to open the outbound connectivity: use a standard public IP with the standard load balancer or define outbound rules.\n  Standard LoadBalancer Because the load balancer in a Kubernetes cluster is managed by the Azure cloud provider, and it may change dynamically (e.g. the public load balancer would be deleted if no services defined with type LoadBalancer), outbound rules are the recommended path if you want to ensure the outbound connectivity for all nodes.\n Especially note:\n  In the context of outbound connectivity, a single standalone VM, all the VM’s in an Availability Set, all the instances in a VMSS behave as a group. This means, if a single VM in an Availability Set is associated with a Standard SKU, all VM instances within this Availability Set now behave by the same rules as if they are associated with Standard SKU, even if an individual instance is not directly associated with it.\n  Public IP’s used as instance-level public IP are mutually exclusive with outbound rules.\n   Here is the recommended way to define the outbound rules when using separate provisioning tools:\n Create a separate IP (or multiple IPs for scale) in a standard SKU for outbound rules. Make use of the allocatedOutboundPorts parameter to allocate sufficient ports for your desired scenario scale. Create a separate pool definition for outbound, and ensure all virtual machines or VMSS virtual machines are in this pool. Azure cloud provider will manage the load balancer rules with another pool, so that provisioning tools and the Azure cloud provider won’t affect each other. Define inbound with load balancing rules and inbound NAT rules as needed, and set disableOutboundSNAT to true on the load balancing rule(s). Don’t rely on the side effect from these rules for outbound connectivity. It makes it messier than it needs to be and limits your options. Use inbound NAT rules to create port forwarding mappings for SSH access to the VM’s rather than burning public IPs per instance.  Exclude nodes from the load balancer  Excluding nodes from Azure LoadBalancer is supported since v1.20.0.\n The kubernetes controller manager supports excluding nodes from the load balancer backend pools by enabling the feature gate ServiceNodeExclusion. To exclude nodes from Azure LoadBalancer, label node.kubernetes.io/exclude-from-external-load-balancers=true should be added to the nodes.\n  To use the feature, the feature gate ServiceNodeExclusion should be on (enabled by default since its beta on v1.19).\n  The labeled nodes would be excluded from the LB in the next LB reconcile loop, which needs one or more LB typed services to trigger. Basically, users could trigger the update by creating a service. If there are one or more LB typed services existing, no extra operations are needed.\n  To re-include the nodes, just remove the label and the update would be operated in the next LB reconcile loop.\n  Using SCTP SCTP protocol services are only supported on internal standard LoadBalancer, hence annotation service.beta.kubernetes.io/azure-load-balancer-internal: \"true\" should be added to SCTP protocol services. See below for an example:\napiVersion:v1kind:Servicemetadata:name:sctpserviceannotations:service.beta.kubernetes.io/azure-load-balancer-internal:\"true\"spec:type:LoadBalancerselector:app:sctpserverports:- name:sctpserverprotocol:SCTPport:30102targetPort:30102Load balancer limits The limits of the load balancer related resources are listed below:\nStandard Load Balancer\n   Resource Limit     Load balancers 1,000   Rules per resource 1,500   Rules per NIC (across all IPs on a NIC) 300   Frontend IP configurations 600   Backend pool size 1,000 IP configurations, single virtual network   Backend resources per Load Balancer 150   High-availability ports 1 per internal frontend   Outbound rules per Load Balancer 600   Load Balancers per VM 2 (1 Public and 1 internal)    The limit is up to 150 resources, in any combination of standalone virtual machine resources, availability set resources, and virtual machine scale-set placement groups.\nBasic Load Balancer\n   Resource Limit     Load balancers 1,000   Rules per resource 250   Rules per NIC (across all IPs on a NIC) 300   Frontend IP configurations 200   Backend pool size 300 IP configurations, single availability set   Availability sets per Load Balancer 1   Load Balancers per VM 2 (1 Public and 1 internal)     There is a restriction of 300 rules per NIC, hence for single SLB mode 300 services are allowed at most. If more services are required, try to enable multiple SLBs.\n ","excerpt":"The way Azure defines a LoadBalancer is different from GCE or AWS. …","ref":"/cloud-provider-azure/topics/loadbalancer/","title":"Azure LoadBalancer"},{"body":"cloud-provider-azure uses go modules for Go dependency management.\nUsage Run hack/update-dependencies.sh whenever vendored dependencies change. This takes a minute to complete.\nUpdating dependencies New dependencies causes golang to recompute the minor version used for each major version of each dependency. And golang automatically removes dependencies that nothing imports any more.\nTo upgrade to the latest version for all direct and indirect dependencies of the current module:\n run go get -u \u003cpackage\u003e to use the latest minor or patch releases run go get -u=patch \u003cpackage\u003e to use the latest patch releases run go get \u003cpackage\u003e@VERSION to use the specified version  You can also manually editing go.mod and update the versions in require and replace parts.\nBecause of staging in Kubernetes, manually go.mod updating is required for Kubernetes and its staging packages. In cloud-provider-azure, their versions are set in replace part, e.g.\nreplace ( ... k8s.io/kubernetes =\u003e k8s.io/kubernetes v0.0.0-20190815230911-4e7fd98763aa ) To update their versions, you need switch to $GOPATH/src/k8s.io/kubernetes, checkout to the version you want upgrade to, and finally run the following commands to get the go modules expected version:\ncommit=$(TZ=UTC git --no-pager show --quiet --abbrev=12 --date='format-local:%Y%m%d%H%M%S' --format=\"%cd-%h\") echo \"v0.0.0-$commit\" After this, replace all kubernetes and staging versions (e.g. v0.0.0-20190815230911-4e7fd98763aa in above example) in go.mod.\nAlways run hack/update-dependencies.sh after changing go.mod by any of these methods (or adding new imports).\nSee golang’s go.mod, Using Go Modules and Kubernetes Go modules docs for more details.\n","excerpt":"cloud-provider-azure uses go modules for Go dependency management. …","ref":"/cloud-provider-azure/development/dependencies/","title":"Dependency Management"},{"body":"NOTE This page only applies after Azure cloud provider implementation code has been moved to this repository.\nThere are some ongoing issues and pull requests addressing the Azure cloud provider in Kubernetes repository.\nWhen we turned to use the standalone cloud provider in this repository, those issues and pull requests should also be moved.\nHere are some notes for issues and pull requests migration.\nIssue migration If issue applies only to Azure cloud provider, please close it and create a new one in this repository.\nIf issue also involves other component, leave it there, but do create a new issue in this repository to track counterpater in Azure cloud provider.\nIn both cases, leave a link to the new created issue in the old issue.\nPull request migration Basically we have migrated code from k8s.io/legacy-cloud-providers/azure/ to github.com/sigs.k8s.io/cloud-provider-azure/pkg/provider.\nThe following steps describe how to port an existing PR from kubernetes repository to this repository.\n Generate pull request patch  In your kubernetes repository, run following to generate a patch for your PR.\n PR_ID: Pull Request ID in kubernetes repository UPSTREAM_BRANCH: Branch name pointing to upstream, basically the branch with url https://github.com/kubernetes/kubernetes.git or https://k8s.io/kubernetes  PR_ID= UPSTREAM_BRANCH=origin PR_BRANCH_LOCAL=PR$PR_ID git fetch $UPSTREAM_BRANCH pull/$PR_ID/head:$PR_BRANCH_LOCAL MERGE_BASE=$(git merge-base $UPSTREAM_BRANCH/master $PR_BRANCH_LOCAL) PATCH_FILE=/tmp/${PR_ID}.patch git diff $MERGE_BASE $PR_BRANCH_LOCAL \u003e $PATCH_FILE git branch -D $PR_BRANCH_LOCAL Transform the patch and apply  Switch to kubernetes-azure-cloud-controller-manager repo. Apply the patch:\nhack/transform-patch.pl $PATCH_FILE | git apply If any of file in the patch does not fall under Azure cloud provider directory, the transform script will prompt a warning.\n","excerpt":"NOTE This page only applies after Azure cloud provider implementation …","ref":"/cloud-provider-azure/contribute/issues-and-pull-requests-migration/","title":"Issues and pull requests migration"},{"body":"","excerpt":"","ref":"/cloud-provider-azure/faq/known-issues/","title":"Known Issues"},{"body":"The AKS-engine supports deploying clusters with customized Cloud Controller Manager (CCM) and Cloud Node Manager (CNM) images. The API model is defined here. Follow this guide to build your CCM and CNM images and fill them into the AKS-engine API model.\nTo manually deploy an out-of-tree cluster, we need to deploy the following manifests. Note that there are some restrictions when setting config flags. To get more infomation, checkout this doc.\ncloud-controller-manager\ncloud-node-manager\nkube-apiserver\nkube-controller-manager\nkube-shedular\nkubelet command\n","excerpt":"The AKS-engine supports deploying clusters with customized Cloud …","ref":"/cloud-provider-azure/example/out-of-tree/","title":"Deploy with Out-of-tree Cloud Provider Azure"},{"body":"Azure cloud provider requires a set of permissions to manage the Azure resources. Here is a list of all permissions and reasons of why they’re required.\n// Required to create, delete or update LoadBalancer for LoadBalancer service Microsoft.Network/loadBalancers/delete Microsoft.Network/loadBalancers/read Microsoft.Network/loadBalancers/write // Required to allow query, create or delete public IPs for LoadBalancer service Microsoft.Network/publicIPAddresses/delete Microsoft.Network/publicIPAddresses/read Microsoft.Network/publicIPAddresses/write // Required if public IPs from another resource group are used for LoadBalancer service // This is because of the linked access check when adding the public IP to LB frontendIPConfiguration Microsoft.Network/publicIPAddresses/join/action // Required to create or delete security rules for LoadBalancer service Microsoft.Network/networkSecurityGroups/read Microsoft.Network/networkSecurityGroups/write // Required to create, delete or update AzureDisks Microsoft.Compute/disks/delete Microsoft.Compute/disks/read Microsoft.Compute/disks/write Microsoft.Compute/locations/DiskOperations/read // Required to create, update or delete storage accounts for AzureFile or AzureDisk Microsoft.Storage/storageAccounts/delete Microsoft.Storage/storageAccounts/listKeys/action Microsoft.Storage/storageAccounts/read Microsoft.Storage/storageAccounts/write Microsoft.Storage/operations/read // Required to create, delete or update routeTables and routes for nodes Microsoft.Network/routeTables/read Microsoft.Network/routeTables/routes/delete Microsoft.Network/routeTables/routes/read Microsoft.Network/routeTables/routes/write Microsoft.Network/routeTables/write // Required to query information for VM (e.g. zones, faultdomain, size and data disks) Microsoft.Compute/virtualMachines/read // Required to attach AzureDisks to VM Microsoft.Compute/virtualMachines/write // Required to query information for vmssVM (e.g. zones, faultdomain, size and data disks) Microsoft.Compute/virtualMachineScaleSets/read Microsoft.Compute/virtualMachineScaleSets/virtualMachines/read Microsoft.Compute/virtualMachineScaleSets/virtualmachines/instanceView/read // Required to add VM to LoadBalancer backendAddressPools Microsoft.Network/networkInterfaces/write // Required to add vmss to LoadBalancer backendAddressPools Microsoft.Compute/virtualMachineScaleSets/write // Required to attach AzureDisks and add vmssVM to LB Microsoft.Compute/virtualMachineScaleSets/virtualmachines/write // Required to upgrade VMSS models to latest for all instances // only needed for Kubernetes 1.11.0-1.11.9, 1.12.0-1.12.8, 1.13.0-1.13.5, 1.14.0-1.14.1 Microsoft.Compute/virtualMachineScaleSets/manualupgrade/action // Required to query internal IPs and loadBalancerBackendAddressPools for VM Microsoft.Network/networkInterfaces/read // Required to query internal IPs and loadBalancerBackendAddressPools for vmssVM microsoft.Compute/virtualMachineScaleSets/virtualMachines/networkInterfaces/read // Required to get public IPs for vmssVM Microsoft.Compute/virtualMachineScaleSets/virtualMachines/networkInterfaces/ipconfigurations/publicipaddresses/read // Required to check whether subnet existing for ILB in another resource group Microsoft.Network/virtualNetworks/read Microsoft.Network/virtualNetworks/subnets/read // Required to create, update or delete snapshots for AzureDisk Microsoft.Compute/snapshots/delete Microsoft.Compute/snapshots/read Microsoft.Compute/snapshots/write // Required to get vm sizes for getting AzureDisk volume limit Microsoft.Compute/locations/vmSizes/read Microsoft.Compute/locations/operations/read ","excerpt":"Azure cloud provider requires a set of permissions to manage the Azure …","ref":"/cloud-provider-azure/topics/azure-permissions/","title":"Azure Permissions"},{"body":"Azure disk Container Storage Interface (CSI) Storage Plugin is moved to https://github.com/kubernetes-sigs/azuredisk-csi-driver. Please check the github link for the documentation.\n","excerpt":"Azure disk Container Storage Interface (CSI) Storage Plugin is moved …","ref":"/cloud-provider-azure/install/azuredisk/","title":"Deploy AzureDisk CSI Driver"},{"body":"","excerpt":"","ref":"/cloud-provider-azure/development/e2e/","title":"E2E tests"},{"body":"Release source There are two major code change sources for this project, either may push forward a new release for Kubernetes azure-cloud-controller-manager:\n  Changes in Kubernetes cloud-controller-manager, which happens in Kubernetes repository Since this project dependes on Kubernetes cloud-controller-manager, we’ll periodically sync changes from Kubernetes upstream repository. When upstream shipped a new release tag, we may consider publishing a new release\n  Changes in Azure cloud provider, which happens directly in this repository Azure cloud provider also accepts new features and bug changes. In cases when a security fix is required or when the changes accumulated to certain amount, we may also consider publishing a new release, even if there is no change from Kubernetes upstream.\n  Versioning This project is a Kubernetes component whereas the functionalities and APIs all go with Kubernetes upstream project, thus we will use same versioning mechanism of Kubernetes, with some subtle differences for Azure cloud provider and non-Kubernetes changes.\nThe basic rule is:\n Every release version follows Semantic Versioning, in the form of MAJOR.MINOR.PATCH For MAJOR.MINOR, it keeps same value as the Kubernetes upstream For PATCH, it is calculated independently:  If upstream Kubernetes has a new a patch release, which introduces change in cloud-controller-manager or any component we depend on, then sync the change and increase the PATCH number. If any code change happens in Azure cloud provider or other dependency projects, which becomes eligible for a new release, then increase the PATCH number.    References:\n Kubernetes Release Versioning Semantic Versioning  Branch and version scheme This project uses golang’s vendoring mechanism for managing dependencies (see Dependency management for detail). When talking about ‘sync from Kubernetes upstream’, it actually means vendoring Kubernetes repository code under the vendor directory.\nDuring each sync from upstream, it is usually fine to sync to latest commit. But if there is a new tagged commit in upstream that we haven’t vendored, we should sync to that tagged commit first, and apply a version tag correspondingly if applicable. The version tag mechanism is a bit different on master branch and releasing branch, please see below for detail.\nThe upstream syncing change should be made in a single Pull Request. If in some case, the upstream change causes a test break, then the pull requests should not be merged until follow up fix commits are added.\nFor example, if upstream change adds a new cloud provider interface, syncing the upstream change may raise a test break, and we should add the implementation (even no-op) in same pull request.\nmaster branch This is the main development branch for merging pull requests. When upgrading dependencies, it will sync from Kubernetes upstream’s master branch.\nFixes to releasing branches should be merged in master branch first, and then ported to corresponding release branch.\nVersion tags:\n X.Y.0-alpha.0  This is initial tag for a new release, it will be applied when a release branch is created. See below for detail   X.Y.0-alpha.W, W \u003e 0  Those version tags are periodically created if enough change accumulated. It does not have direct mapping with X.Y.0-alpha.W in Kubernetes upstream    releasing branch For release X.Y, the branch will have name release-X.Y. When upgrading dependencies, it will sync with Kubernetes upstream’s release-X.Y branch. Release branch would be created when upstream release branch is created and first X.Y.0-beta.0 tag is applied.\nVersion tags:\n X.Y.0-beta.0  X.Y.0-beta.0 would be tagged at first independent commit on release branch, the corresponding separation point commit on master would be tagged X.Y+1.0-alpha.0 No new feature changes are allowed from this time on   X.Y.0-beta.W, W \u003e 0  Those version tags are periodically created if enough change accumulated. It does not have direct mapping with X.Y.0-beta.W in Kubernetes upstream   X.Y.0  This is the final release version. When upstream X.Y.0 tag rolls out, we will begin prepare X.Y.0 release After merging upstream X.Y.0 tag commit, we will run full test cycle to ensure the Azure cloud provider works well before release:  If any test fails, prepare fixes first. If the fix also applies to master branch, then also apply it to master. Rerun full test cycle till all tests got passed stablely Finally, apply X.Y.0 to latest commit of releasing branch   X.Y.1-beta.0 will be tagged at the same commit   X.Y.Z, Z \u003e 0  Those version tags are periodically created if enough change accumulated. It does not have direct mapping with X.Y.Z in Kubernetes upstream Testing and release process follows same rule as X.Y.0    CI and dev version scheme We use git-describe as versioning source, please check version for detail.\nIn this case, for commits that does not have a certain tag, the result version would be something like ‘v0.1.0-alpha.0-25-gd7999d10’.\n","excerpt":"Release source There are two major code change sources for this …","ref":"/cloud-provider-azure/contribute/release-versioning/","title":"Release Versioning"},{"body":"Feature Status: Alpha since v1.12.\nKubernetes v1.12 adds support for Azure availability zones (AZ). Nodes in availability zone will be added with label failure-domain.beta.kubernetes.io/zone=\u003cregion\u003e-\u003cAZ\u003e and topology-aware provisioning is added for Azure managed disks storage class.\nTOC:\n Availability Zones  Pre-requirements Node labels Load Balancer Managed Disks  StorageClass examples PV examples   Appendix Reference    Pre-requirements Because only standard load balancer is supported with AZ, it is a prerequisite to enable AZ for the cluster. It should be configured in Azure cloud provider configure file (e.g. /etc/kubernetes/azure.json):\n{ \"loadBalancerSku\": \"standard\", ... } If topology-aware provisioning feature is used, feature gate VolumeScheduling should be enabled on master components (e.g. kube-apiserver, kube-controller-manager and kube-scheduler).\nNode labels Both zoned and unzoned nodes are supported, but the value of node label failure-domain.beta.kubernetes.io/zone are different:\n For zoned nodes, the value is \u003cregion\u003e-\u003cAZ\u003e, e.g. centralus-1. For unzoned nodes, the value is faultDomain, e.g. 0.  e.g.\n$ kubectl get nodes --show-labelsNAME STATUS AGE VERSION LABELSkubernetes-node12 Ready 6m v1.11 failure-domain.beta.kubernetes.io/region=centralus,failure-domain.beta.kubernetes.io/zone=centralus-1,...Load Balancer loadBalancerSku has been set to standard in cloud provider configure file, so standard load balancer and standard public IPs will be provisioned automatically for services with type LoadBalancer. Both load balancer and public IPs are zone redundant.\nManaged Disks Zone-aware and topology-aware provisioning are supported for Azure managed disks. To support these features, a few options are added in AzureDisk storage class:\n zoned: indicates whether new disks are provisioned with AZ. Default is true. allowedTopologies: indicates which topologies are allowed for topology-aware provisioning. Only can be set if zoned is not false.  StorageClass examples An example of zone-aware provisioning storage class is:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:annotations:labels:kubernetes.io/cluster-service:\"true\"name:managed-premiumparameters:kind:Managedstorageaccounttype:Premium_LRSzoned:\"true\"provisioner:kubernetes.io/azure-diskvolumeBindingMode:WaitForFirstConsumerAnother example of topology-aware provisioning storage class is:\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:annotations:labels:kubernetes.io/cluster-service:\"true\"name:managed-premiumparameters:kind:Managedstorageaccounttype:Premium_LRSprovisioner:kubernetes.io/azure-diskvolumeBindingMode:WaitForFirstConsumerallowedTopologies:- matchLabelExpressions:- key:failure-domain.beta.kubernetes.io/zonevalues:- centralus-1- centralus-2PV examples When feature gate VolumeScheduling disabled, no NodeAffinity set for zoned PV:\n$ kubectl describe pv Name: pvc-d30dad05-9ad8-11e8-94f2-000d3a07de8c Labels: failure-domain.beta.kubernetes.io/region=southeastasia failure-domain.beta.kubernetes.io/zone=southeastasia-2 Annotations: pv.kubernetes.io/bound-by-controller=yes pv.kubernetes.io/provisioned-by=kubernetes.io/azure-disk volumehelper.VolumeDynamicallyCreatedByKey=azure-disk-dynamic-provisioner Finalizers: [kubernetes.io/pv-protection] StorageClass: default Status: Bound Claim: default/pvc-azuredisk Reclaim Policy: Delete Access Modes: RWO Capacity: 5Gi Node Affinity: Required Terms: Term 0: failure-domain.beta.kubernetes.io/region in [southeastasia] failure-domain.beta.kubernetes.io/zone in [southeastasia-2] Message: Source: Type: AzureDisk (an Azure Data Disk mount on the host and bind mount to the pod) DiskName: k8s-5b3d7b8f-dynamic-pvc-d30dad05-9ad8-11e8-94f2-000d3a07de8c DiskURI: /subscriptions/\u003csubscription-id\u003e/resourceGroups/\u003crg-name\u003e/providers/Microsoft.Compute/disks/k8s-5b3d7b8f-dynamic-pvc-d30dad05-9ad8-11e8-94f2-000d3a07de8c Kind: Managed FSType: CachingMode: None ReadOnly: false Events: \u003cnone\u003e When feature gate VolumeScheduling enabled, NodeAffinity will be populated for zoned PV:\n$ kubectl describe pv Name: pvc-0284337b-9ada-11e8-a7f6-000d3a07de8c Labels: failure-domain.beta.kubernetes.io/region=southeastasia failure-domain.beta.kubernetes.io/zone=southeastasia-2 Annotations: pv.kubernetes.io/bound-by-controller=yes pv.kubernetes.io/provisioned-by=kubernetes.io/azure-disk volumehelper.VolumeDynamicallyCreatedByKey=azure-disk-dynamic-provisioner Finalizers: [kubernetes.io/pv-protection] StorageClass: default Status: Bound Claim: default/pvc-azuredisk Reclaim Policy: Delete Access Modes: RWO Capacity: 5Gi Node Affinity: Required Terms: Term 0: failure-domain.beta.kubernetes.io/region in [southeastasia] failure-domain.beta.kubernetes.io/zone in [southeastasia-2] Message: Source: Type: AzureDisk (an Azure Data Disk mount on the host and bind mount to the pod) DiskName: k8s-5b3d7b8f-dynamic-pvc-0284337b-9ada-11e8-a7f6-000d3a07de8c DiskURI: /subscriptions/\u003csubscription-id\u003e/resourceGroups/\u003crg-name\u003e/providers/Microsoft.Compute/disks/k8s-5b3d7b8f-dynamic-pvc-0284337b-9ada-11e8-a7f6-000d3a07de8c Kind: Managed FSType: CachingMode: None ReadOnly: false Events: \u003cnone\u003e While unzoned disks are not able to attach in zoned nodes, NodeAffinity will also be set for them so that they will only be scheduled to unzoned nodes:\n$ kubectl describe pv pvc-bdf93a67-9c45-11e8-ba6f-000d3a07de8c Name: pvc-bdf93a67-9c45-11e8-ba6f-000d3a07de8c Labels: \u003cnone\u003e Annotations: pv.kubernetes.io/bound-by-controller=yes pv.kubernetes.io/provisioned-by=kubernetes.io/azure-disk volumehelper.VolumeDynamicallyCreatedByKey=azure-disk-dynamic-provisioner Finalizers: [kubernetes.io/pv-protection] StorageClass: azuredisk-unzoned Status: Bound Claim: default/unzoned-pvc Reclaim Policy: Delete Access Modes: RWO Capacity: 5Gi Node Affinity: Required Terms: Term 0: failure-domain.beta.kubernetes.io/region in [southeastasia] failure-domain.beta.kubernetes.io/zone in [0] Term 1: failure-domain.beta.kubernetes.io/region in [southeastasia] failure-domain.beta.kubernetes.io/zone in [1] Term 2: failure-domain.beta.kubernetes.io/region in [southeastasia] failure-domain.beta.kubernetes.io/zone in [2] Message: Source: Type: AzureDisk (an Azure Data Disk mount on the host and bind mount to the pod) DiskName: k8s-5b3d7b8f-dynamic-pvc-bdf93a67-9c45-11e8-ba6f-000d3a07de8c DiskURI: /subscriptions/\u003csubscription\u003e/resourceGroups/\u003crg-name\u003e/providers/Microsoft.Compute/disks/k8s-5b3d7b8f-dynamic-pvc-bdf93a67-9c45-11e8-ba6f-000d3a07de8c Kind: Managed FSType: CachingMode: None ReadOnly: false Events: \u003cnone\u003e Appendix Note that unlike most cases, fault domain and availability zones mean different on Azure:\n A Fault Domain (FD) is essentially a rack of servers. It consumes subsystems like network, power, cooling etc. Availability Zones are unique physical locations within an Azure region. Each zone is made up of one or more data centers equipped with independent power, cooling, and networking.  An Availability Zone in an Azure region is a combination of a fault domain, and an update domain (Same like FD, but for updates. When upgrading a deployment, it is carried out one update domain at a time). For example, if you create three or more VMs across three zones in an Azure region, your VMs are effectively distributed across three fault domains and three update domains.\nReference See design docs for AZ in KEP for Azure availability zones.\n","excerpt":"Feature Status: Alpha since v1.12.\nKubernetes v1.12 adds support for …","ref":"/cloud-provider-azure/topics/availability-zones/","title":"Use Availability Zones"},{"body":"Azure file Container Storage Interface (CSI) Storage Plugin is moved to https://github.com/kubernetes-sigs/azurefile-csi-driver. Please check the github link for the documentation.\n","excerpt":"Azure file Container Storage Interface (CSI) Storage Plugin is moved …","ref":"/cloud-provider-azure/install/azurefile/","title":"Deploy AzureFile CSI Driver"},{"body":"This is the staging area of the design docs prior to or under development. Once the feature is done, the corresponding design doc would be moved to Topics.\n","excerpt":"This is the staging area of the design docs prior to or under …","ref":"/cloud-provider-azure/development/design-docs/","title":"Design Docs and KEPs"},{"body":"Security Announcements Join the kubernetes-security-announce group for security and vulnerability announcements.\nYou can also subscribe to an RSS feed of the above using this link.\nReporting a Vulnerability Instructions for reporting a vulnerability can be found on the Kubernetes Security and Disclosure Information page.\nSupported Versions Information about supported Kubernetes versions can be found on the Kubernetes version and version skew support policy page on the Kubernetes website.\n","excerpt":"Security Announcements Join the kubernetes-security-announce group for …","ref":"/cloud-provider-azure/contribute/security/","title":"Security Policy"},{"body":"Feature status: Alpha since v1.12.\nKubernetes v1.12 adds support for cross resource group (RG) nodes and unmanaged (such as on-prem) nodes in Azure cloud provider. A few assumptions are made for such nodes:\n Cross-RG nodes are in same region and set with required labels (as clarified in the following part) Nodes will not be part of the load balancer managed by cloud provider Both node and container networking should be configured properly by provisioning tools AzureDisk is supported for Azure cross-RG nodes, but not for on-prem nodes  TOC:\n Cross resource group nodes  Pre-requirements Cross-RG nodes Unmanaged nodes Reference    Pre-requirements Because cross-RG nodes and unmanaged nodes won’t be added to Azure load balancer backends, feature gate ServiceNodeExclusion should be enabled for master components (e.g. kube-controller-manager).\nCross-RG nodes Cross-RG nodes should register themselves with required labels together with cloud provider:\n node.kubernetes.io/exclude-balancer, which is used to exclude the node from load balancer.  alpha.service-controller.kubernetes.io/exclude-balancer=true should be used if the cluster version is below v1.16.0.   kubernetes.azure.com/resource-group=\u003crg-name\u003e, which provides external RG and is used to get node information. cloud provider config  --cloud-provider=azure when using kube-controller-manager --cloud-provider=external when using cloud-controller-manager    For example,\nkubelet ... \\  --cloud-provider=azure \\  --cloud-config=/etc/kubernetes/azure.json \\  --node-labels=node.kubernetes.io/exclude-balancer=true,kubernetes.azure.com/resource-group=\u003crg-name\u003e Unmanaged nodes On-prem nodes are different from Azure nodes, all Azure coupled features (such as load balancers and Azure managed disks) are not supported for them. To prevent the node being deleted, Azure cloud provider will always assumes the node existing.\nOn-prem nodes should register themselves with labels node.kubernetes.io/exclude-balancer=true and kubernetes.azure.com/managed=false:\n node.kubernetes.io/exclude-balancer=true, which is used to exclude the node from load balancer. kubernetes.azure.com/managed=false, which indicates the node is on-prem or on other clouds.  For example,\nkubelet ...\\  --cloud-provider= \\  --node-labels=node.kubernetes.io/exclude-balancer=true,kubernetes.azure.com/managed=false Reference See design docs for cross resource group nodes in KEP 20180809-cross-resource-group-nodes.\n","excerpt":"Feature status: Alpha since v1.12.\nKubernetes v1.12 adds support for …","ref":"/cloud-provider-azure/topics/cross-resource-group-nodes/","title":"Deploy Cross Resource Group Nodes"},{"body":"To be completed.\n","excerpt":"To be completed.\n","ref":"/cloud-provider-azure/development/future/","title":"Future Plans"},{"body":" This feature is supported since v1.20.0.\n Provider Azure supports sharing one IP address among multiple load balancer typed external or internal services. To share an IP address among multiple public services, a public IP resource is needed. This public IP could be created in advance or let the cloud provider provision it when creating the first external service. Specifically, Azure would create a public IP resource automatically when an external service is discovered.\napiVersion:v1kind:Servicemetadata:name:nginxnamespace:defaultspec:ports:- port:80protocol:TCPtargetPort:80selector:app:nginxtype:LoadBalancerNote that the loadBalancerIP is not set, or Azure would find a pre-allocated public IP with the address. After obtaining the IP address of the service, you could create other services using this address.\napiVersion:v1kind:Servicemetadata:name:httpsnamespace:defaultspec:loadBalancerIP:1.2.3.4# the IP address could be the same as it is of `nginx` serviceports:- port:443protocol:TCPtargetPort:443selector:app:httpstype:LoadBalancerNote that if you specify the loadBalancerIP but there is no corresponding public IP pre-allocated, an error would be reported.\nDNS Even if multiple services can refer to one public IP, the DNS label cannot be re-used. The public IP would have the label kubernetes-dns-label-service: \u003csvcName\u003e to indicate which service is binding to the DNS label. In this case if there is another service sharing this specific IP address trying to refer to the DNS label, an error would be reported.\n The DNS name on the public IP won’t be deleted after the service with the DNS annotation being deleted, because the cloud provider don’t know if the DNS was set by the user or not.\n Restrictions The cloud provider azure manages the lifecycle of the system-created public IPs. By default, there are two kinds of system managed tags: kubernetes-cluster-name and service (see the picture below). The controller manager would add the service name to the service if a service is trying to refer to the public IP, and remove the name from the service if the service is deleted. The public IP would be deleted if there is no service in the tag service. However, according to the docs of azure tags, there are several restrictions:\n  Each resource, resource group, and subscription can have a maximum of 50 tag name/value pairs. If you need to apply more tags than the maximum allowed number, use a JSON string for the tag value. The JSON string can contain many values that are applied to a single tag name. A resource group or subscription can contain many resources that each have 50 tag name/value pairs.\n  The tag name is limited to 512 characters, and the tag value is limited to 256 characters. For storage accounts, the tag name is limited to 128 characters, and the tag value is limited to 256 characters.\n  Based to that, we suggest to use static public IPs when there are more than 10 services sharing the IP address.\n","excerpt":" This feature is supported since v1.20.0.\n Provider Azure supports …","ref":"/cloud-provider-azure/topics/shared-ip/","title":"Multiple Services Sharing One IP Address"},{"body":" This feature is supported since v1.20.0.\n We could use tags to organize your Azure resources and management hierarchy. Cloud Provider Azure supports tagging managed resource through configuration file or service annotation.\nSpecifically, the shared resources (load balancer, route table and security group) could be tagged by setting tags in azure.json:\n{ \"tags\": \"a=b,c=d\", } the controller manager would parse this configuration and tag the shared resources once restarted.\nThe non-shared resource (public IP) could be tagged by setting tags in azure.json or service annotation service.beta.kubernetes.io/azure-pip-tags. The format of the two is similiar and the tags in the annotation would be considered first when there are conflicts between the configuration file and the annotation.\n The annotation service.beta.kubernetes.io/azure-pip-tags only works for managed public IPs. For BYO public IPs, the cloud provider would not apply any tags to them.\n When the configuration, file or annotation, is updated, the old ones would be updated if there are conflicts. For example, after updating {\"tags\": \"a=b,c=d\"} to {\"tags\": \"a=c,e=f\"}, the new tags would be a=c,c=d,e=f.\nIntegrating with system tags  This feature is supported since v1.21.0.\n Normally the controller manager don’t delete the existing tags even if they are not included in the new version of azure configuration files, because the controller manager doesn’t know which tags should be deleted and which should not (e.g., tags managed by cloud provider itself). We can leverage the config systemTags in the cloud configuration file to control what tags can be deleted. Here are the examples:\n   Tags SystemTags existing tags on resources new tags on resources     “a=b,c=d” \"\" {} {“a”: “b”, “c”: “d”}   “a=b,c=d” \"\" {“a”: “x”, “c”: “y”} {“a”: “b”, “c”: “d”}   “a=b,c=d” \"\" {“e”: “f”} {“a”: “b”, “c”: “d”, “e”: “f”} /* won’t delete e because the SystemTags is empty */   “c=d” “a” {“a”: “b”} {“a”: “b”, “c”: “d”} /* won’t delete a because it’s in the SystemTags */   “c=d” “x” {“a”: “b”} {“c”: “d”} /* will delete a because it’s not in Tags or SystemTags */    ","excerpt":" This feature is supported since v1.20.0.\n We could use tags to …","ref":"/cloud-provider-azure/topics/tagging-resources/","title":"Tagging resources managed by Cloud Provider Azure"},{"body":" This feature is supported since v1.20.0.\n There is only one external and one internal Standard Load Balancer (SLB) at most per cluster. Set enableMultipleStandardLoadBalancers=true in the cloud config if you want to turn on the multiple SLB mode. Similar to the basic LB, there will be a 1:1 mapping between each SLB and VMSS/VMAS. The SLB of the primary VMSS/VMAS will be named after clusterName in the cloud config (in AKS, it would be kubernetes) while the name of those belonging to non-primary VMSS/VMAS will be the name of the corresponding vmSet.\n If the cluster provisioning tools like ASK-Engine and CAPZ don’t proactively create a dedicated SLB for each VMSS/VMAS when enabling multiple SLB, only the primary SLB would be created. You could manually trigger the creation by setting the service annotation service.beta.kubernetes.io/azure-load-balancer-mode to bind the service to that VMSS/VMAS. The dedicated SLB would be created once the service reconcile loop is done. Unlike the primary SLB, there is no default outbound rules/IPs for the non-primary SLBs. That means the SLB would be deleted once all the services referencing it are deleted.\n Choose which SLB to use The service annotation service.beta.kubernetes.io/azure-load-balancer-mode will be respected as long as enableMultipleStandardLoadBalancers=true when using standard LB, and the usage is the same as it is in the basic LB clusters. Specifically, there are three selection mode: default to select the primary SLB; __auto__ to select the SLB with minimum rules and vmSetName to select the dedicated SLB of that VMSS/VMAS.\nOutbound Connections of non-primary VMSS/VMAS The outbound rules of the non-primary SLB are not managed by cloud provider azure. Instead, it should be managed by cluster provisioning tools. For now, there is no outbound configuration for the non-primary VMSS/VMAS, but we plan to support customized outbound configurations in AKS and CAPZ in the future.\nSharing the primary SLB with multiple VMSS/VMAS  This feature is supported since v1.21.0\n For each non-primary VMSS/VMAS, one can determine to use dedicated SLB or share the primary SLB. If the VMSS/VMAS names are in the cloud config nodepoolsWithoutDedicatedSLB, those would join the backend pool of the primary SLB while the others would remain to have dedicated SLBs. If the VMSS/VMAS supposed to share the primary SLB owns a dedicated SLB, the dedicated one would be deleted, and the VMSS/VMAS would be joint the primary SLB’s backend pool.\n","excerpt":" This feature is supported since v1.20.0.\n There is only one external …","ref":"/cloud-provider-azure/topics/multiple-slb/","title":"Multiple Standard LoadBalancer per cluster"},{"body":"Currently, the protocol of the health probe is defined as below:\n for local services, HTTP and /healthz would be used; for cluster TCP services, TCP would be used; for cluster UDP services, no health probes.  Since v1.20, two service annotations service.beta.kubernetes.io/azure-load-balancer-health-probe-protocol and service.beta.kubernetes.io/azure-load-balancer-health-probe-request-path are introduced, which determine the new health probe behavior. If the service.beta.kubernetes.io/azure-load-balancer-health-probe-protocol is set, both local and cluster TCP services would use the specified health probe protocol. If the service.beta.kubernetes.io/azure-load-balancer-health-probe-request-path is set, the specified request path would be used instead of /healthz. Note that the request path would be ignored when using TCP or the service.beta.kubernetes.io/azure-load-balancer-health-probe-protocol is empty. More specifically:\n   externalTrafficPolicy service.beta.kubernetes.io/azure-load-balancer-health-probe-protocol service.beta.kubernetes.io/azure-load-balancer-health-probe-request-path protocol request path     local  (ignored) http /healthz   local tcp (ignored) tcp null   local http/https  http/https /healthz   local http/https /custom-path http/https /custom-path   cluster  (ignored) tcp null   cluster tcp (ignored) tcp null   cluster http/https  http/https /healthz   cluster http/https /custom-path http/https /custom-path    ","excerpt":"Currently, the protocol of the health probe is defined as below:\n for …","ref":"/cloud-provider-azure/topics/custom-health-probe/","title":"Custom load balancer health probe"},{"body":" This feature is supported since v1.21.0.\n Background The in-tree Node IPAM controller only supports a fixed node CIDR mask size for all nodes, while in multiple node pool (VMSS) scenarios, different mask sizes are required for different node pools. There is a GCE-specific cloud CIDR allocator for a similar scenario, but that is not exposed in cloud provider API and it is planned to be moved out-of-tree.\nHence this docs proposes an out-of-tree node IPAM controller. Specifically, allocate different pod CIDRs based on different CIDR mask size for different node pools (VMSS or VMAS).\nUsage There are two kinds of CIDR allocator in the node IPAM controller, which are RangeAllocator and CloudAllocator. The RangeAllocator is the default one which allocates the pod CIDR for every node in the range of the cluster CIDR. The CloudAllocator allocates the pod CIDR for every node in the range of the CIDR on the corresponding VMSS or VMAS.\nThe pod CIDR mask size of each node that belongs to a specific VMSS or VMAS is set by a specific tag {\"kubernetesNodeCIDRMaskIPV4\": \"24\"} or {\"kubernetesNodeCIDRMaskIPV6\": \"64\"}. Note that the mask size tagging on the VMSS or VMAS must be within the cluster CIDR, or an error would be thrown.\nWhen the above tag doesn’t exist on VMSS/VMAS, the default mask size (24 for ipv4 and 64 for ipv6) would be used.\nTo turn on the out-of-tree node IPAM controller:\n Disable the in-tree node IPAM controller by setting --allocate-node-cidrs=false in kube-controller-manager. Enable the out-of-tree counterpart by setting --allocate-node-cidrs=true in cloud-controller-manager. To use RangeAllocator:  configure the --cluster-cidr, --service-cluster-ip-range and --node-cidr-mask-size; if you enable the ipv6 dualstack, setting --node-cidr-mask-size-ipv4 and --node-cidr-mask-size-ipv6 instead of --node-cidr-mask-size. An error would be reported if --node-cidr-mask-size and --node-cidr-mask-size-ipv4 (or --node-cidr-mask-size-ipv6) are set to non-zero values at a time. If only --node-cidr-mask-size is set, which is not recommended, the --node-cidr-mask-size-ipv4 and --node-cidr-mask-size-ipv6 would be set to this value by default.   To use CloudAllocator:  set the --cidr-allocator-type=CloudAllocator; configure mask sizes of each VMSS/VMAS by tagging {\"kubernetesNodeCIDRMaskIPV4\": \"custom-mask-size\"} and {\"kubernetesNodeCIDRMaskIPV4\": \"custom-mask-size\"} if necessary.    Configurations kube-controller-manager kube-controller-manager would be configured with option --allocate-node-cidrs=false to disable the in-tree node IPAM controller.\ncloud-controller-manager The following configurations from cloud-controller-manager would be used as default options:\n   name type default description     allocate-node-cidrs bool true Should CIDRs for Pods be allocated and set on the cloud provider.   cluster-cidr string “10.244.0.0/16” CIDR Range for Pods in cluster. Requires –allocate-node-cidrs to be true.   service-cluster-ip-range string \"\" CIDR Range for Services in cluster, this would get excluded from the allocatable range. Requires –allocate-node-cidrs to be true.   node-cidr-mask-size int 24 Mask size for node cidr in cluster. Default is 24 for IPv4 and 64 for IPv6.   node-cidr-mask-size-ipv4 int 24 Mask size for IPv4 node cidr in dual-stack cluster. Default is 24.   node-cidr-mask-size-ipv6 int 64 Mask size for IPv6 node cidr in dual-stack cluster. Default is 64.   cidr-allocator-type string “RangeAllocator” The CIDR allocator type. “RangeAllocator” or “CloudAllocator”.    Limitations  We plan to integrate out-of-tree node ipam controller with aks-engine to provider a better experience. Before that, the manual configuration is required. It is not supported to change the custom mask size value on the tag once it is set. For now, there is no e2e test covering this feature, so there can be potential bugs. It is not recommended enabling it in the production environment.  ","excerpt":" This feature is supported since v1.21.0.\n Background The in-tree Node …","ref":"/cloud-provider-azure/topics/ipam/","title":"Node IPAM controller"},{"body":"Changelog since v0.7.5 Changes by Kind Feature  Feat: Provide IPv6 support for internal load balancer (#703, @tomkerkhove)  Bug or Regression  Fix: not send availability zones as part of create for edge zones (#709, @MirzaSikander)  ","excerpt":"Changelog since v0.7.5 Changes by Kind Feature  Feat: Provide IPv6 …","ref":"/cloud-provider-azure/blog/2021/07/20/v0.7.6/","title":"v0.7.6"},{"body":"Changelog since v1.0.2 Changes by Kind Feature  Feat: Provide IPv6 support for internal load balancer (#703, @tomkerkhove)  Bug or Regression  Fix: not send availability zones as part of create for edge zones (#709, @MirzaSikander)  ","excerpt":"Changelog since v1.0.2 Changes by Kind Feature  Feat: Provide IPv6 …","ref":"/cloud-provider-azure/blog/2021/07/20/v1.0.3/","title":"v1.0.3"},{"body":"Major changes since v0.7.4 Changes by Kind Feature  Chore: upgrade TLS1.0 to TLS1.2 in account creation (#675, @andyzhangx) Feat: Enable creation of storage accounts that support large file shares (#606, @nearora-msft) Feat: add support for additional public IPs via service annotation “service.beta.kubernetes.io/azure-additional-public-ips” (#691, @feiskyer) Feat: enable creation of private endpoint for storage account (#652, @nearora-msft) Feat: support reloading cloud controller manager from secret dynamically (#613, @nilo19)  Documentation  Chore: update docs for service tags NSG (#647, @feiskyer) Docs: add ‘securityGroupResourceGroup’ cloud-config value (#668, @aslafy-z)  Failing Test  Fix: serviceOwnsFrontendIP shouldn’t report error when the public IP doesn’t match (#649, @feiskyer)  Bug or Regression  Do not set cached Sku when updating VMSS and VMSS instances (#630, @feiskyer) Fix: avoid nil-pointer panic when checking the frontend IP configuration (#615, @nilo19) Fix: cleanup outdated routes (#661, @nilo19) Fix: detach disk panic on Azure Stack (#688, @andyzhangx) Fix: make tags case-insensitive for both keys and values (#669, @nilo19) Fix: not tagging static public IP (#616, @nilo19) Fix: remove GetDisk operation in AttachDisk (#678, @andyzhangx) Fix: return empty VMAS name if using standalone VM (#679, @nilo19)  Other (Cleanup or Flake)  Chore: add e2e test for byo public IP (#627, @nilo19) Chore: add more buckets for operation metrics (#656, @andyzhangx) Chore: completely decouple k/k (#601, @nilo19) Chore: only logs rate limiting configurations when rate limit is enabled (#608, @feiskyer) Chore: only put pip if it is necessary (#686, @nilo19) Chore: only reconciling routes in cloud controller manager (#671, @nilo19) Chore: set default config secret name and namespace (#662, @nilo19) Fix: wait for the success of the initial run of syncRegionZonesMap (#646, @nilo19) Update Azure Go SDK to v55.0.0 (#643, @feiskyer) Update cloud-provider vendor to v1.21 (#603, @feiskyer) Upgrade to 2020-02-01/storage (#628, @andyzhangx) GetTestCloud properly initializes the disk controller fields enabling them to be used in unit tests and mocked. (#689, @edreed)  Uncategorized  Add CreateOrUpdateBackendPools() interface for LoadBalancer client (#620, @feiskyer) Feat: add NFSv3 account creation support (#633, @andyzhangx) Fix: leave the probe path empty for TCP probes (#680, @nilo19) Fix: no sleep when GetDisk is throttled (#629, @andyzhangx)  Dependencies Added Nothing has changed.\nChanged Nothing has changed.\nRemoved Nothing has changed.\n","excerpt":"Major changes since v0.7.4 Changes by Kind Feature  Chore: upgrade …","ref":"/cloud-provider-azure/blog/2021/06/21/v0.7.5/","title":"v0.7.5"},{"body":"Major changes since v1.0.0 Cloud Provider Azure v1.0.1 includes several critical bug fixes. The images are available at:\n mcr.microsoft.com/oss/kubernetes/azure-cloud-node-manager:v1.0.1 mcr.microsoft.com/oss/kubernetes/azure-cloud-controller-manager:v1.0.1  Changes by Kind Documentation  Chore: update docs for service tags NSG (#647, @feiskyer) Docs: add ‘securityGroupResourceGroup’ cloud-config value (#668, @aslafy-z)  Failing Test  Fix: serviceOwnsFrontendIP shouldn’t report error when the public IP doesn’t match (#649, @feiskyer)  Bug or Regression  Do not set cached Sku when updating VMSS and VMSS instances (#630, @feiskyer) Fix: cleanup outdated routes (#661, @nilo19) Fix: make tags case-insensitive for both keys and values (#669, @nilo19)  Other (Cleanup or Flake)  Chore: add e2e test for byo public IP (#627, @nilo19) Chore: add more buckets for operation metrics (#656, @andyzhangx) Chore: only reconciling routes in cloud controller manager (#671, @nilo19) Chore: set default config secret name and namespace (#662, @nilo19) Fix: wait for the success of the initial run of syncRegionZonesMap (#646, @nilo19) Update Azure Go SDK to v55.0.0 (#643, @feiskyer) Upgrade to 2020-02-01/storage (#628, @andyzhangx) Add CreateOrUpdateBackendPools() interface for LoadBalancer client (#620, @feiskyer) Feat: add NFSv3 account creation support (#633, @andyzhangx) Fix: no sleep when GetDisk is throttled (#629, @andyzhangx)  ","excerpt":"Major changes since v1.0.0 Cloud Provider Azure v1.0.1 includes …","ref":"/cloud-provider-azure/blog/2021/06/21/v1.0.1/","title":"v1.0.1"},{"body":"Major changes since v1.0.1 Changes by Kind Feature  Chore: upgrade TLS1.0 to TLS1.2 in account creation (#675, @andyzhangx) Feat: add support for additional public IPs via service annotation “service.beta.kubernetes.io/azure-additional-public-ips” (#691, @feiskyer) Feat: enable creation of private endpoint for storage account (#652, @nearora-msft)  Bug or Regression  Fix: detach disk panic on Azure Stack (#688, @andyzhangx) Fix: make tags case-insensitive for both keys and values (#669, @nilo19) Fix: remove GetDisk operation in AttachDisk (#678, @andyzhangx) Fix: return empty VMAS name if using standalone VM (#679, @nilo19)  Other (Cleanup or Flake)  Chore: only put pip if it is necessary (#686, @nilo19) GetTestCloud properly initializes the disk controller fields enabling them to be used in unit tests and mocked. (#689, @edreed)  Uncategorized  Fix: leave the probe path empty for TCP probes (#680, @nilo19)  Dependencies Added Nothing has changed.\nChanged  github.com/Azure/azure-sdk-for-go: v55.0.0+incompatible → v54.1.0+incompatible github.com/Azure/go-autorest/autorest/adal: v0.9.13 → v0.9.10 github.com/Azure/go-autorest/autorest/to: v0.4.0 → v0.2.0 github.com/Azure/go-autorest/autorest: v0.11.18 → v0.11.17 github.com/Azure/go-autorest/logger: v0.2.1 → v0.2.0  Removed Nothing has changed.\n","excerpt":"Major changes since v1.0.1 Changes by Kind Feature  Chore: upgrade …","ref":"/cloud-provider-azure/blog/2021/06/21/v1.0.2/","title":"v1.0.2"},{"body":"Major changes since v0.7.4 Cloud Provider Azure v1.0.0 includes several critical bug fixes. The images are available at:\n mcr.microsoft.com/oss/kubernetes/azure-cloud-node-manager:v1.0.0 mcr.microsoft.com/oss/kubernetes/azure-cloud-controller-manager:v1.0.0  Changes by Kind Feature  Feat: Enable creation of storage accounts that support large file shares (#606, @nearora-msft) Feat: support reloading cloud controller manager from secret dynamically (#613, @nilo19)  Bug or Regression  Fix: avoid nil-pointer panic when checking the frontend IP configuration (#615, @nilo19) Fix: not tagging static public IP (#616, @nilo19)  Other (Cleanup or Flake)  Chore: completely decouple k/k (#601, @nilo19) Chore: only logs rate limiting configurations when rate limit is enabled (#608, @feiskyer) Update cloud-provider vendor to v1.21 (#603, @feiskyer)  ","excerpt":"Major changes since v0.7.4 Cloud Provider Azure v1.0.0 includes …","ref":"/cloud-provider-azure/blog/2021/05/07/v1.0.0/","title":"v1.0.0"},{"body":"Major changes since v0.7.3 Cloud Provider Azure v0.7.4 includes several critical bug fixes. The images are available at:\n mcr.microsoft.com/oss/kubernetes/azure-cloud-node-manager:v0.7.4 mcr.microsoft.com/oss/kubernetes/azure-cloud-controller-manager:v0.7.4  Changes by Kind Documentation  Chore: enrich docs (#590, @nilo19)  Bug or Regression  Fix: arm node provider Windows initialization (#595, @JesusAlvarezTorres) Fix: call the counterpart function of availabilitySet when the instance is not a vmss vm (#597, @nilo19) Fix: potential race condition in detach disk (#593, @andyzhangx) Fix: support sharing the primary slb when there are both external and internal load balancers in the cluster (#588, @nilo19)  ","excerpt":"Major changes since v0.7.3 Cloud Provider Azure v0.7.4 includes …","ref":"/cloud-provider-azure/blog/2021/04/23/v0.7.4/","title":"v0.7.4"},{"body":"Major changes since v0.7.2 Cloud Provider Azure v0.7.3 supports out-of-tree node ipam controller, sharing the primary SLB with multiple vmSets, and a bunch of other features/bug fixes. The images are available at:\n mcr.microsoft.com/oss/kubernetes/azure-cloud-node-manager:v0.7.3 mcr.microsoft.com/oss/kubernetes/azure-cloud-controller-manager:v0.7.3  Changes by Kind Feature  Chore: remove get file in CreateFileShare (#534, @andyzhangx) Feat: add ARM node provider (#580, @JesusAlvarezTorres) Feat: add disable RetentionPolicy parameter (#545, @andyzhangx) Feat: add update vm interface (#592, @andyzhangx) Feat: implement cloud allocator for vmas (#555, @nilo19) Feat: implement cloud cidr allocator for VMSS (#539, @nilo19) Feat: support sharing the primary slb with multiple vmSets (#578, @nilo19) Feat: support system tag (#558, @nilo19) Feat: upgrade azure sdk to v53.1.0 (#589, @andyzhangx)  Documentation  Doc: add doc for out-of-tree node ipam controller (#553, @nilo19)  Bug or Regression  Allow disabling AzureStackCloud API versions when using AzureStackCloud config on public cloud (#525, @feiskyer) Avoid caching the VMSS instances whose network profile is nil (#583, @feiskyer) Azure_storageaccount.go:99] found skip-matching tag for account %!s(*string=0xc000cbd7b0), skip matching (#529, @andyzhangx) Ensure only LoadBalancer rule is created when HA mode is enabled (#536, @feiskyer) Ensure service deleted when the Azure resource group has been deleted (#584, @feiskyer) Fix node public IP fetching from instance metadata service when the node is part of standard load balancer backend pool. (#540, @feiskyer) Fix: avoid panic when RouteTablePropertiesFormat is nil (#568, @feiskyer) Fix: not delete existing pip when service is deleted (#574, @nilo19) Fix: support sharing the primary slb when there are both external and internal load balancers in the cluster (#588, @nilo19) Fixed routes not created issues before Pod scheduling. When using kubenet, 1) cloud-node-manager supports “–wait-routes=true” to indicate a node would wait for route updates before accepting Pod scheduling and 2) route controller would wait a while for new routes to take effect (default is 30s). (#528, @feiskyer) Ignore not a VMSS error for VMAS nodes in reconcileBackendPools (#551, @CecileRobertMichon)  Other (Cleanup or Flake)  Chore: move consts in azure_vmss.go to consts.go (#554, @nilo19) Chore: remove bazel support (#585, @nilo19) Chore: switch to network api 2020-08-01 (#569, @nilo19) Enable docker BuildKit and update Go to 1.15.8 (#548, @CecileRobertMichon) Update Azure compute API version to 2020-12-01 (#579, @feiskyer)  Uncategorized  Fix availability set cache in vmss cache (#537, @CecileRobertMichon) Fix: check disk state before attach disk (#564, @andyzhangx) Fix: convert backend pool id to lower case before using it (#561, @nilo19)  Dependencies Added  github.com/gofrs/uuid: v4.0.0+incompatible  Changed  github.com/Azure/azure-sdk-for-go: v51.2.0+incompatible → v53.1.0+incompatible  ","excerpt":"Major changes since v0.7.2 Cloud Provider Azure v0.7.3 supports …","ref":"/cloud-provider-azure/blog/2021/04/19/v0.7.3/","title":"v0.7.3"},{"body":"Cloud Provider Azure v0.7.2 allows to disable AzureStackCloud API versions when using AzureStackCloud config on public cloud (e.g. for customizing ARM endpoints). The images are available at\n mcr.microsoft.com/oss/kubernetes/azure-cloud-node-manager:v0.7.2 mcr.microsoft.com/oss/kubernetes/azure-cloud-controller-manager:v0.7.2  Changelog since v0.7.1 Changes by Kind Bug or Regression  Allow disabling AzureStackCloud API versions when using AzureStackCloud config on public cloud (#525, @feiskyer)  ","excerpt":"Cloud Provider Azure v0.7.2 allows to disable AzureStackCloud API …","ref":"/cloud-provider-azure/blog/2021/02/28/v0.7.2/","title":"v0.7.2"},{"body":"Changelog since v0.7.0 Changes by Kind Feature  Feat: Add service annotation ServiceAnnotationDenyAllExpectSourceRanges (#487, @nilo19) Feat: skip account matching with special tags (#490, @andyzhangx) Feat: vm client changes for Azure Stack Hub support (#477, @JesusAlvarezTorres) Implement cloudprovider.InstancesV2 interface (#466, @nilo19) Support etag when putting network interface. (#483, @nilo19) Updates all the references for azure network API to point to 2020-07-01 which is the latest API Version (#502, @MirzaSikander)  Bug or Regression  Aggregate errors when putting vmss (#482, @nilo19) Output the actual error when VMSS PUT fails rather than the error from the previous GET (#486, @devigned)  Other (Cleanup or Flake)  Add e2e test for annotation service.beta.kubernetes.io/azure-deny-all-except-load-balancer-source-ranges (#489, @nilo19) Add log level in armclient (#497, @nilo19)  ","excerpt":"Changelog since v0.7.0 Changes by Kind Feature  Feat: Add service …","ref":"/cloud-provider-azure/blog/2021/02/24/v0.7.1/","title":"v0.7.1"},{"body":"Major changes since v0.6.0 Cloud Provider Azure v0.7.0 updates Kubernetes vendor to v1.20 and moves to beta. The images are available at\n mcr.microsoft.com/oss/kubernetes/azure-cloud-node-manager:v0.7.0 mcr.microsoft.com/oss/kubernetes/azure-cloud-controller-manager:v0.7.0  Enhancements  Features inherited from in-tree Azure cloud provider  Multiple standard load balancers in the same cluster. Multiple load balancer typed services sharing one IP address. Customized load balancer health probe. Tagging resources managed by provider azure.   Code improvements inherited from in-tree Azure cloud provider  Add metrics for cloud provider operations like “EnsureLoadBalancer”. Improve the unit/E2E test coverage in provider azure.   Upgrade Azure compute API version to 2020-06-30: (#444, @andyzhangx) Use batch operation for azure disk attach/detach: (#453, @andyzhangx)  Bug Fixes  Fix nil VMSS name when setting service to auto mode (#439, @nilo19) Fix readyz probe (#394, @nilo19) Ignore in-cluster config when --master or --kubeconfig is set explicitly (#397, @nilo19) Support change the LB selection mode on the existing services (#445, @nilo19) Use network.Interface.VirtualMachine.ID to get the VM (#443, @nilo19) Skip the exclude LB test on multi node pool cluster (#455, @nilo19)  Cleanups  Duplicate the in-tree cloud provider code to the out-of-tree repo (#433, @nilo19)  Dependencies Added  cloud.google.com/go/bigquery: v1.4.0 cloud.google.com/go/datastore: v1.1.0 cloud.google.com/go/firestore: v1.1.0 cloud.google.com/go/pubsub: v1.2.0 cloud.google.com/go/storage: v1.6.0 dmitri.shuralyov.com/gpu/mtl: 666a987 github.com/armon/go-metrics: f0300d1 github.com/armon/go-radix: 7fddfc3 github.com/bketelsen/crypt: 5cbc8cc github.com/checkpoint-restore/go-criu/v4: v4.1.0 github.com/chzyer/logex: v1.1.10 github.com/chzyer/readline: 2972be2 github.com/chzyer/test: a1ea475 github.com/cilium/ebpf: 1c8d4c9 github.com/containerd/cgroups: 0dbf7f0 github.com/containerd/console: v1.0.0 github.com/containerd/containerd: v1.4.1 github.com/containerd/continuity: aaeac12 github.com/containerd/fifo: a9fb20d github.com/containerd/go-runc: 5a6d9f3 github.com/containerd/ttrpc: v1.0.2 github.com/containerd/typeurl: v1.0.1 github.com/coreos/bbolt: v1.3.2 github.com/coreos/go-systemd/v22: v22.1.0 github.com/cyphar/filepath-securejoin: v0.2.2 github.com/euank/go-kmsg-parser: v2.0.0+incompatible github.com/fvbommel/sortorder: v1.0.1 github.com/globalsign/mgo: eeefdec github.com/go-gl/glfw/v3.3/glfw: 6f7a984 github.com/go-gl/glfw: e6da0ac github.com/godbus/dbus/v5: v5.0.3 github.com/gopherjs/gopherjs: 0766667 github.com/gorilla/mux: v1.8.0 github.com/hashicorp/consul/api: v1.1.0 github.com/hashicorp/consul/sdk: v0.1.1 github.com/hashicorp/errwrap: v1.0.0 github.com/hashicorp/go-cleanhttp: v0.5.1 github.com/hashicorp/go-immutable-radix: v1.0.0 github.com/hashicorp/go-msgpack: v0.5.3 github.com/hashicorp/go-multierror: v1.0.0 github.com/hashicorp/go-rootcerts: v1.0.0 github.com/hashicorp/go-sockaddr: v1.0.0 github.com/hashicorp/go-uuid: v1.0.1 github.com/hashicorp/go.net: v0.0.1 github.com/hashicorp/logutils: v1.0.0 github.com/hashicorp/mdns: v1.0.0 github.com/hashicorp/memberlist: v0.1.3 github.com/hashicorp/serf: v0.8.2 github.com/ianlancetaylor/demangle: 5e5cf60 github.com/jmespath/go-jmespath/internal/testify: v1.5.1 github.com/jtolds/gls: v4.20.0+incompatible github.com/karrick/godirwalk: v1.16.1 github.com/kr/logfmt: b84e30a github.com/mindprince/gonvml: 9ebdce4 github.com/mistifyio/go-zfs: f784269 github.com/mitchellh/cli: v1.0.0 github.com/mitchellh/go-testing-interface: v1.0.0 github.com/mitchellh/gox: v0.4.0 github.com/mitchellh/iochan: v1.0.0 github.com/moby/sys/mountinfo: v0.1.3 github.com/modocache/gover: b58185e github.com/morikuni/aec: v1.0.0 github.com/niemeyer/pretty: a10e7ca github.com/opencontainers/image-spec: v1.0.1 github.com/opencontainers/runtime-spec: 4d89ac9 github.com/pascaldekloe/goe: 57f6aae github.com/pborman/uuid: v1.2.0 github.com/posener/complete: v1.1.1 github.com/ryanuber/columnize: 9b3edd6 github.com/sean-/seed: e2103e2 github.com/seccomp/libseccomp-golang: v0.9.1 github.com/smartystreets/assertions: b2de0cb github.com/smartystreets/goconvey: v1.6.4 github.com/subosito/gotenv: v1.2.0 github.com/syndtr/gocapability: d983527 github.com/willf/bitset: d5bec33 github.com/yuin/goldmark: v1.1.27 golang.org/x/term: 7de9c90 gopkg.in/ini.v1: v1.51.0 k8s.io/api: fcac651 k8s.io/apiextensions-apiserver: a7ee1ef k8s.io/apimachinery: 15c5dba k8s.io/apiserver: aed7ab0 k8s.io/cli-runtime: 2e4b259 k8s.io/client-go: e24efdc k8s.io/cluster-bootstrap: 614b98e k8s.io/code-generator: v0.21.0-alpha.0 k8s.io/component-base: 1e84b32 k8s.io/component-helpers: 7cb42b6 k8s.io/controller-manager: b2c380a k8s.io/cri-api: v0.21.0-alpha.0 k8s.io/csi-translation-lib: 8333033 k8s.io/kube-aggregator: 6c47de4 k8s.io/kube-controller-manager: 18c28a4 k8s.io/kube-proxy: deb12d4 k8s.io/kube-scheduler: 0f62d39 k8s.io/kubectl: 5cfbd40 k8s.io/kubelet: 92ded5e k8s.io/legacy-cloud-providers: 716c3da k8s.io/metrics: d70c0e0 k8s.io/mount-utils: v0.21.0-alpha.0 k8s.io/sample-apiserver: 1f4e6a9 rsc.io/binaryregexp: v0.2.0  Updated  cloud.google.com/go: v0.38.0 → v0.54.0 github.com/Azure/azure-sdk-for-go: 8277be3 → v49.1.0+incompatible github.com/GoogleCloudPlatform/k8s-cloud-provider: 27a4ced → 7901bc8 github.com/Microsoft/go-winio: v0.4.14 → v0.4.15 github.com/Microsoft/hcsshim: 672e52e → 5eafd15 github.com/alecthomas/template: a0175ee → fb15b89 github.com/alecthomas/units: 2efee85 → c3de453 github.com/aws/aws-sdk-go: v1.28.2 → v1.35.24 github.com/containernetworking/cni: v0.7.1 → v0.8.0 github.com/coredns/corefile-migration: v1.0.6 → v1.0.10 github.com/coreos/etcd: v3.3.10+incompatible → v3.3.13+incompatible github.com/creack/pty: v1.1.7 → v1.1.9 github.com/dnaeon/go-vcr: v1.0.1 → v1.1.0 github.com/docker/docker: be7ac8b → bd33bbf github.com/docker/go-connections: v0.3.0 → v0.4.0 github.com/fsnotify/fsnotify: v1.4.7 → v1.4.9 github.com/go-kit/kit: v0.8.0 → v0.9.0 github.com/go-logfmt/logfmt: v0.3.0 → v0.4.0 github.com/google/cadvisor: v0.35.0 → v0.38.5 github.com/google/pprof: 3ea8567 → 1ebb73c github.com/googleapis/gax-go/v2: v2.0.4 → v2.0.5 github.com/gorilla/websocket: v1.4.0 → v1.4.2 github.com/jmespath/go-jmespath: c2b33e8 → v0.4.0 github.com/jstemmer/go-junit-report: af01ea7 → v0.9.1 github.com/kr/pretty: v0.1.0 → v0.2.0 github.com/kr/text: v0.1.0 → v0.2.0 github.com/mattn/go-isatty: v0.0.9 → v0.0.4 github.com/moby/ipvs: v1.0.0 → v1.0.1 github.com/mrunalp/fileutils: 7d4729f → abd8a0e github.com/opencontainers/runc: v1.0.0-rc10 → v1.0.0-rc92 github.com/opencontainers/selinux: 5215b18 → v1.6.0 github.com/quobyte/api: v0.1.2 → v0.1.8 github.com/spf13/viper: v1.3.2 → v1.7.0 github.com/storageos/go-api: 343b3ef → v2.2.0+incompatible github.com/tmc/grpc-websocket-proxy: 89b8d40 → 0ad062e github.com/urfave/cli: v1.20.0 → v1.22.2 github.com/vishvananda/netlink: v1.0.0 → v1.1.0 github.com/vishvananda/netns: be1fbed → db3c7e5 go.etcd.io/bbolt: v1.3.3 → v1.3.5 go.opencensus.io: v0.21.0 → v0.22.3 golang.org/x/exp: 4b39c73 → 6cc2880 golang.org/x/image: 0694c2d → cff245a golang.org/x/lint: 959b441 → 738671d golang.org/x/mobile: d3739f8 → d2bd2a2 golang.org/x/mod: 4bf6d31 → v0.3.0 golang.org/x/net: 13f9640 → ac852fb golang.org/x/tools: 5eefd05 → c1934b7 golang.org/x/xerrors: a985d34 → 5ec99f8 google.golang.org/api: 5213b80 → v0.20.0 google.golang.org/protobuf: v1.24.0 → v1.25.0 gopkg.in/check.v1: 788fd78 → 8fa4692 honnef.co/go/tools: v0.0.1-2019.2.2 → v0.0.1-2020.1.3 k8s.io/cloud-provider: 52e5381 → 82fca6d k8s.io/klog/v2: v2.2.0 → v2.4.0 k8s.io/kube-openapi: 6aeccd4 → d219536 k8s.io/kubernetes: bb8a5d2 → f58c4d8 k8s.io/system-validators: v1.1.2 → v1.2.0 sigs.k8s.io/structured-merge-diff/v4: v4.0.1 → v4.0.2  Removed  github.com/xlab/handysort: fb3537e k8s.io/kubernetes/staging/src/k8s.io/api: 70a6823 k8s.io/kubernetes/staging/src/k8s.io/apiextensions-apiserver: 70a6823 k8s.io/kubernetes/staging/src/k8s.io/apimachinery: 70a6823 k8s.io/kubernetes/staging/src/k8s.io/apiserver: 70a6823 k8s.io/kubernetes/staging/src/k8s.io/cli-runtime: 70a6823 k8s.io/kubernetes/staging/src/k8s.io/client-go: 70a6823 k8s.io/kubernetes/staging/src/k8s.io/cluster-bootstrap: 70a6823 k8s.io/kubernetes/staging/src/k8s.io/code-generator: 70a6823 k8s.io/kubernetes/staging/src/k8s.io/component-base: 70a6823 k8s.io/kubernetes/staging/src/k8s.io/cri-api: 70a6823 k8s.io/kubernetes/staging/src/k8s.io/csi-translation-lib: 70a6823 k8s.io/kubernetes/staging/src/k8s.io/kube-aggregator: 70a6823 k8s.io/kubernetes/staging/src/k8s.io/kube-controller-manager: 70a6823 k8s.io/kubernetes/staging/src/k8s.io/kube-proxy: 70a6823 k8s.io/kubernetes/staging/src/k8s.io/kube-scheduler: 70a6823 k8s.io/kubernetes/staging/src/k8s.io/kubectl: 70a6823 k8s.io/kubernetes/staging/src/k8s.io/kubelet: 70a6823 k8s.io/kubernetes/staging/src/k8s.io/legacy-cloud-providers: 70a6823 k8s.io/kubernetes/staging/src/k8s.io/metrics: 70a6823 k8s.io/kubernetes/staging/src/k8s.io/sample-apiserver: 70a6823 vbom.ml/util: db5cfe1  ","excerpt":"Major changes since v0.6.0 Cloud Provider Azure v0.7.0 updates …","ref":"/cloud-provider-azure/blog/2021/01/06/v0.7.0/","title":"v0.7.0"},{"body":"Major changes since v0.5.0   Update vendor against k/k release-1.19(#385) Increase the e2e test coverage for cluster autoscaler(#364) Use hugo to generate doc website(#358) Update E2E test related docs and script(#355) Partly decouple k/k(#350) Update go module against k8s.io/cloud-provider(#348) Use distroless/static as base image(#333) Enable running ccm e2e test in a job(#345)  The image is available at\n mcr.microsoft.com/oss/kubernetes/azure-cloud-node-manager:v0.6.0 mcr.microsoft.com/oss/kubernetes/azure-cloud-controller-manager:v0.6.0  Since v0.5.0, our docs are moved to a dedicated website and the docs/ directory is deprecated.\n","excerpt":"Major changes since v0.5.0   Update vendor against k/k …","ref":"/cloud-provider-azure/blog/2020/09/01/v0.6.0/","title":"v0.6.0"},{"body":"Changes since v0.5.0   Update Kubernetes vendor to adopt bug fixes from in-tree cloud provider(#330) Use a service account for CCM (#329) Update images for out-of-tree examples (#328) Fix wrong init url for kubemark tests (#327)  The image is available at\n mcr.microsoft.com/oss/kubernetes/azure-cloud-node-manager:v0.5.1 mcr.microsoft.com/oss/kubernetes/azure-cloud-controller-manager:v0.5.1  ","excerpt":"Changes since v0.5.0   Update Kubernetes vendor to adopt bug fixes …","ref":"/cloud-provider-azure/blog/2020/04/27/v0.5.1/","title":"v0.5.1"},{"body":"The alpha version of azure-cloud-controller-manager and azure-cloud-node-manager, which have upgraded Kubernetes version to v1.18. It also adds Windows support for azure-cloud-node-manager.\nPlease see docs for documentation.\nThe image is available at\n mcr.microsoft.com/oss/kubernetes/azure-cloud-node-manager:v0.5.0 mcr.microsoft.com/oss/kubernetes/azure-cloud-controller-manager:v0.5.0  ","excerpt":"The alpha version of azure-cloud-controller-manager and …","ref":"/cloud-provider-azure/blog/2020/03/27/v0.5.0/","title":"v0.5.0"},{"body":"The alpha version of azure-cloud-controller-manager and azure-cloud-node-manager, which fixes the node address update issues.\nThe image is available at\n mcr.microsoft.com/oss/kubernetes/azure-cloud-node-manager:v0.4.1 mcr.microsoft.com/oss/kubernetes/azure-cloud-controller-manager:v0.4.1  ","excerpt":"The alpha version of azure-cloud-controller-manager and …","ref":"/cloud-provider-azure/blog/2019/12/30/v0.4.1/","title":"v0.4.1"},{"body":"The alpha version of azure-cloud-controller-manager and azure-cloud-node-manager, which have upgraded Kubernetes version to v1.17.\nPlease see docs for documentation.\nThe image is available at\n mcr.microsoft.com/oss/kubernetes/azure-cloud-node-manager:v0.4.0 mcr.microsoft.com/oss/kubernetes/azure-cloud-controller-manager:v0.4.0  ","excerpt":"The alpha version of azure-cloud-controller-manager and …","ref":"/cloud-provider-azure/blog/2019/12/17/v0.4.0/","title":"v0.4.0"},{"body":"The alpha version of azure-cloud-controller-manager and azure-cloud-node-manager, which have upgraded Kubernetes version to v1.16.\nPlease see docs for documentation.\nThe image is available at\n mcr.microsoft.com/oss/kubernetes/azure-cloud-controller-manager:v0.3.0 mcr.microsoft.com/oss/kubernetes/azure-cloud-node-manager:v0.3.0  ","excerpt":"The alpha version of azure-cloud-controller-manager and …","ref":"/cloud-provider-azure/blog/2019/09/24/v0.3.0/","title":"v0.3.0"},{"body":"The alpha version of azure-cloud-controller-manager, which has upgraded Kubernetes version to v1.15.0.\nPlease see docs for documentation.\nThe image is available at mcr.microsoft.com/k8s/core/azure-cloud-controller-manager:v0.2.0.\n","excerpt":"The alpha version of azure-cloud-controller-manager, which has …","ref":"/cloud-provider-azure/blog/2019/06/27/v0.2.0/","title":"v0.2.0"},{"body":"The alpha version of azure-cloud-controller-manager. Please see docs for documentation.\nThe image is available at mcr.microsoft.com/k8s/core/azure-cloud-controller-manager:v0.1.0.\n","excerpt":"The alpha version of azure-cloud-controller-manager. Please see docs …","ref":"/cloud-provider-azure/blog/2019/03/26/v0.1.0/","title":"v0.1.0"},{"body":"Overview Here provides some E2E tests only specific to Azure provider.\nPrerequisite Deploy a Kubernetes cluster with Azure CCM Refer step 1-3 in e2e-tests for deploying the Kubernetes cluster.\nSetup Azure credentials export AZURE_TENANT_ID=\u003ctenant-id\u003e # the tenant ID export AZURE_SUBSCRIPTION_ID=\u003csubscription-id\u003e # the subscription ID export AZURE_CLIENT_ID=\u003cservice-principal-id\u003e # the service principal ID export AZURE_CLIENT_SECRET=\u003cservice-principal-secret\u003e # the service principal secret export AZURE_ENVIRONMENT=\u003cAzurePublicCloud\u003e # the cloud environment (optional, default is AzurePublicCloud) export AZURE_LOCATION=\u003clocation\u003e # the location export AZURE_LOADBALANCER_SKU=\u003cloadbalancer-sku\u003e # the sku of load balancer (optional, default is basic) Setup KUBECONFIG   Locate your kubeconfig and set it as env variable export KUBECONFIG=\u003ckubeconfig\u003e or cp \u003ckubeconfig\u003e ~/.kube/config\n  Test it via kubectl version\n  Run Test Have installed ginkgo   Run ginkgo ./tests/e2e/ \nFor more usage of ginkgo, please follow ginkgo\n  Without ginkgo  Run go test ./tests/e2e/ -timeout 0  After a long time test, a JUnit report will be generated in a directory named by the cluster name\n","excerpt":"Overview Here provides some E2E tests only specific to Azure provider. …","ref":"/cloud-provider-azure/development/e2e/e2e-tests-azure/","title":"Azure E2E tests"},{"body":" azure disk plugin known issues  Recommended stable version for azure disk 1. disk attach error 2. disk unavailable after attach/detach a data disk on a node 3. Azure disk support on Sovereign Cloud 4. Time cost for Azure Disk PVC mount 5. Azure disk PVC Multi-Attach error, makes disk mount very slow or mount failure forever 6. WaitForAttach failed for azure disk: parsing “/dev/disk/azure/scsi1/lun1”: invalid syntax 7. uid and gid setting in azure disk 8. Addition of a blob based disk to VM with managed disks is not supported 9. dynamic azure disk PVC try to access wrong storage account (of other resource group) 10. data loss if using existing azure disk with partitions in disk mount 11. Delete azure disk PVC which is already in use by a pod 12. create azure disk PVC failed due to account creation failure 13. cannot find Lun for disk 14. azure disk attach/detach failure, mount issue, i/o error 15. azure disk could be not detached forever 16. potential race condition issue due to detach disk failure retry 17. very slow disk attach/detach issue when disk num is large 18. detach azure disk make VM run into a limbo state 19. disk attach/detach self-healing on VMAS 20. azure disk detach failure if node not exists 21. invalid disk URI error 22. vmss dirty cache issue 23. race condition when delete disk right after attach disk 24. attach disk costs 10min 25. Multi-Attach error 26. attached non-existing disk volume on agent node 27. failed to get azure instance id for node (not a vmss instance)    Recommended stable version for azure disk    k8s version stable version     v1.15 1.15.11+   v1.16 1.16.10+   v1.17 1.17.6+   v1.18 1.18.3+   v1.19 1.19.0+    1. disk attach error Issue details:\nIn some corner case(detaching multiple disks on a node simultaneously), when scheduling a pod with azure disk mount from one node to another, there could be lots of disk attach error(no recovery) due to the disk not being released in time from the previous node. This issue is due to lack of lock before DetachDisk operation, actually there should be a central lock for both AttachDisk and DetachDisk operations, only one AttachDisk or DetachDisk operation is allowed at one time.\nThe disk attach error could be like following:\nCannot attach data disk 'cdb-dynamic-pvc-92972088-11b9-11e8-888f-000d3a018174' to VM 'kn-edge-0' because the disk is currently being detached or the last detach operation failed. Please wait until the disk is completely detached and then try again or delete/detach the disk explicitly again. Related issues\n Azure Disk Detach are not working with multiple disk detach on the same Node Azure disk fails to attach and mount, causing rescheduled pod to stall following node disruption Since Intel CPU Azure update, new Azure Disks are not mounting, very critical…  Busy azure-disk regularly fail to mount causing K8S Pod deployments to halt  Mitigation:\n option#1: Update every agent node that has attached or detached the disk in problem  In Azure cloud shell, run\n$vm = Get-AzureRMVM -ResourceGroupName $rg -Name $vmname Update-AzureRmVM -ResourceGroupName $rg -VM $vm -verbose -debug In Azure cli, run\naz vm update -g \u003cgroup\u003e -n \u003cname\u003e  option#2:   kubectl cordon node #make sure no scheduling on this node kubectl drain node #schedule pod in current node to other node restart the Azure VM for node via the API or portal, wait until VM is “Running” kubectl uncordon node  Fix\n PR fix race condition issue when detaching azure disk has fixed this issue by add a lock before DetachDisk     k8s version fixed version     v1.6 no fix   v1.7 1.7.14   v1.8 1.8.9   v1.9 1.9.5   v1.10 1.10.0    2. disk unavailable after attach/detach a data disk on a node  💡 NOTE: Azure platform has fixed the host cache issue, the suggested host cache setting of data disk is ReadOnly now, more details about azure disk cache setting Issue details:\n From k8s v1.7, default host cache setting changed from None to ReadWrite, this change would lead to device name change after attach multiple disks on a node, finally lead to disk unavailable from pod. When access data disk inside a pod, will get following error:\n[root@admin-0 /]# ls /datadisk ls: reading directory .: Input/output error In my testing on Ubuntu 16.04 D2_V2 VM, when attaching the 6th data disk will cause device name change on agent node, e.g. following lun0 disk should be sdc other than sdk.\nazureuser@k8s-agentpool2-40588258-0:~$ tree /dev/disk/azure ... â””â”€â”€ scsi1 â”œâ”€â”€ lun0 -\u003e ../../../sdk â”œâ”€â”€ lun1 -\u003e ../../../sdj â”œâ”€â”€ lun2 -\u003e ../../../sde â”œâ”€â”€ lun3 -\u003e ../../../sdf â”œâ”€â”€ lun4 -\u003e ../../../sdg â”œâ”€â”€ lun5 -\u003e ../../../sdh â””â”€â”€ lun6 -\u003e ../../../sdi Related issues\n device name change due to azure disk host cache setting unable to use azure disk in StatefulSet since /dev/sd* changed after detach/attach disk Disk error when pods are mounting a certain amount of volumes on a node unable to use azure disk in StatefulSet since /dev/sd* changed after detach/attach disk Input/output error when accessing PV PersistentVolumeClaims changing to Read-only file system suddenly  Workaround:\n add cachingmode: None in azure disk storage class(default is ReadWrite), e.g.  kind:StorageClassapiVersion:storage.k8s.io/v1metadata:name:hddprovisioner:kubernetes.io/azure-diskparameters:skuname:Standard_LRSkind:Managedcachingmode:NoneFix\n PR fix device name change issue for azure disk could fix this issue too, it will change default cachingmode value from ReadWrite to None.     k8s version fixed version     v1.6 no such issue as cachingmode is already None by default   v1.7 1.7.14   v1.8 1.8.11   v1.9 1.9.4   v1.10 1.10.0    3. Azure disk support on Sovereign Cloud Fix\n PR Azure disk on Sovereign Cloud fixed this issue     k8s version fixed version     v1.7 1.7.9   v1.8 1.8.3   v1.9 1.9.0   v1.10 1.10.0    4. Time cost for Azure Disk PVC mount Original time cost for Azure Disk PVC mount on a standard node size(e.g. Standard_D2_V2) is around 1 minute, podAttachAndMountTimeout is 2 minutes, total waitForAttachTimeout is 10 minutes, so a disk remount(detach and attach in sequential) would possibly cost more than 2min, thus may fail.\n Note: for some smaller VM size which has only 1 CPU core, time cost would be much bigger(e.g. \u003e 10min) since container is hard to get CPU slot.\n Related issues\n ‘timeout expired waiting for volumes to attach/mount for pod when cluster’ when node-vm-size is Standard_B1s  Fix\n PR using cache fix fixed this issue, which could reduce the mount time cost to around 30s.     k8s version fixed version     v1.8 no fix   v1.9 1.9.2   v1.10 1.10.0    5. Azure disk PVC Multi-Attach error, makes disk mount very slow or mount failure forever  💡 NOTE: AKS and current aks-engine won’t have this issue since it’s not using containerized kubelet\n Issue details:\nWhen schedule a pod with azure disk volume from one node to another, total time cost of detach \u0026 attach is around 1 min from v1.9.2, while in v1.9.x, there is an UnmountDevice failure issue in containerized kubelet which makes disk mount very slow or mount failure forever, this issue only exists in v1.9.x due to PR Refactor nsenter, v1.10.0 won’t have this issue since devicePath is updated in v1.10 code\nerror logs:\n kubectl describe po POD-NAME  Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 3m default-scheduler Successfully assigned deployment-azuredisk1-6cd8bc7945-kbkvz to k8s-agentpool-88970029-0 Warning FailedAttachVolume 3m attachdetach-controller Multi-Attach error for volume \"pvc-6f2d0788-3b0b-11e8-a378-000d3afe2762\" Volume is already exclusively attached to one node and can't be attached to another Normal SuccessfulMountVolume 3m kubelet, k8s-agentpool-88970029-0 MountVolume.SetUp succeeded for volume \"default-token-qt7h6\" Warning FailedMount 1m kubelet, k8s-agentpool-88970029-0 Unable to mount volumes for pod \"deployment-azuredisk1-6cd8bc7945-kbkvz_default(5346c040-3e4c-11e8-a378-000d3afe2762)\": timeout expired waiting for volumes to attach/mount for pod \"default\"/\"deployment-azuredisk1-6cd8bc7945-kbkvz\". list of unattached/unmounted volumes=[azuredisk]  kubelet logs from the new node  E0412 20:08:10.920284 7602 nestedpendingoperations.go:263] Operation for \"\\\"kubernetes.io/azure-disk//subscriptions/xxx/resourceGroups/MC_xxx_eastus/providers/Microsoft.Compute/disks/kubernetes-dynamic-pvc-11035a31-3e8d-11e8-82ec-0a58ac1f04cf\\\"\" failed. No retries permitted until 2018-04-12 20:08:12.920234762 +0000 UTC m=+1467.278612421 (durationBeforeRetry 2s). Error: \"Volume has not been added to the list of VolumesInUse in the node's volume status for volume \\\"pvc-11035a31-3e8d-11e8-82ec-0a58ac1f04cf\\\" (UniqueName: \\\"kubernetes.io/azure-disk//subscriptions/xxx/resourceGroups/MC_xxx_eastus/providers/Microsoft.Compute/disks/kubernetes-dynamic-pvc-11035a31-3e8d-11e8-82ec-0a58ac1f04cf\\\") pod \\\"symbiont-node-consul-0\\\" (UID: \\\"11043b12-3e8d-11e8-82ec-0a58ac1f04cf\\\") \" Related issues\n UnmountDevice would fail in containerized kubelet upgrade k8s process is broke  Mitigation:\nIf azure disk PVC mount successfully in the end, there is no action, while if it could not be mounted for more than 20min, following actions could be taken:\n check whether volumesInUse list has unmounted azure disks, run:  kubectl get no NODE-NAME -o yaml \u003e node.log all volumes in volumesInUse should be also in volumesAttached, otherwise there would be issue\n restart kubelet on the original node would solve this issue: sudo kubectl kubelet restart  Fix\n PR fix nsenter GetFileType issue in containerized kubelet fixed this issue     k8s version fixed version     v1.8 no such issue   v1.9 v1.9.7   v1.10 no such issue    After fix in v1.9.7, it took about 1 minute for scheduling one azure disk mount from one node to another, you could find details here.\nSince azure disk attach/detach operation on a VM cannot be parallel, scheduling 3 azure disk mounts from one node to another would cost about 3 minutes.\n6. WaitForAttach failed for azure disk: parsing “/dev/disk/azure/scsi1/lun1”: invalid syntax Issue details: MountVolume.WaitForAttach may fail in the azure disk remount\nerror logs:\nin v1.10.0 \u0026 v1.10.1, MountVolume.WaitForAttach will fail in the azure disk remount, error logs would be like following:\n incorrect DevicePath format on Linux  MountVolume.WaitForAttach failed for volume \"pvc-f1562ecb-3e5f-11e8-ab6b-000d3af9f967\" : azureDisk - Wait for attach expect device path as a lun number, instead got: /dev/disk/azure/scsi1/lun1 (strconv.Atoi: parsing \"/dev/disk/azure/scsi1/lun1\": invalid syntax) Warning FailedMount 1m (x10 over 21m) kubelet, k8s-agentpool-66825246-0 Unable to mount volumes for pod  wrong DevicePath(LUN) number on Windows   Warning FailedMount 1m kubelet, 15282k8s9010 MountVolume.WaitForAttach failed for volume \"disk01\" : azureDisk - WaitForAttach failed within timeout node (15282k8s9010) diskId:(andy-mghyb 1102-dynamic-pvc-6c526c51-4a18-11e8-ab5c-000d3af7b38e) lun:(4) Related issues\n WaitForAttach failed for azure disk: parsing “/dev/disk/azure/scsi1/lun1”: invalid syntax Pod unable to attach PV after being deleted (Wait for attach expect device path as a lun number, instead got: /dev/disk/azure/scsi1/lun0 (strconv.Atoi: parsing “/dev/disk/azure/scsi1/lun0”: invalid syntax)  Fix\n PR fix WaitForAttach failure issue for azure disk fixed this issue     k8s version fixed version     v1.8 no such issue   v1.9 no such issue   v1.10 1.10.2    7. uid and gid setting in azure disk Issue details: Unlike azure file mountOptions, you will get following failure if set mountOptions like uid=999,gid=999 in azure disk mount:\nWarning FailedMount 63s kubelet, aks-nodepool1-29460110-0 MountVolume.MountDevice failed for volume \"pvc-d783d0e4-85a1-11e9-8a90-369885447933\" : azureDisk - mountDevice:FormatAndMount failed with mount failed: exit status 32 Mounting command: systemd-run Mounting arguments: --description=Kubernetes transient mount for /var/lib/kubelet/plugins/kubernetes.io/azure-disk/mounts/m436970985 --scope -- mount -t xfs -o dir_mode=0777,file_mode=0777,uid=1000,gid=1000,defaults /dev/disk/azure/scsi1/lun2 /var/lib/kubelet/plugins/kubernetes.io/azure-disk/mounts/m436970985 Output: Running scope as unit run-rb21966413ab449b3a242ae9b0fbc9398.scope. mount: wrong fs type, bad option, bad superblock on /dev/sde, missing codepage or helper program, or other error That’s because azureDisk use ext4,xfs file system by default, mountOptions like [uid=x,gid=x] could not be set in mount time.\nRelated issues\n Timeout expired waiting for volumes to attach Pod failed mounting xfs format volume with mountOptions Allow volume ownership to be only set after fs formatting  Solution:\n option#1: Set uid in runAsUser and gid in fsGroup for pod: security context for a Pod  e.g. Following setting will set pod run as root, make it accessible to any file:\napiVersion:v1kind:Podmetadata:name:security-context-demospec:securityContext:runAsUser:0fsGroup:0 Note: Since gid \u0026 uid is mounted as 0(root) by default, if set as non-root(e.g. 1000), k8s will use chown to change all dir/files under that disk, this is a time consuming job, which would make mount device very slow, in this issue: Timeout expired waiting for volumes to attach, it costs about 10 min for chown operation complete.\n  option#2: use chown in initContainers  initContainers: - name: volume-mount image: busybox command: [\"sh\", \"-c\", \"chown -R 100:100 /data\"] volumeMounts: - name: \u003cyour data volume\u003e mountPath: /data  new upstream feature to address this: Allow volume ownership to be only set after fs formatting  8. Addition of a blob based disk to VM with managed disks is not supported Issue details:\nFollowing error may occur if attach a blob based(unmanaged) disk to VM with managed disks:\nWarning FailedMount 42s (x2 over 1m) attachdetach AttachVolume.Attach failed for volume \"pvc-f17e5e77-474e-11e8-a2ea-000d3a10df6d\" : Attach volume \"holo-k8s-dev-dynamic-pvc-f17e5e77-474e-11e8-a2ea-000d3a10df6d\" to instance \"k8s-master-92699158-0\" failed with compute.VirtualMachinesClient#CreateOrUpdate: Failure responding to request: StatusCode=409 -- Original Error: autorest/azure: Service returned an error. Status=409 Code=\"OperationNotAllowed\" Message=\"Addition of a blob based disk to VM with managed disks is not supported.\" This issue is by design as in Azure, there are two kinds of disks, blob based(unmanaged) disk and managed disk, an Azure VM could not attach both of these two kinds of disks.\nSolution:\nUse default azure disk storage class in acs-engine, as default will always be identical to the agent pool, that is, if VM is managed, it will be managed azure disk class, if unmanaged, then it’s unmanaged disk class.\n9. dynamic azure disk PVC try to access wrong storage account (of other resource group) Issue details:\nIn a k8s cluster with blob based VMs(won’t happen in AKS since AKS only use managed disk), create dynamic azure disk PVC may fail, error logs is like following:\nFailed to provision volume with StorageClass \"default\": azureDisk - account ds6c822a4d484211eXXXXXX does not exist while trying to create/ensure default container Related issues\n Multiple clusters - dynamic PVCs try to access wrong storage account (of other resource group)  Fix\n PR fix storage account not found issue: use ListByResourceGroup instead of List() fixed this issue     k8s version fixed version     v1.8 1.8.13   v1.9 1.9.9   v1.10 no such issue    Work around:\nthis bug only exists in blob based VM in v1.8.x, v1.9.x, so if specify ManagedDisks when creating k8s cluster in acs-engine(AKS is using managed disk by default), it won’t have this issue:\n\"agentPoolProfiles\": [ { ... \"storageProfile\" : \"ManagedDisks\", ... } 10. data loss if using existing azure disk with partitions in disk mount Issue details:\nWhen use an existing azure disk(also called static provisioning) in pod, if that disk has partitions, the disk will be formatted in the pod mounting process, actually k8s volume don’t support mount disk with partitions, disk mount would fail finally. While for mounting existing azure disk that has partitions, data will be lost since it will format that disk first. This issue happens only on Linux.\nRelated issues\n data loss if using existing azure disk with partitions in disk mount  Fix\n PR fix data loss issue if using existing azure disk with partitions in disk mount will let azure provider return error when mounting existing azure disk that has partitions     k8s version fixed version     v1.8 1.8.15   v1.9 1.9.11   v1.10 1.10.5   v1.11 1.11.0    Work around:\nDon’t use existing azure disk that has partitions, e.g. following disk in LUN 0 that has one partition:\nazureuser@aks-nodepool1-28371372-0:/$ ls -l /dev/disk/azure/scsi1/ total 0 lrwxrwxrwx 1 root root 12 Apr 27 08:04 lun0 -\u003e ../../../sdc lrwxrwxrwx 1 root root 13 Apr 27 08:04 lun0-part1 -\u003e ../../../sdc1 11. Delete azure disk PVC which is already in use by a pod Issue details:\nFollowing error may occur if delete azure disk PVC which is already in use by a pod:\nkubectl describe pv pvc-d8eebc1d-74d3-11e8-902b-e22b71bb1c06 ... Message: disk.DisksClient#Delete: Failure responding to request: StatusCode=409 -- Original Error: autorest/azure: Service returned an error. Status=409 Code=\"OperationNotAllowed\" Message=\"Disk kubernetes-dynamic-pvc-d8eebc1d-74d3-11e8-902b-e22b71bb1c06 is attached to VM /subscriptions/{subs-id}/resourceGroups/MC_markito-aks-pvc_markito-aks-pvc_westus/providers/Microsoft.Compute/virtualMachines/aks-agentpool-25259074-0.\" Fix:\nThis is a common k8s issue, other cloud provider would also has this issue. There is a PVC protection feature to prevent this, it’s alpha in v1.9, and beta(enabled by default) in v1.10\nWork around: delete pod first and then delete azure disk pvc after a few minutes\n12. create azure disk PVC failed due to account creation failure  please note this issue only happens on unmanaged k8s cluster\n Issue details: User may get Account property kind is invalid for the request error when trying to create a new unmanaged azure disk PVC, error would be like following:\nazureuser@k8s-master-17140924-0:/tmp$ kubectl describe pvc Name: pvc-azuredisk Namespace: default StorageClass: hdd Status: Bound ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning ProvisioningFailed 31m persistentvolume-controller Failed to provision volume with StorageClass \"hdd\": Create Storage Account: ds10e15ed89c5811e8a0a70, error: storage.AccountsClient#Create: Failure sending request: StatusCode=400 -- Original Error: Code=\"AccountPropertyIsInvalid\" Message=\"Account property kind is invalid for the request.\" Fix\n PR fix azure disk create failure due to sdk upgrade fixed this issue     k8s version fixed version     v1.9 no such issue   v1.10 no such issue   v1.11 1.11.3   v1.12 no such issue    Work around:\n create a storage account and specify that account in azure disk storage class, e.g.  kind:StorageClassapiVersion:storage.k8s.io/v1beta1metadata:name:ssdprovisioner:kubernetes.io/azure-diskparameters:skuname:Premium_LRSstorageAccount:customerstorageaccountkind:Dedicated13. cannot find Lun for disk Issue details:\nFollowing error may occur if attach a disk to a node:\nMountVolume.WaitForAttach failed for volume \"pvc-12b458f4-c23f-11e8-8d27-46799c22b7c6\" : Cannot find Lun for disk kubernetes-dynamic-pvc-12b458f4-c23f-11e8-8d27-46799c22b7c6 Related issues\n GetAzureDiskLun sometimes costs 1 min which is too long time  Fix\n PR fix azure disk attachment error on Linux will extract the LUN num from device path only on Linux     k8s version fixed version     v1.9 no such issue   v1.10 1.10.10   v1.11 1.11.5   v1.12 1.12.3   v1.13 no such issue    Work around:\nwait for a few more minutes should work\n14. azure disk attach/detach failure, mount issue, i/o error Issue details:\nWe found a disk attach/detach issue due to dirty vm cache PR introduced from v1.9.2, it would lead to following disk issues:\n disk attach/detach failure for a long time disk I/O error unexpected disk detachment from VM VM running into failed state due to attaching non-existing disk   Note: above error may only happen when there are multiple disk attach/detach operations in parallel and it’s not easy to repro since it happens on a little possibility.\n Related issues\n Azure Disks volume attach still times out on Kubernetes 1.10 Azure Disks occasionally mounted in a way leading to I/O errors  Fix\nWe changed the azure disk attach/detach retry logic in k8s v1.13, switch to use k8s attach-detach controller to do attach/detach disk retry and clean vm cache after every disk operation, this issue is proved to be fixed in our disk attach/detach stress test and also verified in customer env:\n PR remove retry operation on attach/detach azure disk in azure cloud provider PR fix azure disk attach/detach failed forever issue PR fix detach azure disk issue due to dirty cache     k8s version fixed version     v1.9 issue introduced in v1.9.2, no cherry-pick fix allowed   v1.10 1.10.12   v1.11 1.11.6   v1.12 1.12.4   v1.13 no such issue    Work around:\n if there is attach disk failure for long time, restart controller manager may work if there is disk not detached for long time, detach that disk manually  Related issues\n Multi Attach Error  15. azure disk could be not detached forever Issue details:\nIn some condition when first detach azure disk operation failed, it won’t retry and the azure disk would be still attached to the original VM node.\nFollowing error may occur when move one disk from one node to another(keyword: ConflictingUserInput):\n[Warning] AttachVolume.Attach failed for volume “pvc-7b7976d7-3a46-11e9-93d5-dee1946e6ce9” : Attach volume “kubernetes-dynamic-pvc-7b7976d7-3a46-11e9-93d5-dee1946e6ce9\" to instance “/subscriptions/XXX/resourceGroups/XXX/providers/Microsoft.Compute/virtualMachines/aks-agentpool-57634498-0” failed with compute.VirtualMachinesClient#CreateOrUpdate: Failure sending request: StatusCode=0 -- Original Error: autorest/azure: Service returned an error. Status= Code=“ConflictingUserInput” Message=“Disk ‘/subscriptions/XXX/resourceGroups/XXX/providers/Microsoft.Compute/disks/kubernetes-dynamic-pvc-7b7976d7-3a46-11e9-93d5-dee1946e6ce9’ cannot be attached as the disk is already owned by VM ‘/subscriptions/XXX/resourceGroups/XXX/providers/Microsoft.Compute/virtualMachines/aks-agentpool-57634498-1’.” Fix\nWe added retry logic for detach azure disk:\n PR add retry for detach azure disk     k8s version fixed version     v1.10 N/A   v1.11 1.11.9   v1.12 1.12.7   v1.13 1.13.4   v1.14 1.14.0   v1.15 1.15.0    Work around:\n if there is disk not detached for long time, detach that disk manually  16. potential race condition issue due to detach disk failure retry Issue details:\nIn some error condition when detach azure disk failed, azure cloud provider will retry 6 times at most with exponential backoff, it will hold the data disk list for about 3 minutes with a node level lock, and in that time period, if customer update data disk list manually (e.g. need manual operationto attach/detach another disk since there is attach/detach error, ) , the data disk list will be obselete(dirty data), then weird VM status happens, e.g. attach a non-existing disk, we should split those retry operations, every retry should get a fresh data disk list in the beginning.\nFix\nFollowing PR refined detach azure disk retry operation, make every detach azure disk operation in a standalone function\n PR fix detach azure disk back off issue which has too big lock in failure retry condition PR fix azure disk list corruption issue     k8s version fixed version     v1.10 N/A   v1.11 no fix   v1.12 1.12.9   v1.13 1.13.6   v1.14 1.14.2   v1.15 1.15.0    Work around:\nDetach all the non-existing disks from VM (could do that in azure portal by bulk update)\n Detaching disk one by one using cli may fail since they are already non-existing disks.\n 17. very slow disk attach/detach issue when disk num is large Issue details:\nWe hit very slow disk attach/detach issue when disk num is large(\u003e 10 disks on one VM)\nFix\nAzure disk team are fixing this issue.\nWork around:\nNo workaround.\n18. detach azure disk make VM run into a limbo state Issue details:\nIn some corner condition, detach azure disk would sometimes make VM run into a limbo state\nFix\nFollowing two PRs would fix this issue by retry update VM if detach disk partially fail:\n fix azure retry issue when return 2XX with error fix: retry detach azure disk issue     k8s version fixed version     v1.11 no fix   v1.12 1.12.10   v1.13 1.13.8   v1.14 1.14.4   v1.15 1.15.0    Work around:\nUpdate VM status manually would solve the problem:\n Update Availability Set VM  az vm update -n \u003cVM_NAME\u003e -g \u003cRESOURCE_GROUP_NAME\u003e  Update Scale Set VM  az vmss update-instances -g \u003cRESOURCE_GROUP_NAME\u003e --name \u003cVMSS_NAME\u003e --instance-id \u003cID(number)\u003e 19. disk attach/detach self-healing on VMAS Issue details: There could be disk detach failure due to many reasons(e.g. disk RP busy, controller manager crash, etc.), and it would fail when attach one disk to other node if that disk is still attached to the old node, user needs to manually detach disk in problem in the before, with this fix, azure cloud provider would check and detach this disk if it’s already attached to the other node, that’s like self-healing. This PR could fix lots of such disk attachment issue.\nFix\nFollowing PR would first check whether current disk is already attached to other node, if so, it would trigger a dangling error and k8s controller would detach disk first, and then do the attach volume operation.\nThis PR would also fix a “disk not found” issue when detach azure disk due to disk URI case sensitive case, error logs are like following(without this PR):\nazure_controller_standard.go:134] detach azure disk: disk not found, diskURI: /subscriptions/xxx/resourceGroups/andy-mg1160alpha3/providers/Microsoft.Compute/disks/xxx-dynamic-pvc-41a31580-f5b9-4f08-b0ea-0adcba15b6db Fix\n Fix on VMAS  fix: detach azure disk issue using dangling error fix: azure disk name matching issue       k8s version fixed version     v1.12 no fix   v1.13 1.13.11   v1.14 1.14.7   v1.15 1.15.4   v1.15 1.16.0     Fix on VMSS  fix: azure disk dangling attach issue on VMSS which would cause API throttling       k8s version fixed version     v1.15 no fix   v1.16 1.16.9   v1.17 1.17.6   v1.18 1.18.3   v1.19 1.19.0    Work around:\nmanually detach disk in problem and wait for disk attachment happen automatically\n20. azure disk detach failure if node not exists Issue details: If a node with a Azure Disk attached is deleted (before the volume is detached), subsequent attempts by the attach/detach controller to detach it continuously fail, and prevent the controller from attaching the volume to another node.\nFix\n fix: azure disk detach failure if node not exists     k8s version fixed version     v1.12 no fix   v1.13 1.13.9   v1.14 1.14.8   v1.15 1.15.5   v1.16 1.16.1   v1.16 1.17.0    Work around:\nRestart kube-controller-manager on master node.\n21. invalid disk URI error Issue details:\nWhen user use an existing disk in static provisioning, may hit following error:\nAttachVolume.Attach failed for volume \"azure\" : invalid disk URI: /subscriptions/xxx/resourcegroups/xxx/providers/Microsoft.Compute/disks/Test_Resize_1/” Fix\n fix: make azure disk URI as case insensitive     k8s version fixed version     v1.13 no fix   v1.14 1.14.9   v1.15 1.15.6   v1.16 1.16.0   v1.17 1.17.0    Work around:\nUse resourceGroups instead of resourcegroups in disk PV configuration\n22. vmss dirty cache issue Issue details:\nclean vmss cache should happen after disk attach/detach operation, now it’s before those operations, which would lead to dirty cache. since update operation may cost 30s or more, and at that time period, if there is another get vmss operation, it would get the old data disk list\n VMSS disk attach/detach issues w/ v1.13.12, v1.14.8, v1.15.5, v1.16.2 Disk attachment/mounting problems, all pods with PVCs stuck in ContainerCreating  Fix\n fix vmss dirty cache issue     k8s version fixed version notes     v1.13 no fix regression since 1.13.12 (hotfixed in AKS release)   v1.14 1.14.10 regression only in 1.14.8, 1.14.9 (hotfixed in AKS release)   v1.15 1.15.7 regression only in 1.15.5, 1.15.6 (hotfixed in AKS release)   v1.16 1.16.4 regression only in 1.16.2, 1.16.3 (hotfixed in AKS release)   v1.17 1.17.0     Work around:\nDetach disk in problem manually\n23. race condition when delete disk right after attach disk Issue details:\nThere is condition that attach and delete disk happens in same time, azure CRP don’t check such race condition\n should not delete an azure disk when that disk is being attached  Fix\n fix race condition when delete azure disk right after attach azure disk     k8s version fixed version notes     v1.13 no fix hotfixed in AKS release since 1.13.12   v1.14 1.14.10 hotfixed in AKS release in 1.14.8, 1.14.9   v1.15 1.15.7 hotfixed in AKS release in 1.15.5, 1.15.6   v1.16 1.16.4 hotfixed in AKS release in 1.16.2, 1.16.3   v1.17 1.17.0     Work around:\nDetach disk in problem manually\n24. attach disk costs 10min Issue details:\nPR Fix aggressive VM calls for Azure VMSS change getVMSS cache TTL from 1min to 10min, getVMAS cache TTL from 5min to 10min, that will cause error WaitForAttach ... Cannot find Lun for disk, and it would make attach disk opeation costs 10min on VMSS and 15min on VMAS, detailed error would be like following:\nEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 29m default-scheduler Successfully assigned authentication/authentication-mssql-statefulset-0 to aks-nodepool1-29122124-vmss000004 Normal SuccessfulAttachVolume 28m attachdetach-controller AttachVolume.Attach succeeded for volume \"pvc-8d9f0ade-1825-11ea-83a0-22ced17d4a3d\" Warning FailedMount 23m (x10 over 27m) kubelet, aks-nodepool1-29122124-vmss000004 MountVolume.WaitForAttach failed for volume \"pvc-8d9f0ade-1825-11ea-83a0-22ced17d4a3d\" : Cannot find Lun for disk kubernetes-dynamic-pvc-8d9f0ade-1825-11ea-83a0-22ced17d4a3d Warning FailedMount 23m (x3 over 27m) kubelet, aks-nodepool1-29122124-vmss000004 Unable to mount volumes for pod \"authentication-mssql-statefulset-0_authentication(8df467e7-1825-11ea-83a0-22ced17d4a3d)\": timeout expired waiting for volumes to attach or mount for pod \"authentication\"/\"authentication-mssql-statefulset-0\". list of unmounted volumes=[authentication-mssql-persistent-data-storage]. list of unattached volumes=[authentication-mssql-persistent-data-storage default-token-b7spv] Normal Pulled 21m kubelet, aks-nodepool1-29122124-vmss000004 Container image \"mcr.microsoft.com/mssql/server:2019-CTP3.2-ubuntu\" already present on machine Normal Created 21m kubelet, aks-nodepool1-29122124-vmss000004 Created container authentication-mssql Normal Started 21m kubelet, aks-nodepool1-29122124-vmss000004 Started container authentication-mssql This slow disk attachment issue only exists on 1.13.12+, 1.14.8+, fortunately, from k8s 1.15.0, this issue won’t happen, since getDiskLUN logic has already been refactored (already has PR:fix azure disk lun error, won’t depend on getVMSS operation to get disk LUN.\nRelate issues:\n GetAzureDiskLun sometimes costs 10min which is too long time  Fix\n fix azure disk lun error     k8s version fixed version notes     v1.13 no fix need to hotfix in AKS release since 1.13.12 (slow disk attachment exists on 1.13.12+)   v1.14 in cherry-pick need to hotfix in AKS release in 1.14.8, 1.14.9 (slow disk attachment exists on 1.14.8+)   v1.15 1.15.0    v1.16 1.16.0     Work around:\nWait for about 10min or 15min, MountVolume.WaitForAttach operation would retry and would finally succeed\n25. Multi-Attach error Issue details:\nIf two pods on different nodes are using same disk PVC(this issue may also happen when doing rollingUpdate in Deployment using one replica), would probably hit following error:\nEvents: Warning FailedAttachVolume 9m attachdetach-controller Multi-Attach error for volume \"pvc-fc0bed38-48bf-43f1-a7e4-255eef48ffb9\" Volume is already used by pod(s) sqlserver3-5b8449449-5chzx Warning FailedMount 42s (x4 over 7m) kubelet, aks-nodepool1-15915763-vmss000001 Unable to mount volumes for pod \"sqlserver3-55754785bb-jjr6d_default(55381f38-9640-43a9-888d-096387cbb780)\": timeout expired waiting for volumes to attach or mount for pod \"default\"/\"sqlserver3-55754785bb-jjr6d\". list of unmounted volumes=[mssqldb]. list of unattached volumes=[mssqldb default-token-q7cw9] The above issue is upstream issue(detailed error code), it could be due to following reasons:\n two pods are using same disk PVC, this issue could happen even using Deployment with one replica(see below workaround) one node is in Shutdown(deallocated) state, this is by design now and there is on-going upstream work to fix this issue  Propose to taint node “shutdown” condition add node shutdown KEP      workaround: user could use set terminationGracePeriodSeconds: 0 in deployment or kubectl delete pod PODNAME --grace-period=0 --force to delete pod on the deallocated node Azure cloud provider solution: delete shutdown node(in InstanceExistsByProviderID) like what other cloud provider does today, while it may lead to other problem(e.g. node label loss), see details: Common handling of stopped instances across cloud providers.    since azure disk PVC could not be attached to one node.\nRelate issues:\n Trouble attaching volume  Work around:\nWhen using disk PVC config in deployment, maxSurge: 0 could make sure there would not be no more than two pods in Running/ContainerCreating state when doing rollingUpdate:\ntemplate: ... strategy: rollingUpdate: maxSurge: 0 maxUnavailable: 1 type: RollingUpdate Refer to Rolling Updates with Kubernetes Deployments for more detailed rollingUpdate config, and you could find maxSurge: 0 setting example here\nNote\n error messages:  Multi-Attach error for volume \"pvc-e9b72e86-129a-11ea-9a02-9abdbf393c78\" Volume is already used by pod(s)    two pods are using same disk PVC, this issue could happen even using Deployment with one replica, check detailed explanation and workaround here with above explanation\n Multi-Attach error for volume \"pvc-0d7740b9-3a43-11e9-93d5-dee1946e6ce9\" Volume is already exclusively attached to one node and can't be attached to another  This could be a transient error when move volume from one node to another, use following command to find attached node:\nkubectl get no -o yaml | grep volumesAttached -A 15 | grep pvc-0d7740b9-3a43-11e9-93d5-dee1946e6ce9 -B 10 -A 15 related code: reportMultiAttachError\n26. attached non-existing disk volume on agent node Issue details:\nThere is little possibility that attach/detach disk and disk deletion happened in same time, that would cause race condition. This PR add remediation when attach/detach disk, if returned 404 error, it will filter out all non-existing disks and try attach/detach operation again.\nFix\n fix: add remediation in azure disk attach/detach fix: azure disk remediation issue     k8s version fixed version     v1.14 no fix   v1.15 1.15.11   v1.16 1.16.8   v1.17 1.17.4   v1.18 1.18.0    Work around:\nDetach disk in problem manually\n27. failed to get azure instance id for node (not a vmss instance) Issue details:\nPR#81266 does not convert the VMSS node name which causes error like this:\nfailed to get azure instance id for node \\\"k8s-agentpool1-32474172-vmss_1216\\\" (not a vmss instance) That will make dangling attach return error, and k8s volume attach/detach controller will getVmssInstance, and since the nodeName is in an incorrect format, it will always clean vmss cache if node not found, thus incur a get vmss API call storm.\nFix\n fix: azure disk dangling attach issue on VMSS which would cause API throttling     k8s version fixed version     v1.14 only hotfixed with image mcr.microsoft.com/oss/kubernetes/hyperkube:v1.14.8-hotfix.20200529.1   v1.15 only hotfixed with image mcr.microsoft.com/oss/kubernetes/hyperkube:v1.15.11-hotfix.20200529.1, mcr.microsoft.com/oss/kubernetes/hyperkube:v1.15.12-hotfix.20200603   v1.16 1.16.10 (also hotfixed with image mcr.microsoft.com/oss/kubernetes/hyperkube:v1.16.9-hotfix.20200529.1)   v1.17 1.17.6   v1.18 1.18.3   v1.19 1.19.0    Work around:\n Stop kube-controller-manager detach disk in problem from that vmss node manually  az vmss disk detach -g \u003cRESOURCE_GROUP_NAME\u003e --name \u003cVMSS_NAME\u003e --instance-id \u003cID(number)\u003e --lun number e.g. per below logs,\nE0501 11:15:40.981758 1 attacher.go:277] failed to detach azure disk \"/subscriptions/xxx/resourceGroups/rg/providers/Microsoft.Compute/disks/rg-dynamic-pvc-dc282131-b669-47db-8d57-cb3b9789ac3e\", err failed to get azure instance id for node \"k8s-agentpool1-32474172-vmss_1216\" (not a vmss instance)  find lun number of disk rg-dynamic-pvc-dc282131-b669-47db-8d57-cb3b9789ac3e:  az vmss show -g rg --name k8s-agentpool1-32474172-vmss --instance-id 1216  detach vmss disk manually:  az vmss disk detach -g rg --name k8s-agentpool1-32474172-vmss --instance-id 1216 --lun number Start kube-controller-manager  ","excerpt":" azure disk plugin known issues  Recommended stable version for azure …","ref":"/cloud-provider-azure/faq/known-issues/azuredisk/","title":"AzureDisk CSI Driver Known Issues"},{"body":" azure file plugin known issues  Recommended stable version for azure file 1. azure file mountOptions setting  file/dir mode setting: other useful mountOptions setting:   2. permission issue of azure file dynamic provision in acs-engine 3. Azure file support on Sovereign Cloud 4. azure file dynamic provision failed due to cluster name length issue 5. azure file dynamic provision failed due to no storage account in current resource group 6. azure file plugin on Windows does not work after node restart 7. file permission could not be changed using azure file, e.g. postgresql 8. Could not delete pod with AzureFile volume if storage account key changed 9. Long latency when handling lots of small files 10. allow access from selected network setting on storage account will break azure file dynamic provisioning 11. azure file remount on Windows in same node would fail 12. update azure file secret if azure storage account key changed 13. Create Azure Files PV AuthorizationFailure when using advanced networking 14. initial delay(5s) in mounting azure file    Recommended stable version for azure file    k8s version stable version     v1.7 1.7.14+   v1.8 1.8.11+   v1.9 1.9.7+   v1.10 1.10.2+   v1.11 1.11.8+   v1.12 1.12.6+   v1.13 1.13.4+   v1.14 1.14.0+    1. azure file mountOptions setting file/dir mode setting: Issue details:\n fileMode, dirMode value would be different in different versions, in latest master branch, it’s 0755 by default, to set a different value, follow this mount options support of azure file (available from v1.8.5). For version v1.8.0-v1.8.4, since mount options support of azure file is not available, as a workaround, securityContext could be specified for the pod, detailed pod example  securityContext:runAsUser:XXXfsGroup:XXX   version fileMode, dirMode value     v1.6.x, v1.7.x 0777   v1.8.0 ~ v1.8.5, v1.9.0 0700   v1.8.6 or later, v1.9.1 ~ v1.10.9, v1.11.0 ~ v1.11.3, v1.12.0 ~ v.12.1 0755   v1.10.10 or later 0777   v1.11.4 or later 0777   v1.12.2 or later 0777   v1.13.x 0777    other useful mountOptions setting:  mfsymlinks: make azure file(cifs) mount supports symbolic link nobrl: Do not send byte range lock requests to the server. This is necessary for certain applications that break with cifs style mandatory byte range locks (and most cifs servers do not yet support requesting advisory byte range locks). Error message could be like following:  Error: SQLITE_BUSY: database is locked Related issues\n azureFile volume mode too strict for container with non root user Unable to connect to SQL-lite db mounted on AzureFile/AzureDisks [SQLITE_BUSY: database is locked] Allow nobrl parameter like docker to use sqlite over network drive Error to deploy mongo with azure file storage  2. permission issue of azure file dynamic provision in acs-engine Issue details:\nFrom acs-engine v0.12.0, RBAC is enabled, azure file dynamic provision does not work from this version\nerror logs:\nEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Warning ProvisioningFailed 8s persistentvolume-controller Failed to provision volume with StorageClass \"azurefile\": Couldn't create secret secrets is forbidden: User \"system:serviceaccount:kube-syste m:persistent-volume-binder\" cannot create secrets in the namespace \"default\" Warning ProvisioningFailed 8s persistentvolume-controller Failed to provision volume with StorageClass \"azurefile\": failed to find a matching storage account Related issues\n azure file PVC need secrets create permission for persistent-volume-binder  Workaround:\n Add a ClusterRole and ClusterRoleBinding for azure file dynamic privision  kubectl create -f https://raw.githubusercontent.com/andyzhangx/Demo/master/aks-engine/rbac/azure-cloud-provider-deployment.yaml  delete the original PVC and recreate PVC  Fix\n PR in acs-engine: fix azure file dynamic provision permission issue  3. Azure file support on Sovereign Cloud Azure file on Sovereign Cloud is supported from v1.7.11, v1.8.0\n4. azure file dynamic provision failed due to cluster name length issue Issue details: k8s cluster name length must be less than 16 characters, otherwise following error will be received when creating dynamic privisioning azure file pvc, this bug exists in [v1.7.0, v1.7.10]:\n Note: check cluster-name by running grep cluster-name /etc/kubernetes/manifests/kube-controller-manager.yaml on master node\n persistentvolume-controller Warning ProvisioningFailed Failed to provision volume with StorageClass \"azurefile\": failed to find a matching storage account Fix\n PR Fix share name generation in azure file provisioner     k8s version fixed version     v1.7 1.7.11   v1.8 1.8.0   v1.9 1.9.0    5. azure file dynamic provision failed due to no storage account in current resource group Issue details:\nWhen create an azure file PVC, there will be error if there is no storage account in current resource group, error info would be like following:\nEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Warning ProvisioningFailed 10s (x5 over 1m) persistentvolume-controller Failed to provision volume with StorageClass \"azurefile-premium\": failed to find a matching storage account Related issues\n failed to create azure file pvc if there is no storage account in current resource group  Workaround: specify a storage account in azure file dynamic provision, you should make sure the specified storage account is in the same resource group as your k8s cluster. In AKS, the specified storage account should be in shadow resource group(naming as MC_+{RESOUCE-GROUP-NAME}+{CLUSTER-NAME}+{REGION}) which contains all resources of your aks cluster.\nFix\n PR fix the create azure file pvc failure if there is no storage account in current resource group     k8s version fixed version     v1.7 1.7.14   v1.8 1.8.9   v1.9 1.9.4   v1.10 1.10.0    6. azure file plugin on Windows does not work after node restart Issue details: azure file plugin on Windows does not work after node restart, this is due to New-SmbGlobalMapping cmdlet has lost account name/key after reboot\nRelated issues\n azure file plugin on Windows does not work after node restart  Workaround:\n delete the original pod with azure file mount create the pod again  Fix\n PR fix azure file plugin failure issue on Windows after node restart     k8s version fixed version     v1.7 not support in upstream   v1.8 1.8.10   v1.9 1.9.7   v1.10 1.10.0    7. file permission could not be changed using azure file, e.g. postgresql error logs when running postgresql on azure file plugin:\ninitdb: could not change permissions of directory \"/var/lib/postgresql/data\": Operation not permitted fixing permissions on existing directory /var/lib/postgresql/data Issue details: azure file plugin is using cifs/SMB protocol, file/dir permission could not be changed after mounting\nWorkaround:\nUse mountOptions with dir_mode, file_mode set as 0777:\nkind:StorageClassapiVersion:storage.k8s.io/v1metadata:name:azurefileprovisioner:kubernetes.io/azure-filemountOptions:- dir_mode=0777- file_mode=0777 follow detailed config here\n Related issues Persistent Volume Claim permissions\n8. Could not delete pod with AzureFile volume if storage account key changed Issue details:\n kubelet fails to umount azurefile volume when there is azure file connection, below is an easy repro:  create a pod with azure file mount regenerate the account key of the storage account delete the pod, and the pod will never be deleted due to UnmountVolume.TearDown error    error logs\nnestedpendingoperations.go:263] Operation for \"\\\"kubernetes.io/azure-file/cc5c86cd-422a-11e8-91d7-000d3a03ee84-myvolume\\\" (\\\"cc5c86cd-422a-11e8-91d7-000d3a03ee84\\\")\" failed. No retries permitted until 2018-04-17 10:35:40.240272223 +0000 UTC m=+1185722.391925424 (durationBeforeRetry 500ms). Error: \"UnmountVolume.TearDown failed for volume \\\"myvolume\\\" (UniqueName: \\\"kubernetes.io/azure-file/cc5c86cd-422a-11e8-91d7-000d3a03ee84-myvolume\\\") pod \\\"cc5c86cd-422a-11e8-91d7-000d3a03ee84\\\" (UID: \\\"cc5c86cd-422a-11e8-91d7-000d3a03ee84\\\") : Error checking if path exists: stat /var/lib/kubelet/pods/cc5c86cd-422a-11e8-91d7-000d3a03ee84/volumes/kubernetes.io~azure-file/myvolume: resource temporarily unavailable ... kubelet_volumes.go:128] Orphaned pod \"380b02f3-422b-11e8-91d7-000d3a03ee84\" found, but volume paths are still present on disk Workaround:\nmanually umount the azure file mount path on the agent node and then the pod will be deleted right after that\nsudo umount /var/lib/kubelet/pods/cc5c86cd-422a-11e8-91d7-000d3a03ee84/volumes/kubernetes.io~azure-file/myvolume Fix\n PR Fix bug:Kubelet failure to umount mount points     k8s version fixed version     v1.7 no fix(no cherry-pick fix is allowed)   v1.8 1.8.8   v1.9 1.9.7   v1.10 1.10.0    Related issues\n UnmountVolume.TearDown fails for AzureFile volume, locks up node Kubelet failure to umount glusterfs mount points  9. Long latency compared to disk when handling lots of small files Related issues\n azurefile is very slow Can’t roll out Wordpress chart with PV on AzureFile  10. allow access from selected network setting on storage account will break azure file dynamic provisioning When set allow access from selected network on storage account and will get following error when creating a file share by k8s:\npersistentvolume-controller (combined from similar events): Failed to provision volume with StorageClass \"azurefile\": failed to create share kubernetes-dynamic-pvc-xxx in account xxx: failed to create file share, err: storage: service returned error: StatusCode=403, ErrorCode=AuthorizationFailure, ErrorMessage=This request is not authorized to perform this operation. That’s because k8s persistentvolume-controller is on master node which is not in the selected network, and that’s why it could not create file share on that storage account.\nWorkaround:\nuse azure file static provisioning instead\n create azure file share in advance, and then provide storage account and file share name in k8s, here is an example  Related issues\n Azure Files PV AuthorizationFailure when using advanced networking   11. azure file remount on Windows in same node would fail Issue details:\nIf user delete a pod with azure file mount in deployment and it would probably schedule a pod on same node, azure file mount will fail since New-SmbGlobalMapping command would fail if file share is already mounted on the node.\nerror logs\nError logs would be like following:\nE0118 08:15:52.041014 2112 nestedpendingoperations.go:267] Operation for \"\\\"kubernetes.io/azure-file/42c0ea39-1af9-11e9-8941-000d3af95268-pvc-d7e1b5f9-1af3-11e9-8941-000d3af95268\\\" (\\\"42c0ea39-1af9-11e9-8941-000d3af95268\\\")\" failed. No retries permitted until 2019-01-18 08:15:53.0410149 +0000 GMT m=+732.446642701 (durationBeforeRetry 1s). Error: \"MountVolume.SetUp failed for volume \\\"pvc-d7e1b5f9-1af3-11e9-8941-000d3af95268\\\" (UniqueName: \\\"kubernetes.io/azure-file/42c0ea39-1af9-11e9-8941-000d3af95268-pvc-d7e1b5f9-1af3-11e9-8941-000d3af95268\\\") pod \\\"deployment-azurefile-697f98d559-6zrlf\\\" (UID: \\\"42c0ea39-1af9-11e9-8941-000d3af95268\\\") : azureMount: SmbGlobalMapping failed: exit status 1, only SMB mount is supported now, output: \\\"New-SmbGlobalMapping : Generic failure \\\\r\\\\nAt line:1 char:190\\\\r\\\\n+ ... ser, $PWord;New-SmbGlobalMapping -RemotePath $Env:smbremotepath -Cred ...\\\\r\\\\n+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\\\r\\\\n + CategoryInfo : NotSpecified: (MSFT_SmbGlobalMapping:ROOT/Microsoft/...mbGlobalMapping) [New-SmbGlobalMa \\\\r\\\\n pping], CimException\\\\r\\\\n + FullyQualifiedErrorId : HRESULT 0x80041001,New-SmbGlobalMapping\\\\r\\\\n \\\\r\\\\n\\\"\" Fix\n PR fix smb remount issue on Windows     k8s version fixed version     v1.10 no fix   v1.11 1.11.8   v1.12 1.12.6   v1.13 1.13.4   v1.14 1.14.0    Related issues\n azure file remount on Windows in same node would fail Mounting volume to pods fails randomly  12. update azure file secret if azure storage account key changed Issue details: There would be azure file mount failure if azure storage account key changed\nWorkaround: User needs to update azurestorageaccountkey field manually in azure file secret(secret name format: azure-storage-account-{storage-account-name}-secret in default namespace):\nkubectl delete secret azure-storage-account-{storage-account-name}-secret kubectl create secret generic azure-storage-account-{storage-account-name}-secret --from-literal azurestorageaccountname=... --from-literal azurestorageaccountkey=\"...\" --type=Opaque  make sure there is no \\r in the account name and key, here is a failed case\n  delete original pod(may use --force --grace-period=0) and wait a few minutes for new pod retry azure file mount  13. Create Azure Files PV AuthorizationFailure when using advanced networking Issue details:\nWhen create an azure file PV using advanced networking, user may hit following error:\nerr: storage: service returned error: StatusCode=403, ErrorCode=AuthorizationFailure, ErrorMessage=This request is not authorized to perform this operation Before api-version 2019-06-01, create file share action is considered as data-path operation, since 2019-06-01, it would be considered as control-path operation, not blocked by advanced networking any more.\nRelated issues\n Azure Files PV AuthorizationFailure when using advanced networking Azure Files PV AuthorizationFailure when using advanced networking  Fix\n PR Switch to use AzureFile management SDK     k8s version fixed version     v1.18 no fix   v1.19 1.19.0    Workaround:\nShut down the advanced networking when create azure file PV.\n14. initial delay(5s) in mounting azure file Issue details:\nWhen starting pods with AFS volumes, there is an initial delay of five seconds until the pod is transitioning from the “Scheduled” state. The reason for this is that currently the volume mounting happens inside a wait.Poll which will initially wait a specified interval(currently 5 seconds) before execution. This issue is introduced by PR fix: azure file mount timeout issue with v1.15.11+, v1.16.8+, v1.17.4+, v1.18.0+\nFix\n initial delay(5s) when starting Pods with Azure File volumes  Fix\n PR fix: initial delay in mounting azure disk \u0026 file     k8s version fixed version     v1.15 no fix   v1.16 1.16.14   v1.17 1.17.10   v1.18 1.18.7   v1.19 1.19.0    ","excerpt":" azure file plugin known issues  Recommended stable version for azure …","ref":"/cloud-provider-azure/faq/known-issues/azurefile/","title":"AzureFile CSI Driver Known Issues"},{"body":" Cloud Provider Azure An Azure implementation of the Kubernetes Cloud Provider.\nGet Started   Contribute   \n          Contributions welcome! We do a Pull Request contributions workflow on GitHub. New users are always welcome!\nRead more …\n   Join our slack channel! Please join #provider-azure in Kubernetes slack workspace.\nRead more …\n   Check out release notes! For announcement of latest features, etc.\nRead more …\n    ","excerpt":" Cloud Provider Azure An Azure implementation of the Kubernetes Cloud …","ref":"/cloud-provider-azure/","title":"Cloud Provider Azure"},{"body":"","excerpt":"","ref":"/cloud-provider-azure/contribute/","title":"Contribution"},{"body":"","excerpt":"","ref":"/cloud-provider-azure/development/","title":"Development Guide"},{"body":"","excerpt":"","ref":"/cloud-provider-azure/example/","title":"Example"},{"body":"What is Cloud Provider Azure? A Kubernetes Cloud Provider consists of two parts: a provider-specified cloud-controller-manager (or kube-controller-manager for in-tree version) and a provider-specified implementation of Kubernetes cloud provider interface. Currently, the Azure cloud-controller-manager is outside of Kubernetes repo and the cloud provider interface implementation is in pkg/provider.\nThe cloud-controller-manager is a Kubernetes control plane component which embeds cloud-specific control logic. It lets you link your cluster into your cloud provider’s API, and separates out the components that interact with that cloud platform from components that just interact with your cluster.\nBy decoupling the interoperability logic between Kubernetes and the underlying cloud infrastructure, the cloud-controller-manager component enables cloud providers to release features at a different pace compared to the main Kubernetes project.\nWhat is the difference between in-tree and out-of-tree cloud provider? In-tree cloud providers are the providers we develop \u0026 release in the main Kubernetes repository. This results in embedding the knowledge and context of each cloud provider into most of the Kubernetes components. This enables more native integrations such as the kubelet requesting information about itself via a metadata service from the cloud provider.\nOut-of-tree cloud providers are providers that can be developed, built, and released independent of Kubernetes core. This requires deploying a new component called the cloud-controller-manager which is responsible for running all the cloud specific controllers that were previously run in the kube-controller-manager.\nWhich one is recommended? We recommend using the in-tree cloud provider at this time because it’s out-of-tree counterpart is not 100% ready. However, out-of-tree cloud provider will become the No.1 pick in the near future.\n","excerpt":"What is Cloud Provider Azure? A Kubernetes Cloud Provider consists of …","ref":"/cloud-provider-azure/faq/","title":"FAQ"},{"body":"","excerpt":"","ref":"/cloud-provider-azure/install/","title":"Deploy Cloud Provider Azure"},{"body":"Prerequisite   An azure service principal\nPlease follow this guide for creating an azure service principal The service principal should either have:\n Contributor permission of a subscription Contributor permission of a resource group. In this case, please create the resource group first    Docker daemon enabled\n  How to run Kubernetes e2e tests locally  Prepare dependency project    aks-engine\nBinary downloads for the latest version of aks-engine for are available on Github. Download AKS Engine for your operating system, extract the binary and copy it to your $PATH.\nOn macOS, you can install aks-engine with Homebrew. Run the command brew install Azure/aks-engine/aks-engine to do so. You can install Homebrew following the instructions.\nOn Windows, you can install aks-engine via Chocolatey by executing the command choco install aks-engine. You can install Chocolatey following the instructions.\nOn Linux, it could also be installed by following commands:\n$ curl -o get-akse.sh https://raw.githubusercontent.com/Azure/aks-engine/master/scripts/get-akse.sh $ chmod 700 get-akse.sh $ ./get-akse.sh   Kubernetes\nThis serves as E2E tests case source, it should be located at $GOPATH/src/k8s.io/kubernetes.\ncd $GOPATH/src go get -d k8s.io/kubernetes   kubectl\nKubectl allows you to run command against Kubernetes cluster, which is also used for deploying CSI plugins. You can follow here to install kubectl. e.g. on Linux\ncurl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl chmod +x kubectl sudo mv kubectl /usr/local/bin/    Build docker image azure-cloud-controller-manager and push it to your docker image repository.\ngit clone https://github.com/kubernetes-sigs/cloud-provider-azure $GOPATH/src/sigs.k8s.io/cloud-provider-azure cd $GOPATH/src/sigs.k8s.io/cloud-provider-azure export IMAGE_REGISTRY=\u003cusername\u003e export IMAGE_TAG=\u003ctag\u003e make build-images make push-images # or manually `docker push`    Deploy a Kubernetes cluster with the above azure-cloud-controller-manager image.\nTo deploy a cluster, export all the required environmental variables first and then invoke make deploy:\nexport RESOURCE_GROUP_NAME=\u003cresource group name\u003e export AZURE_LOCATION=\u003clocation\u003e export AZURE_SUBSCRIPTION_ID=\u003csubscription ID\u003e export AZURE_CLIENT_ID=\u003cclient id\u003e export AZURE_CLIENT_SECRET=\u003cclient secret\u003e export AZURE_TENANT_ID=\u003ctenant id\u003e export USE_CSI_DEFAULT_STORAGECLASS=\u003ctrue/false\u003e export K8S_RELEASE_VERSION=\u003ck8s release version\u003e export CCM_IMAGE=\u003cimage of the cloud controller manager\u003e export CNM_IMAGE=\u003cimage of the cloud node manager\u003e make deploy To connect the cluster:\nexport KUBECONFIG=$GOPATH/src/sigs.k8s.io/cloud-provider-azure/_output/$(ls -t _output | head -n 1)/kubeconfig/kubeconfig.$LOCATION.json kubectl cluster-info   To check out more of the deployed cluster , replace kubectl cluster-info with other kubectl commands. To further debug and diagnose cluster problems, use kubectl cluster-info dump\nGet kubetest binary  go get -u k8s.io/test-infra/kubetest Run E2E tests  Please first ensure the kubernetes project locates at $GOPATH/src/k8s.io/kubernetes, the e2e tests will be built from that location.\ncd $GOPATH/src/k8s.io/kubernetes make WHAT='test/e2e/e2e.test' make WHAT=cmd/kubectl make ginkgo export KUBERNETES_PROVIDER=azure export KUBERNETES_CONFORMANCE_TEST=y export KUBERNETES_CONFORMANCE_PROVIDER=azure export CLOUD_CONFIG=$GOPATH/src/sigs.k8s.io/cloud-provider-azure/tests/k8s-azure/manifest/azure.json # some test cases require ssh configurations export KUBE_SSH_KEY_PATH=path/to/ssh/privatekey export KUBE_SSH_USER={ssh_user} # Replace the test_args with your own. kubetest --test --provider=local --check-version-skew=false --test_args='--ginkgo.focus=Port\\sforwarding' ","excerpt":"Prerequisite   An azure service principal\nPlease follow this guide for …","ref":"/cloud-provider-azure/development/e2e/e2e-tests/","title":"Kubernetes E2E tests"},{"body":"","excerpt":"","ref":"/cloud-provider-azure/blog/","title":"Release Notes"},{"body":"","excerpt":"","ref":"/cloud-provider-azure/search/","title":"Search Results"},{"body":"","excerpt":"","ref":"/cloud-provider-azure/topics/","title":"Topics"}]